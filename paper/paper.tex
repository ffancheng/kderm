\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Distortion corrected kernel density estimate on Riemannian manifolds},
            pdfkeywords={manifold learning, variable bandwidth, Riemannian metric, geodesic distance, Gaussian kernels},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Distortion corrected kernel density estimate on Riemannian manifolds}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\RequirePackage{bera}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\subsection}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}
        \placefig{2}{1.5}{width=5cm}{monash2}
        \placefig{16.9}{1.5}{width=2.1cm}{MBusSchool}
        \begin{textblock}{4}(16.9,4)ISSN 1440-771X\end{textblock}
        \begin{textblock}{7}(12.7,27.9)\hfill
        \includegraphics[height=0.7cm]{AACSB}~~~
        \includegraphics[height=0.7cm]{EQUIS}~~~
        \includegraphics[height=0.7cm]{AMBA}
        \end{textblock}
        \vspace*{2cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \footnotesize http://monash.edu/business/ebs/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}\vspace*{2cm}}}
\def\pageone{{\sffamily\setstretch{1}%
        \thispagestyle{empty}%
        \vbox to \textheight{%
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false, date=year}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}


\nojel

\RequirePackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}
\def\placefig#1#2#3#4{\begin{textblock}{.1}(#1,#2)\rlap{\includegraphics[#3]{#4}}\end{textblock}}


\nocover

\author{Fan~Cheng, Anastasios~Panagiotelis, Rob J~Hyndman}
\addresses{\textbf{Fan Cheng}\newline
Monash University
\newline{Email: \href{mailto:Fan.Cheng@monash.edu}{\nolinkurl{Fan.Cheng@monash.edu}}}\\[1cm]
\textbf{Anastasios Panagiotelis}\newline
The University of Sydney
\newline{Email: \href{mailto:Anastasios.Panagiotelis@sydney.edu.au}{\nolinkurl{Anastasios.Panagiotelis@sydney.edu.au}}}\\[1cm]
\textbf{Rob J Hyndman}\newline
Monash University
\newline{Email: \href{mailto:Rob.Hyndman@monash.edu}{\nolinkurl{Rob.Hyndman@monash.edu}}}\\[1cm]
}

\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf Cheng, Panagiotelis, Hyndman: \@date}
\makeatother

%% Any special functions or other packages can be loaded here.
\usepackage{tabu,placeins,algorithmic}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\urlstyle{tt}  % use monospace font for urls
\usepackage[cal = txupr]{mathalpha}
\geometry{margin=2.2cm}
\mathtoolsset{showonlyrefs}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
% \def\equationautorefname~#1\null{Equation~(#1)\null} % NOT showing Equation 1, but Section 2.1
\usepackage{bm}
\DeclareMathOperator\supp{supp}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

% Adjust headwidth in case user has changed geometry in header-includes
\renewcommand{\headwidth}{\textwidth}

\begin{document}
\maketitle
\begin{abstract}
Manifold learning can be used to obtain a low-dimensional representation of the underlying Riemannian manifold given the high-dimensional data. However, kernel density estimates of the low-dimensional embedding with a fixed bandwidth fail to account for the way manifold learning algorithms distort the geometry of the Riemannian manifold. We propose a novel distortion-corrected kernel density estimator (DC-KDE) for any manifold learning embedding by introducing the estimated Riemannian metric of each point to fix the distortion in the line and volume elements. The geometric information of the manifold guarantees a more accurate density estimation of the true manifold, which subsequently could be used for anomaly detection. To compare our proposed estimator with a fixed-bandwidth kernel density estimator, we run two simulations with a 2-D data from Gaussian mixture model mapped into a 3-D twin peaks shape and a 5-D semi-hypersphere mapped in a 100-D space. We demonstrate that the proposed DC-KDE could improve the density estimates given a good manifold learning embedding and has higher rank correlations with the true manifold density. A shiny app in R is also developed for various simulation scenarios. The proposed method is applied to density estimation in statistical manifolds of electricity usage with the Irish smart meter data. This demonstrates the estimator's capability to fix the distortion of the manifold geometry and a new approach to anomaly detection in high-dimensional data.
\end{abstract}
\begin{keywords}
manifold learning, variable bandwidth, Riemannian metric, geodesic distance, Gaussian kernels
\end{keywords}

\newpage

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Multivariate kernel density estimation has gained lots of attention in exploratory data analysis. It is a non-parametric technique to estimate the data density based on weighted kernels centered at the data which usually belongs to a subset of \(\mathbb{R}^d\). Applications of kernel density estimation {[}KDE; \textcite{Parzen1962-gt}; \textcite{Chen2017-dw}{]} include finding hot spots of traffic network in the GIS environment \autocite{Xie2008-eb,Okabe2009-nb}, automatic detection in visual surveillance systems \autocite{Elgammal2002-cw}, wind power density detection \autocite{Jeon2012-ac}, prime prediction via Twitter messages \autocite{Gerber2014-tq}, and so on. However, when samples are assumed to be drawn from a Riemannian manifold embedded in a high-dimensional space of much more than \(\mathbb{R}^d\), kernel density estimation has to be generalized to a non-Euclidean space and further approximation methods have to be adapted. \textcite{Pelletier2005-vu} propose a kernel density estimator based on the Riemannian geodesic distance of the manifold but it is only applicable when the underlying manifold is known. Manifold learning algorithms could be applied to reduce the dimension and get an approximation of the manifold, but different manifold learning algorithms could induce different distortions of the same manifold. Therefore, we propose a distortion-corrected kernel density estimator for Riemannian manifolds embedded in more than \(\mathbb{R}^d\). Our estimator could be applied to any reasonable manifold learning embedding from the high-dimensional sample data and fix the distortions at each point with estimated Riemannian geodesic distance and volume density function. This estimator could be further applied for unsupervised tasks such as anomaly detection where the outliers are the lowest density points.

For a given kernel function, kernel density estimation is flexible to learn the shape of the underlying density of the data controlled by the bandwidth and the selection of bandwidth is crucial in KDE \autocite{Jones1990-oe,Terrell1992-ut}.
Many bandwidth selection methods have been proposed in the literature, including the rule-of-thumb, cross-validation \autocite{Jones1992-ta,Sain1994-gr} and plug-in methods \autocites[See][ for details]{Heidenreich2013-bl,Scott2015-vl}.
For univariate kernel density estimation, the bandwidth selection problem has been thoroughly investigated \autocites[See][ for reviews]{Jones1992-ef,Cao1994-st,Jones1996-cb,Wand1994-xu}. The generalization to multivariate case could mostly be found in \textcite{Duong2003-sp}, \textcite{Duong2004-rh}, and \textcite{Chacon2010-wm}. In this paper, we focus on the multivariate kernel density estimation.

Note that a fixed bandwidth matrix \(\pmb{H}\) is a global smoothing parameter for all data points. However, when the local data structure is not universal for all sample data, which is true in most applications, an adaptive bandwidth matrix that is varying rather than fixed at each data point is needed.
The bandwidth is varied depending on either the location of the sample points {[}sample smoothing estimator; \textcite{Terrell1992-ut}{]} or that of the estimated points {[}balloon estimator; \textcite{Terrell1992-ut}{]}. In this paper, the densities are estimated at the sample points themselves, so we only need to consider the case where the bandwidth changes for each sample point and will refer to this as the \emph{variable/adaptive kernel density estimation} {[}VKDE; Section 6.6 of \textcite{Scott2015-vl}{]} unless otherwise stated.
However, these kernel density estimators are based on random samples in the Euclidean space.

For samples points lying on a manifold with the differentiable structure called the Riemannian manifold, \textcite{Pelletier2005-vu} generalize the kernel density estimator based on the kernel weights from the geodesic distance between the estimated points and the sample points. The idea of the estimator is to use a strictly positive function of the geodesic distance on the manifold and then normalize it with the volume density function of the Riemannian manifold for curvature \autocite{Henry2009-ll}.
However, in many application scenarios, we tend to find that the sample points are not drawn directly from the manifolds because they are embedded in a much higher-dimensional space. Therefore, the kernel density estimator from \textcite{Pelletier2005-vu} is not applicable because the geodesic distance and the volume density function are unknown.
This is when we introduce manifold learning to reduce the input data dimension. For these high-dimensional data set, various manifold learning algorithms including ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP (see details of these algorithms in \textcite{Cheng2021-dh}), could be applied to get a low-dimensional embedding, which are used as approximations of the underlying manifold.

In manifold learning, the underlying idea is that the data lies on a low-dimensional smooth manifold that is embedded in a high-dimensional space. One of the fundamental objectives of manifold learning is to explore the geometry of the dataset, including the distances between points and volumes of regions of data. These intrinsic geometric attributes of the data, such as distances, angles, and areas, however, can be distorted in the low-dimensional embedding, leading to failure in recovering the geometry of the manifold \autocite{Goldberg2008-co}. To tackle this problem and measure the distortion incurred in manifold learning,
\textcite{Perrault-Joncas2013-pq} propose the Learn Metric algorithm to augment any existing embedding output with geometric information in the Riemannian metric of the manifold itself. By applying this algorithm, the outputs of different manifold learning methods can be unified and compared under the same framework, which would highly benefit in improving the effectiveness of the embedding.
The Riemannian metric using the method of \textcite{Perrault-Joncas2013-pq} gives some idea of the distortion of an embedding. Mapping the points through a non-linear function ``stretches'' some regions of space and ``shrinks'' others. The Riemannian gives us an idea of the direction and angle of this stretching at each point, which is informative for learning the manifold.

By exploiting the connection between the estimated Riemannian metric and the Riemannian geodesic distance as well as the volume density function for curvature, we propose the main contribution of the paper, which is the variable distortion-corrected kernel density estimator (DC-KDE) for manifold learning embedding. Starting from the high-dimensional sample data, we apply manifold learning algorithms to get the low-dimensional embedding in the same dimensional space as the underlying manifold together with the estimated Riemannian matrix at each embedding point. Then the DC-KDE is used to estimate the density of the manifold and distortions induced by manifold learning methods are fixed with the estimated geometric information. Our distortion-corrected estimator is novel in filling the gap between the high-dimensional sample space and the density of the unknown manifold. These density estimates are useful in many other areas, including classification, clustering and anomaly detection. Similar to \textcite{Cheng2021-dh}, the highest density region plots\autocite{Hyndman1996-lk} could be generated using the kernel density estimates for outlier visualization, which brings a novel anomaly detection method for Riemannian manifolds.

The rest of the paper is organized as follows. In \autoref{kderm}, we present our distortion-corrected kernel density estimator for Riemannian manifolds. We start by introducing the multivariate kernel density estimate with adaptive bandwidth and the kernel density estimator for Riemannian manifolds. Then we provide justification for the use of Riemannian metric to correct the distortions in manifold learning embedding and further apply the proposed estimator for anomaly detection. \autoref{simulation} is composed of two simulations with the proposed anomaly detection algorithm; the first deals with 2-dimensional data from gaussian mixture model mapped into a 3-D twin peaks structure and the second with a 5-D semi-hypersphere data mapped in a 100-D space. Different manifold learning algorithms are applied to the high-dimensional data to get the low-dimensional embedding which are then used to estimate densities and detect anomalies.
\autoref{application} contains the application to visualize and identify anomalous households in the Irish smart meter dataset. Conclusions and discussions are presented in \autoref{conclusion}. Readers interested in the notions of Riemannian geometry mentioned in this paper could use \autoref{riemgeo} as a reference.

\hypertarget{kderm}{%
\section{Distortion Corrected Kernel density estimate on Riemannian manifolds}\label{kderm}}

In this section, we introduce our method for kernel density estimation on manifolds that uses an embedding from a dimension reduction algorithm while correcting for the distortion induced by this embedding. Since some readers may be unfamiliar with the nuances of manifolds, we first discuss kernel density estimation for data in Euclidean space, then illustrate in \autoref{Pellet} how this generalizes to the estimator of \textcite{Pelletier2005-vu}, when the data lie on some known manifold. In \autoref{MetLearn}, we describe the Learn Metric algorithm of \textcite{Perrault-Joncas2013-pq}, which augments an embedding derived from a dimension reduction algorithm with an estimate of the Riemannian metric expressed in local coordinates. By combining elements from the work of \textcite{Pelletier2005-vu} and \textcite{Perrault-Joncas2013-pq}, we derive our own novel distortion corrected kernel density estimate in \autoref{DCKDE}. To keep this section as succinct as possible, we do not define concepts such as manifolds, charts, geodesic distance etc., but provide this information for readers unfamiliar with differential geometry in \autoref{riemgeo}.

In the following, we denote \(M\) as the \(d\)-dimensional manifold from which our data are sampled. Points on this manifold are denoted \(\pmb{p}\) in general, with \(\pmb{p}_1,\dots,\pmb{p}_n\) denoting the observed sample. Often \(\pmb{p}_i\) will be high-dimensional vectors such that \(\pmb{p}_i\in\mathbb{R}^m\) with \(m\gg d\), however this need not be the case. For instance, \(\pmb{p}_i\) may be probability distributions on a statistical manifold. The methods we propose for estimating the density at each \(\pmb{p}_i\) only require some sense of distance between the `input' points, \(d(\pmb{p}_i,\pmb{p}_j)\), such that we can apply dimension reduction algorithms to obtain an `output' embedding, \(\pmb{y}_1,\dots,\pmb{y}_n\), where \(\pmb{y}_i\in\mathbb{R}^d\). We will denote this embedding as \(\pmb{y}_i=\psi(\pmb{p}_i)\). Finally, we denote by \(\lambda\) the Lebesgue measure of \(\mathbb{R}^d\), letting \(\|\cdot\|\) be the usual Euclidean norm and following \textcite{Pelletier2005-vu}, we make these assumptions about the kernel function \(K:\mathbb{R}_+\rightarrow\mathbb{R}\),
\begin{align}
\label{eq:kernelcondition}
& (i) \int_{\mathbb{R}^{d}} K(\|\pmb{y}\|) \mathrm{d} \lambda(\pmb{y})=1;
(ii) \int_{\mathbb{R}^{d}} \pmb{y} K(\|\pmb{y}\|) \mathrm{d} \lambda(\pmb{y})=0;
(iii) \int_{\mathbb{R}^{d}}\|\pmb{y}\|^{2} K(\|\pmb{y}\|) \mathrm{d} \lambda(\pmb{y})<\infty; \\
& (iv) \supp K=[0; 1];
(v) \sup K(\|\pmb{y}\|)=K(0).
\end{align}
Note that these conditions are different from (and in some cases stricter than) those normally used for kernel density estimation. For instance, condition (iv) requires the support of the kernel to be bounded. The reasons for this will become clearer when we discuss the manifold setting in more detail. Also, for illustration purposes, in this section we pay particular attention to the uniform kernel for which \(K(z)\) equals one if \(0\leq z\leq1\) and zero otherwise. In our empirical section, more general kernel functions can be, and are, employed.

For data \(\pmb{y}_i\in\mathbb{R}^d\) with \(i=1,\dots,N\) and assuming a bandwidth matrix \(r\pmb{I}\) where \(r\) is a global bandwidth, then the usual kernel density estimator at a point \(\pmb{y}\) is given by
\begin{equation}
\label{eq:vkde}
\hat{f}(\pmb{y})=\frac{1}{N}\sum\limits_{i=1}^N \frac{1}{r^d} K\left(\frac{\|\pmb{y}-\pmb{y}_i\|}{r}\right).
\end{equation}

The intuition behind this estimator is very clear for a uniform kernel. The density at a point \(\pmb{y}\) is equal to the proportion of sample points that lie within a ball of radius \(r\) centered at \(\pmb{y}\), times a term that ensures the density integrates to 1. In general, the bandwidth matrix need not be proportional to the identity matrix. However, the intuition remains the same, only that the ball of radius \(r\) centered at \(\pmb{y}\) is found with respect to Mahalanobis distance rather than the usual Euclidean distance. For more on kernel density estimation in the Euclidean case, see \textcite{Scott2015-vl} and references therein.

Kernel density estimators of this form can and have been applied directly on the output embedding \(\pmb{y}\), and we will consider this approach as a benchmark in \autoref{simulation}. As a non-linear transformation, any dimension reduction algorithm `distorts' the density. To make this clear consider the simpler case computing the density after a change of variables \(\psi:\mathbb{R}^d\rightarrow\mathbb{R}^d\), which involves a Jacobian term. A similar notion applies to a manifold embedding so that the density of the output vectors \(\pmb{y}_i\) differs from the density on the manifold itself. Furthermore, standard kernel density estimates applied directly on the output embedding will be sensitive to the choice of dimension reduction algorithm since each different algorithm will distort the density in its own. This motivates a kernel desnity estimate that corrects for the distortion induced by \(\psi\).

\hypertarget{Pellet}{%
\subsection{Kernel Density estimation on manifolds}\label{Pellet}}

For kernel density estimation on a known manifold, \textcite{Pelletier2005-vu} propose the following estimator,
\begin{equation}
\label{eq:denriem}
\hat{f}(\pmb{p}) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{r^d \theta_{\pmb{p}_i}(\pmb{p})} K\left(\frac{d_g(\pmb{p}, \pmb{p}_i)}{r}\right),
\end{equation}

where \(d_g(\pmb{p}, \pmb{p}_i)\) denotes the geodesic distance between two points on the manifold \(\pmb{p}\) and \(\pmb{p}_i\) and \(\theta_{\pmb{p}_i}(\pmb{p})\) is known as the volume density function. The intuition behind the term \(K\left(\frac{d_g(\pmb{p}, \pmb{p}_i)}{r}\right)\) is relatively clear. For example, for a uniform kernel, the estimator at point \(\pmb{p}\) will still depend on the proportion of sample points within a ball of radius \(r\) centered at \(\pmb{p}\). However in this case, the geodesic distance on the manifold is used, rather than Euclidean or Mahalanobis distance. An additional technical assumption is that \(r\) is less than the injectivity radius of the manifold. A definition of the injectivity radius is given by \textcite{Chavel2006-mp} and also provided in the appendix. For our purposes, it is sufficient to note that this assumption precludes the possibility that the radius of a ball around \(\pmb{p}\) is so large that some points `fall inside' the ball more than once. For example on a sphere, a ball with radius greater than half the circumference of a great circle will wrap back around the sphere. This phenomenon also explains why the kernel function must be bounded for density estimation on manifolds.

The inclusion of the volume density function \(\theta_{\pmb{p}_i}(\pmb{p})\) is perhaps not as immediately clear, therefore, before providing formal details, we will briefly discuss the intuition behind the inclusion of this term. We have already highlighted that when using a uniform kernel, the kernel density estimate at a point \(\pmb{p}\) directly depends on the proportion of sample points within a ball of radius \(r\) around \(\pmb{p}\). However, the volume of this ball must also be taken into account. In Euclidean space with the usual Lebesgue measure, a radius \(r\) ball will always have the same volume regardless of its center. The same does not hold for manifolds and the volume density function ensures that the density estimate integrates to one.

More formally, the volume density function can be explained as follows. Consider the exponential map around \(\pmb{p}\), given by \(exp_{\pmb{p}}(\pmb{q})\), mapping vectors in the tangent space, \(\pmb{v}\in T_{\pmb{p}}M\), to points on the manifold, \(\pmb{q}\in M\). Loosely, \(\pmb{v}\) `points' in the direction of the geodesic between \(\pmb{p}\) and \(\pmb{q}\) and travelling along this geodesic at uniform speed \(\|\pmb{v}\|\) takes place in one unit of time. Now, consider a chart \(\varphi\) mapping points in the neighborhood of \(\pmb{p}\), via the inverse of the exponential map, to these \(\pmb{v}\) vectors, expressed in some local coordinate system. The volume density function is the square root of the determinant of the Riemannian metric expressed in this coordinate system. For more on the volume density function, see \textcite{Le_Brigant2019-lj}.

\hypertarget{MetLearn}{%
\subsection{Riemannian metric estimation}\label{MetLearn}}

To be able to apply the estimator of \textcite{Pelletier2005-vu} to the case where the manifold is not known, but where coordinates \(\pmb{y}_i\) for \(i=1,\dots,n\) are obtained from a dimension reduction algorithm, requires an estimate of the Riemannian metric in the coordinate system. Formally, the Riemannian metric \(g\) is a symmetric and positive definite tensor field which defines an inner product \(\langle,\rangle_g\) on the tangent space \(T_{\pmb{p}} M\) for every point \(\pmb{p} \in M\). The inner product between two tangent vectors \(u,v \in T_{\pmb{p}}M\), given by \(\langle u,v \rangle_g\), can be used to define geometric quantities. For example, angles on a manifold are given by \(\cos{\theta}=\frac{\langle u,v \rangle_g}{|u| |v|}\), while distances and volumes on manifolds are also defined with reference to the Riemannian metric. While the defined tangent vectors, the Riemannian metric and the geometric quantities are invariant to any specific choice of coordinates, they can still be expressed in terms of local coordinate systems. This is precisely the situation when data on a manifold are mapped to \(d\)-dimensional Euclidean vectors \(\pmb{y}_1,\pmb{y}_2\dots,\pmb{y}_n\) via a dimension reduction algorithm. After this mapping, angles, distances and volumes in this Euclidean `output space' are not the same as on the manifold since dimension reduction algorithms introduce distortions. To alleviate this issue, \textcite{Perrault-Joncas2013-pq} propose a method to augment \(\pmb{y}_1,\pmb{y}_2\dots,\pmb{y}_n\) with \(d\times d\) positive definite matrices, \(\pmb{H}_1,\pmb{H}_2\dots,\pmb{H}_n\), at each data point. These matrices estimate the Riemannian metric in local coordinates defined by the dimension reduction algorithm. For example, the angle between \(\pmb{p}_j\) and \(\pmb{p}_k\) at \(\pmb{p}_i\) depends (up to a first order approximation) on the inner product \((\pmb{y}_j-\pmb{y}_i)'\pmb{H}^{-1}_i(\pmb{y}_k-\pmb{y}_i)\) rather than the usual Euclidean inner product \((\pmb{y}_j-\pmb{y}_i)'(\pmb{y}_k-\pmb{y}_i)\).

While full details are provided in \textcite{Perrault-Joncas2013-pq}, we briefly describe the Learn Metric algorithm here. There are four main steps in the algorithm. First, a weighted neighborhood graph is constructed, with edges between \(\pmb{p}_i\) and \(\pmb{p}_j\) when \(\pmb{p}_i\) is a K-nearest neighbor of \(\pmb{p}_j\) or vice versa, and edge weights depending on the distance between \(\pmb{p}_i\) and \(\pmb{p}_j\) on the manifold. Second, the discrete Laplacian on this graph \(\hat{\mathcal{L}}_{\varepsilon,n}\) is estimated \autocite{Zhou2011-za}, where \(\varepsilon\) is the radius parameter for the nearest neighbor graph. Third, a dimension reduction method is applied to obtain the output embedding \(\pmb{y}_1,\dots,\pmb{y}_n\). Fourth, the Riemannian metric at each point is estimated by exploiting the connection between the Riemannian metric and the Laplace Beltrami operator (to which the graph Laplacian at Step 2 is a discrete estimator). Full details on these four steps are provided in \autoref{alg:learnmetric}. This algorithm is implemented in a Python library \emph{megaman} \autocite{McQueen2016-xz} although our own results are based on a re-implementation of the algorithm in \emph{R}. Two parameters, \(c=0.25\) and \(\sqrt{\varepsilon} = 0.4\), are set as suggested in the \emph{megaman} library.

As pointed out by \textcite{Perrault-Joncas2013-pq}, dimension reduction can be carried out such that the dimension of the output vectors is larger than the intrinsic manifold dimension \(d\). In this case, the ranks of the matrices \(\pmb{H}_i\) are equal to \(d\). Using a larger embedding dimension is justified since it is in general not possible to embed a manifold of dimension \(d\) globally into \(d\)-dimensional Euclidean space. In our simulated examples, we abstract from this issue by constructing examples that can be globally embedded into \(d\)-dimesional Euclidean space. In practice, to determine the dimension of the manifold, the \emph{two-nearest neighbor estimator (TWO-NN estimator)} \autocite{Facco2017-rl,Denti2021-jl} can be used. The \emph{R} library \emph{intRinsic} \autocite{Denti2021-qc} implements this algorithm and is used in all examples involving real data where the intrinsic dimension is unknown.

\begin{algorithm}[!htb]
  \caption{Learn metric algorithm in \cite{Perrault-Joncas2013-pq} }
  \label{alg:learnmetric}
  \DontPrintSemicolon
  \SetAlgoLined
  \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}\SetKwInOut{Parameter}{parameter}\SetKwInOut{OptParameter}{optimization parameter}
  \Input{ high-dimensional data $\pmb{x}_i \in \mathbb{R}^s$ for all $i=1,\ldots,N$ }
  \Output{ low-dimensional data $\pmb{y}_i \in \mathbb{R}^d$ and its Riemannian metric $\pmb{H}(\pmb{y}_i)$ for all $i=1,\ldots,N$ }
  \Parameter{ embedding dimension $d$, bandwidth parameter $\sqrt{\varepsilon}$, manifold learning algorithm }
  \OptParameter{ manifold learning parameters EMBED }
  \BlankLine
  \begin{algorithmic}[1]

  \STATE Construct a weighted neighborhood graph $\mathcal{G}_{w,\varepsilon}$ with weight matrix $\pmb{W}$ where $w_{i,j}=\exp(-\frac{1}{\varepsilon}\|\pmb{x}_i-\pmb{x}_j\|^2)$ for data points $\pmb{x}_i,\pmb{x}_j \in \mathbb{R}^s$;

  \STATE Calculate the $N\times N$ geometric graph Laplacian $\widetilde{\mathcal{L}}_{\varepsilon,N}$ by
  $$
  \widetilde{\mathcal{L}}_{\varepsilon,N} = 1/(c\varepsilon)(\widetilde{D}^{-1} \widetilde{W} - I_N),
  $$
  where $\widetilde{D}=diag{\widetilde{W}\pmb{1}}$, $\widetilde{W} = D^{-1}WD^{-1}$, and $D = diag{W\pmb{1}}$;

  \STATE Embed all data point $\pmb{X}\in \mathbb{R}^s$ to embedding coordinates $\pmb{Y}=(\pmb{y}^1,\dots,\pmb{y}^d)^\prime$ by any existing manifold learning algorithm EMBED;

  \STATE Obtain the matrix $\pmb{\tilde{H}}$ of all data point by applying the graph Laplacian $\widetilde{\mathcal{L}}_{\sqrt{\varepsilon},N}$ to the embedding coordinates matrix $\pmb{Y}$ with each element vector in $\pmb{\tilde{H}}$ being
  $$
    \pmb{\tilde{H}}^{i j} = \frac{1}{2} \left[\tilde{\mathcal{L}}_{\varepsilon, N}\left(\pmb{y}^i \cdot \pmb{y}^j\right) - \pmb{y}_i \cdot\left(\tilde{\mathcal{L}}_{\varepsilon, n} \pmb{y}^j\right) - \pmb{y}^j \cdot\left(\tilde{\mathcal{L}}_{\varepsilon, n} \pmb{y}^i\right)\right],
  $$
  where $i,j=1,\dots,d$ and the $\cdot$ calculation is the elementwise product between two vectors; 

  \STATE Calculate the Riemannian metric $\pmb{H}$ as the rank $d$ pseudo inverse of $\tilde{\pmb{H}}$ with 
  $$
    \pmb{H} = \pmb{U} diag{1/(\Lambda[1:d])} \pmb{U}^\prime,
  $$
  where $[\pmb{U}, \Lambda]$ is the eigendecomposition of matrix $\pmb{\tilde{h}}(x)$, and $U$ is the matrix of column eigenvectors ordered by the eigenvalues $\Lambda$ in descending order.

  \end{algorithmic}
\end{algorithm}

\hypertarget{DCKDE}{%
\subsection{Distortion corrected KDE}\label{DCKDE}}

With all fundamentals introduced, we can now give our novel Distortion Corrected KDE (DC-KDE) as
\begin{equation}
\label{eq:denestimator}
\hat{f}(\pmb{y}_j) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{r^d} \bigg(\frac{|\det \pmb{H}_j|}{|\det \pmb{H}_i|} \bigg)^{1/2} K\bigg( \frac{\| \pmb{H}^{-1/2}_i (\pmb{y}_j - \pmb{y}_i)\|}{r} \bigg).
\end{equation}

The estimator has a similar structure to Equation \eqref{eq:denriem} with some key differences. To understand these differences, it is first critical to appreciate that the coordinates \(\pmb{H}^{-1/2}_i (\pmb{y}_j - \pmb{y}_i)\) give an embedding that is approximately isometric in a small neighborhood around the \(i^{th}\) observed point (this insight is discussed at length is Section 6.2 of \textcite{Perrault-Joncas2013-pq}). This is crucial for two reasons. First, this implies that the term \(\| \pmb{H}^{-1/2}_i (\pmb{y}_j - \pmb{y}_i)\|\) approximates the geodesic distance between \(\pmb{y}_i\) and \(\pmb{y}_j\). Second, the estimator in Equation \eqref{eq:denriem} is valid only when the coordinate mapping is the logarithmic map around \(\pmb{y}_i\), and it is this mapping that is approximated by \(\pmb{H}^{-1/2}_i (\pmb{y}_j - \pmb{y}_i)\). For this reason there is a ratio of two determinants to ensure the density integrates to one, the first is a consequence of the mapping from the manifold to the coordinate system (from a dimension reduction algorithm), while the second is the transformation \(\pmb{H}^{-1/2}_i (\pmb{y}_j - \pmb{y}_i)\) which ensures that the embedding approximates the logarithmic map. Also worth noting is the resemblance between the estimator and multivariate variable bandwidth estimation \autocite{Breiman1977-qc,Jones1990-oe,Terrell1992-ut}.

One limitation of the kernel density estimator is that the density can be estimated only at points where data have been observed since the estimator requires the Riemannian \(\pmb{H}_j\). To estimate the density at points that do not correspond to observed data, any smoothed average of nearest neighbors can be used instead. We note that the particular downstream task that we are interested in is anomaly detection for which only the density estimates at observed sample points are required since anomalies are identified as the points with lowest density. The entire workflow is summarized in \autoref{fig:vkde}. The last two steps in \autoref{fig:vkde} are our main contributions, generating distortion-corrected KDE with adaptive Riemannian metric \(\pmb{H}_i\) at each point and computing the highest density region plots based on the density estimates for anomaly detection. Compared to the anomaly detection with a general kernel density estimator in \textcite{Cheng2021-dh}, the changes are also highlighted in blue. With this anomaly detection process, outliers based on lowest densities could be detected more accurately regardless of the distortion in manifold learning.



\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/DC-KDE} 

}

\caption{The proposed schematic for anomaly detection with distortion corrected kernel density estimates.}\label{fig:vkde}
\end{figure}

\hypertarget{simulation}{%
\section{Simulations}\label{simulation}}

In this section, we examine two scenarios for both low and high dimensions to test our proposed distortion corrected KDE. For visualization purposes, \autoref{twodgaussian} presents an example of a two-dimensional manifold embedded in 3-dimensional ambient Euclidean space. As a high-dimensional example, the second simulation in \autoref{fivedgaussian} is based on a 4-dimensional manifold embedded in a 100-dimensional ambient space. To estimate the density, we use the dimension reduction algorithms ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP. In general, we aim to highlight two advantages of our proposed distortion corrected KDE compared to KDE applied directly to the output coordinates. First, the density estimates are closer to the ground truth when distortion correction is used, and as a consequence, distortion correction is more adept at detecting anomalies. Second, we show how density estimation and anomaly detection are more robust to a different choice of dimension reduction method when distortion correction is used.

\hypertarget{twodgaussian}{%
\subsection{Twin peaks example}\label{twodgaussian}}

The simulation setup for the twin peaks example is to first generate vectors \(\pmb{v}_1,\dots,\pmb{v}_N\) for \(N=2000\) from a 2-dimensional Gaussian mixture model. The mixture has four components with different means
\(\pmb{\mu_1}=(0.25, 0.25)^\prime, \pmb{\mu_2}=(0.25, 0.75)^\prime, \pmb{\mu_3}=(0.75, 0.25)^\prime, \pmb{\mu_4}=(0.75, 0.75)^\prime\) and the same variance-covariance matrix \(\pmb{\Sigma}_i=diag(0.016, 0.016), i=1,2,3,4\). The mixture proportions
are equally set as \(\pi_i=0.25, i=1,2,3,4\). The two dimensional data in \autoref{fig:metadensity} is mapped to a `twin peaks' surface via the following
\begin{equation}
\label{eq:twinpeak}
\begin{array}{lcl}
x_1 = v_1, \\
x_2 = v_2, \\
x_3 = \sin(\pi v_1) \tan (3 v_2).
\end{array}
\end{equation}

The three-dimensional twin peaks mapping is shown in \autoref{fig:twinpeaks3d}. The colors in both \autoref{fig:metadensity} and \autoref{fig:twinpeaks3d} indicate the true density of the data via the twin peaks mapping, with lower density points in darker colors scattered in the outer as well as center areas.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/truedensity_twinpeaks_dc_labv} 

}

\caption{Underlying data for the Gaussian mixture model of four kenels with means $(0.25, 0.25), (0.25, 0.75), (0.75, 0.25), (0.75, 0.75)$ and the same variance-covariance matrix $diag(0.016, 0.016)$. The colors indicate the true density of the data when they are mapped via the twin peaks function. Lower density points in darker colors are scattered both in the outer and center areas.}\label{fig:metadensity}
\end{figure}



\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/scatterplot3d_twinpeaks_dc} 

}

\caption{Scatterplot of the 3-d twin peaks data with the same colors indicating the true density as in \autoref{fig:metadensity}.}\label{fig:twinpeaks3d}
\end{figure}

It is important to note that the \emph{true density} on the manifold is not simply a Gaussian mixture, since the mapping in Equation \eqref{eq:twinpeak} distorts the distribution. To recover the true distribution requires the correct Jacobian term for the pushforward from \(\pmb{v}\) to the volume form of twin peaks manifold. By treating the \(\pmb{v}\) as an `output' embedding from input points \(\pmb{x}\) that lie on the true manifold and applying the Learn Metric algorithm, we can obtain \(\pmb{\Gamma}_i\) for \(i=1,\dots,n\) where \(\pmb{\Gamma}_i\) is the Riemannian of the coordinate system given by \(\pmb{v}\). This notation is distinct from \(\pmb{H}_i\) which is the output of the Learn Metric algorithm for a coordinate system obtained via a dimension reduction algorithm. The true density on the manifold can be obtained as \(f(\pmb{p}_i)=f(\pmb{v}_i)|\pmb{\Gamma}_i|^{1/2}\), where \(f(\pmb{v}_i)\) is a the density of a four component mixture of normals. Knowledge of \(\pmb{v}\) and \(\pmb{\Gamma_i}\) will not be used when estimating the density but only to establish a `ground truth' for densities on the manifold. \autoref{fig:metadensity} shows the simulated \(\pmb{v}\) with color indicating the true density of data on the manifold. Anomalies are defined as points with the lowest densities shown in darker colors and with `typical' points having the highest density shown in yellow. The anomalies are found around the edges of the plot, but there are is also a low density region between the means of the four mixture components. The objective is to determine whether we can correctly identify these features without any knowledge of the true density or the \(\pmb{v}\).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak2000_5levels_outliers_comparison_5ml_r0_5} 

}

\caption{Highest density region plots of five manifold learning embeddings of the twin peaks data in each row. The top 20 outliers, hightlighted in black and indexed in blue text, are found by the true manifold density (left panel), DC-KDE (middle panel) and KDE (right panel). DC-KDE finds more true outliers than KDE in all five rows.}\label{fig:tpoutliers}
\end{figure}

\autoref{fig:tpoutliers} summarizes the results. Each row of panels corresponds to a different dimension reduction technique, while the left, center and right columns correspond to density estimates for the ground truth density, distortion corrected KDE and KDE respectively. We set the bandwidth parameter in the DC-KDE \eqref{eq:denestimator} as \(r=0.5\). Colors show the different estimated density at each point with anomalies shown in black with blue indexing, and higher density points shown in yellow. For many methods, the salient features of the ground truth distribution are clear regardless of whether distortion correction is applied, for example for ISOMAP, all three plots, identify a similar set of outliers and four high density regions. On the other hand for LLE, the left panel shows that dimension reduction pulls outliers on the manifold in towards the centre. The distortion corrected KDE can account for this, while KDE without distortion correction on the other hand does not correctly identify the anomalies. For t-SNE, the ground truth and distortion corrected KDE identify four regions of high density, while a KDE estimate without distortion correction seems to identify a larger number of modes. This concurs with the common observation that t-SNE tends to output clusters even where such clusters may not be present in the underlying data.

\begin{table}

\caption{\label{tab:tpcors}Correlation between true density ranking and estimated density ranking for different manifold learning embeddings of the twin peak data. Distortion corrected KDE outperfoms for all dimension reduction algorithms and gives the higher rank correlation to the output of t-SNE and UMAP.}
\centering
\begin{tabular}[t]{l>{}l>{}l>{}l>{}l>{}l}
\toprule
  & ISOMAP & LLE & Laplacian.Eigenmaps & t.SNE & UMAP\\
\midrule
DC-KDE & \textbf{0.823} & \textbf{0.673} & \textbf{0.672} & \textbf{0.806} & \textbf{0.794}\\
KDE & 0.798 & 0.500 & 0.606 & 0.451 & 0.469\\
\bottomrule
\end{tabular}
\end{table}

We can gain further insight by comparing the correlation between ranks of true densities and estimated densities from KDE with and without density correction by \autoref{tab:tpcors}. Distortion correction improves the rank correlation for all dimension reduction algorithms. In particular, while applying KDE to the output of t-SNE and UMAP leads to a moderate correlation below 0.5, applying distortion correction improves these rank correlations to values close to 0.8.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/Twin Peak2000_densityrank_comparison_isomap_radius8_r0_5_logrank_rec_colprob_smallblocks3_crossfalse} 

}

\caption{Scatterplot of log scale ranks of true density and estimated density ranks for DC-KDE (in the left panel) and KDE (in the right panel) based on ISOMAP embedding. The colors indicate different level of highest density regions and the shapes indicate whether the density estimators correctly classify the true anomalies. The shading contains all anomalies that are both truely and correctly identified. KDE without distortion correction gives more misclassified anomalies.}\label{fig:tpisomapden}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/Twin Peak2000_densityrank_comparison_tsne_radius8_r0_5_logrank_rec_colprob_smallblocks3_crossfalse} 

}

\caption{Scatterplot of log scale ranks of true density and estimated density ranks for DC-KDE (in the left panel) and KDE (in the right panel) based on t-SNE embedding. KDE without distortion correction gives many more misclassified anomalies.}\label{fig:tptsneden}
\end{figure}

In \autoref{fig:tpisomapden}, we plot ranks of the estimated density against the true density for the ISOMAP embedding with left panels showing results for distortion correction and the right panel showing results without distortion correction. Data are presented on a log scale to highlight anomalies. The bottom left shaded region contains all points that are truly anomalies and are identified as such (true positives), where an anomaly is defined as a point not falling within a \(99\%\) highest density region. The middle shaded region contain anomalies that are true positives in the sense of not lying in a \(90\%\) HDR. Points lying outside squares (shown as triangles) are incorrectly classified. For example, the three red triangles in the middle left of the left panel are truly anomalies since they lie outside the \(99\%\) HDR, but are not classified as such (although they are true positives if a 90\% HDR is used). Overall, the right panel contains many more misclassified anomalies, which shows that failing to apply distortion correction can have a severe impact on anomaly detection. \autoref{fig:tptsneden} shows the same plot but for t-SNE. The quality of t-SNE is worse than ISOMAP in this example therefore many more anomalies are misclassified. However, the difference between KDE with and without distortion correction is stark. These results highlight the importance of applying distortion correction especially when the quality of dimension reduction may not be high.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak2000_density_compare_isomapvs4ml_radius8_r0_5_rank} 

}

\caption{Comparison of ranks of the estimated densities based on ISOMAP  and four other dimension reduction algorithms in each row. Distortion corrected KDE (on the left panel) and KDE (on the right panel) are compared and DC-KDE shows the robustness to the use of different dimension reduction methods.}\label{fig:tpisomapvs4ml}
\end{figure}

Finally, \autoref{fig:tpisomapvs4ml} demonstrates the robustness of distortion correction methods to the use of dimension reduction algorithm. Each row of panels compares ranks of the estimated densities based on a dimension reduction algorithm to the estimated density based on ISOMAP. The left column shows results when distortion correction is applied, the right column when it is not applied. It can be seen that the rank correlation between estimates based on different dimension reduction algorithms is much higher when distortion correction is applied. This is critical since conclusions will be more robust to the choice of dimension reduction algorithm.

\hypertarget{fivedgaussian}{%
\subsection{Semi-hypersphere example embedded in 100-D space}\label{fivedgaussian}}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/tourr_5d_semisphere} 

}

\caption{Scatterplot display of the animation of a 5-D tour path with shapes indexing two Gaussian mixture components and the colors showing the distance to the kernel cores. Distant points in darker colors could be seen as anomalies.}\label{fig:fivedmeta}
\end{figure}

As a high-dimensional experiment, we generate the underlying data from a 5-dimensional semi-hypersphere, embedded within 100-dimensional ambient space. To start with, we simulate vectors \((\pmb{v}_1, \dots, \pmb{v}_N)^\prime\) for \(N=10,000\) points from a 4-dimensional Gaussian mixture model with two mixture components, \(\mathcal{N}(\pmb{\mu}_1, \pmb{\Sigma}_1)\) and \(\mathcal{N}(\pmb{\mu}_2, \pmb{\Sigma}_2)\), with the same means \(\pmb{\mu}_1 = \pmb{\mu}_2 =(0, 0, 0, 0)^\prime\) and different variance-covariance matrices \(\pmb{\Sigma}_1 = diag(1,1,1,1)\) and \(\pmb{\Sigma}_2 = diag(2,2,2,2)\). The mixture proportions are set as \(\pi_1=0.99\) and \(\pi_2=0.01\). With this design, the observations from the second component tend to be outlying anomalies. The data are mapped to a hemisphere via the equation
\(v_1^2 + v_2^2 + v_3^2 + v_4^2 + v_5^2 = r^2\) where \(v_5>0\) and \(r\) is set as \(8\).
\autoref{fig:fivedmeta} shows scatterplot which is a single frame from a 5-D tour path\footnote{See the animation of the 5-D grand tour at \url{https://github.com/ffancheng/kderm/blob/master/paper/figures/tourr_5d_animation.gif}.} animation using the R package \emph{tourr} \autocite{Wickham2011-ir}. The round and triangular point shapes indicate the two mixture components \(\mathcal{N}(\pmb{\mu}_1, \pmb{\Sigma}_1)\) and
\(\mathcal{N}(\pmb{\mu}_2, \pmb{\sigma}_2)\), and the colors indicate the distance between each point and the centre of the distribution. It can be seen that the most distant points are in darker colors and triangular shapes, meaning that the most anomalous observations are generated from the second mixture component.

To embed the 5-D hyper-semisphere into 100-D space, we append 95 zero columns to \(\pmb{v}_i\) so that \(\pmb{v}_i=(v_1, \dots, v_5, 0, \dots, 0), i=1,\dots,N\). Next, we rotate the \(100\)-dimensional vectors \((\pmb{v}_1, \dots, \pmb{v}_N)^\prime\) by multiplying by a randomly generated rotation matrix. To generate the rotation matrix we first simulate elements from a uniform \((0,1)\) distribution, stack them into a \(100\times 100\) matrix \(\pmb{A}\) and then take the R matrix from the QR decomposition of \(\pmb{A}\). Rotating the vectors results in input vectors that are no longer sparse. Nonetheless, the intrinsic dimension of this is still \(d=4\).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/sim4d10000_density_comparison_4ml_radius10_k200_rankdensity_circleoutlier_with1rec} 

}

\caption{Rank comparison between the true density and estimated density from both DC-KDE and KDE. Four manifold learning methods are used rowwise. The point shapes indicates whether they are the true outleirs, and the grey shading highlights the top 1\% rank region. The colors show the distance to the center of the semisphere, with darker points being distant from the center.}\label{fig:fivedisomapden}
\end{figure}

Following similar steps to \autoref{twodgaussian}, we estimate the densities of the 4-D manifold and compare them with the ground truth density. The bandwidth parameter for the DC-KDE estimator is set as \(r=1\) for this example. In \autoref{tab:fivedcors}, the rank correlations between true densities and estimated densites from DC-KDE and KDE are presented. For ISOMAP and LLE, KDE without distortion correction has a slightly higher rank correlation with the true density than for KDE with distortion correction, but these differences are negligible and both density estimators have a very high correlation of more than 0.96. However, for Laplacian Eigenmaps and UMAP there more distortion is induced throughdimension reduction, the rank correlation between estimated and true densities is close to 0 when distortion correction is not applied. In the same settings, the corresponding rank correlations are relatively high at 0.87 and 0.78 for Laplacian Eigenmaps and UMAP respectively,when distortion correction is applied.

Figure \autoref{fig:fivedisomapden} is constructed in a similar fashion to \autoref{fig:tpisomapden} to show the effect of distortion correction on the detection of anomalies. As in \autoref{fig:fivedmeta}, the point shapes indicate which of the two mixture Gaussian mixture components an observation was generated from and the colors indicate the distance from the centre of the distribution with outliers shown in darker colors. The ranks of the estimated densities are shown against ranks of true densities (on the log scale) with panels on the left showing distortion corrected KDE and panels on the right showing KDE without distortion correction. The panels from top to bottom show four dimension reduction algorithms: ISOMAP, LLE, Laplacian Eigenmaps and UMAP. Note that we exclude t-SNE algorithm in this section because it is designed mainly for low-dimensional visualization purposes and is only applicable to an embedding dimension less than or equal to three.

Comparing the left and right panel for each row, we notice that KDE with distortion correction, has fewer misclassified observations, and therefore outperforms KDE. For UMAP, almost all ground truth anomalies (points outside a \(99\%\) HDR) are not correctly detected using KDE, while almost all anomalies are correctly detected after correcting for distortion. For DC-KDE in the left panel there are several triangles in a horizontal line, this arises,due to many points having an estimated density of close to zero (values less than \(10^{-7}\)) which leads to tied ranks. These low-density points are all detected as anomalies for all dimension reduction algorithms, which again shows the robustness of DC-KDE.

\begin{table}

\caption{\label{tab:fivedcors}Correlation between true density and estimated density for four manifold learning embeddings.}
\centering
\begin{tabular}[t]{l>{}l>{}l>{}l>{}l}
\toprule
  & ISOMAP & LLE & Laplacian.Eigenmaps & UMAP\\
\midrule
DC-KDE & 0.968 & 0.970 & \textbf{0.8674} & \textbf{0.782}\\
KDE & \textbf{0.976} & \textbf{0.971} & 0.0328 & -0.181\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:fourdhdrtable}Percentage comparison of correct highest density regions in density estimation of four manifold learning embeddings.}
\centering
\begin{tabular}[t]{l|>{}c>{}c|>{}c>{}c|>{}c>{}c|>{}cc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{ISOMAP} & \multicolumn{2}{c}{LLE} & \multicolumn{2}{c}{Laplacian Eigenmaps} & \multicolumn{2}{c}{UMAP} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5} \cmidrule(l{3pt}r{3pt}){6-7} \cmidrule(l{3pt}r{3pt}){8-9}
  & DC-KDE & KDE & DC-KDE & KDE & DC-KDE & KDE & DC-KDE & KDE\\
\midrule
>99\% HDR & \cellcolor[HTML]{E6E6E6}{\textbf{0.830}} & 0.490 & \cellcolor[HTML]{E6E6E6}{\textbf{0.830}} & 0.240 & \cellcolor[HTML]{E6E6E6}{\textbf{0.840}} & 0.840 & \cellcolor[HTML]{E6E6E6}{\textbf{0.770}} & 0.000\\
99\% HDR & \cellcolor[HTML]{E6E6E6}{\textbf{0.818}} & 0.685 & \cellcolor[HTML]{E6E6E6}{\textbf{0.805}} & 0.478 & \cellcolor[HTML]{E6E6E6}{\textbf{0.815}} & 0.595 & \cellcolor[HTML]{E6E6E6}{\textbf{0.632}} & 0.032\\
\bottomrule
\end{tabular}
\end{table}

\autoref{tab:fourdhdrtable} further illustrates the percentage of correctly classified anomalies in the \(>99\%\) and \(99\%\) highest density regions. Compared with KDE, the percentages are always higher when distortion correction methods are used. The percentages with distortion correction are very close for ISOMAP, LLE and Laplacian Eigenmaps with a value of around \(83\%\) while slightly lower for UMAP at around \(77\%\). This could be due to the severe distortion usually induced by the UMAP algorithm.

\hypertarget{application}{%
\section{Application}\label{application}}

In this application, we use the smart meter data from the \emph{CER Smart Metering Project - Electricity Customer Behaviour Trial, 2009-2010} in Ireland \autocite{cer2012-data} between 14 July 2009 and 31 December 2010. The CER dataset\footnote{accessed via the Irish Social Science Data Archive - www.ucd.ie/issda.} records the half-hourly electricity consumption of individual residential and commercial properties, but not including energy for cooling or heating systems. We selected the \(3,639\) residential data with no missing values during the data collection period for a total of \(535\) days.

For the electricity consumption data of residential individuals, it is worthwhile to explore the distribution of electricity demand rather than the raw consumption data, so as to study the usage patterns of different households or different periods or the week \autocite{Hyndman2018-ia}. This can be considered as a case of dimension reduction on a statistical manifold, that is a manifold with elements that are probability distributions. \textcite{Cheng2021-dh} propose estimators of the Total Variation Metric and Hellinger distance between distributions that can be used in a computationally practical manner for dimension reduction on statistical manifolds. Again, we use ISOMAP, LLE, Laplacian Eigenmaps, t-SNE and UMAP for dimension reduction to obtain a 2 dimensional embedding for kernel density estimation and anomaly detection. To this end, we compare the density estimates from KDE with and without distortion correction and show how robust the anomalies are to different dimension reduction algorithms, only when distortion correction is used. We use the highest density regions plot to visualize the density estimates. However, for this real data set with unknown structure, the ground truth densities are unknown and it is not possible to tell which anomalies are the true ones.

Although full details for data processing and dimension reduction are provided in \textcite{Cheng2021-dh}, we briefly describe the process here. For each household, a discrete approximation to the distribution of electricity demand at each one of the \(336\) half-hourly periods of the week is found. For any pair of households the total variation metric can found between the distributions corresponding to any half hour of the week, and summing over these gives a distance measure between the pair of households, subsequently used for dimension reduction algorithms and the Learn matric algorithm. The bandwidth parameter \(r\) is set as 1 in Equation \eqref{eq:denestimator}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/kde_2densities_raw_electricity} 

}

\caption{Electricity usage plots of all 535 days for the most typical household and two anomalies in rows and KDE with distortion correction (left panel) and KDE (right panel) in columns.}\label{fig:electricityoutliers}
\end{figure}

\autoref{fig:electricityoutliers} shows the electricity usage data of three households for density estimation both with and without distortion correction respectively, with the top one being the most typical household with the highest density and the bottom two being the top two outliers with the lowest densities.
The typical households in the top row are similar except that there are a few spikes for household \(1472\) (selected as most typical when using KDE without distortion correction). As for the anomalies, distortion corrected KDE tends to capture instance of unusual electricity demand where usage is very low with some sudden spikes such as ID 3243 or households with a very high baseline electricity usage only across a sub-period of time such as ID 2838. In contrast, KDE without distortion correction can select households that do not exhibit anomalous behavior at all, such as household 5146 and the selection of such houesholds as anomalies may be an artefact of distortion induced by a dimension reduction algorithm.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/hdr_electricity_hdrbox_3id_7dow} 

}

\caption{Quantile region plots of electricity demand against the time of week for one typical household 1321 and two anomalies, 3243 and 2838. The quantile regions displayed in the plot are 99.0\%, 95.0\%, 75\%, and 50\%. Household 1321 has a repeated period-of-the-week electricity usage pattern slightly different for weekdays and weekends.}\label{fig:electricityhdrsfixed}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/electricity_hdrbox_3id_7dow} 

}

\caption{Quantile region plots of electricity demand against the time of week for one typical household 1472 and two anomalies, 7049 and 5136. The day-of-the-week patterns is much more different for Household 7049 compared with 5136, the latter of which does not have the obvious meal time related usage patterens.}\label{fig:electricityhdrsvkde}
\end{figure}

Further insights could be gained by comparing the quantile region plots of electricity demand against the time of the week for the same typical or anomalous households in \autoref{fig:electricityhdrsfixed} and \autoref{fig:electricityhdrsvkde} which are respectively selected when distortion correction is and is not applied. Again the distribution of both typical households in the top panel has shown a repeated period-of-the-week usage pattern, with higher usage during mealtime on all seven days of the week and slightly higher usage for weekends.
This repeated pattern in a week window is also obvious for the typical household ID 1321 from DC-KDE. As for the households selected when distortion correction is applied, the middle row in \autoref{fig:electricityhdrsfixed} shows a household with small spikes at noon but only on Tuesday and Sunday. The anomalies detected without distortion correction show an increasing electricity demand across the day of the week. These findings show the difference between anomalies detected with different kernel density estimation methods.

\hypertarget{conclusion}{%
\section{Conclusions}\label{conclusion}}

In this paper, we propose a novel distortion correction method to estimate the density of an embedding from manifold learning algorithms and further identify outliers based on the densities. Compared with KDE, our distortion corrected KDE makes use of geometric information for each data point to correct the distortion induced by the embedding. The Riemannian metric is estimated with the Learn Metric algorithm to approximate the geodesic distance and volume density function locally at each point. We compare our proposed method with KDE by two simulation settings, a 2-D manifold embedded as a 3-D twin peaks shape and a 4-D manifold mapped in a 100-D ambient space, and show that DC-KDE could generate more accurate kernel density estimation is more robust to the choice of dimension redution algorithm.

As an empirical example, we explore the distributions of \(3,639\) households and \(336\) time periods of the week in the Irish smart meter data. Five manifold learning algorithms, including ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP, are applied to get the 2-D embedding, followed by density estimation with KDE and DC-KDE. Without the ground truth density, we compare both density estimates by looking at the distributions of the most typical households with the highest densities and the anomalous households with the top two lowest densities. Both methods could identify the typical households with certain usage patterns, while the outliers are anomalous in different ways.

There are several open questions to be explored. The first involves the selection of bandwidth parameters for kernel density estimation, which has been explored in many KDE-related literatures. The second one is related to the quality of the embedding from dimension reduction methods. Although our distortion correction method is fairly robust to different choices of manifold learning methods, in certain cases, when the data structure is too complex and the distortion is too severe to correct, the quantitive relationship between embedding quality and density estimation accuracy is not immediate clear. The embedding quality could be measured using one of the metrics discussed in the online supplementary material of \textcite{Cheng2021-dh}, but when the ground truth densities are unknown, which is usually the case with real-world data set, it is hard to tell wherther the distortion are corrected in the right way. The density estimates on the edges of the whole data structure could alos be explored because most outer area points tend to be detected as outliers. However, the outperformance of DC-KDE than KDE has been shown in the higher dimensional simulation data and the electricity usage data, which are more related to real-life data sets.

\hypertarget{acknowledgment}{%
\section*{Acknowledgment}\label{acknowledgment}}
\addcontentsline{toc}{section}{Acknowledgment}

This research was supported in part by the Monash eResearch Centre and eSolutions-Research Support Services through the use of the MonARCH HPC Cluster. The first author acknowledges the financial support of the Monash Graduate Scholarship (MGS) and the Monash International Tuition Scholarship (MITS) at the Monash University.

\appendix

\hypertarget{riemgeo}{%
\section{Appendix: Notions about Riemannian geometry}\label{riemgeo}}

In this appendix, we present some notions about the Riemannian geometry used in this paper.

\hypertarget{differentiable-manifolds}{%
\subsection{Differentiable manifolds}\label{differentiable-manifolds}}

In topology, a \emph{homeomorphism} is a bijective map between two topological spaces that is continuous in both directions. A \emph{Hausdorff space} is a topological space where any two distinct points can be separated by disjoint neighborhoods. And a \(d\)-dimensional (topological) \emph{manifold} \(M\) is a connected Hausdorff space \((M, \mathcal{T}_M)\) where the neighborhood \(U\) for each point \(p\) is homeomorphic to an open subset \(V\) of the Euclidean space \(\mathbb{R}^d\). Such a homeomorphism \(\varphi: U \rightarrow V\) together with \(U\) gives a (coordinate) \emph{chart}, denoted as \((U, \varphi)\), with the corresponding local coordinates \((x^1(p),\dots, x^d(p)) := \varphi(p)\). Further, a collection of charts \(\{U_\alpha, \varphi_\alpha\}\) ranging over the manifold \(M\) is called an \emph{atlas}, denoted as \(\mathcal{A}\).

The manifold \(M\) is a \emph{differentiable manifold} if there exists an atlas of \(M\), \(\{U_\alpha, \varphi_\alpha\}\), such that the \emph{transition maps} between any two charts,
\[
\varphi_\beta \circ \varphi_\alpha^{-1}: \varphi_\alpha(U_\alpha \cap U_\beta) \rightarrow \varphi_\beta(U_\alpha \cap U_\beta),
\] are differentiable of class \(C^\infty\) (smooth).
Let \(\varphi\) be an injective map: \(E \rightarrow \varphi(E)\). Then \(\varphi\) is an \emph{embedding} of \(E\) into \(M\) if and only if
\(\varphi: E \rightarrow \varphi(E)\) is a homeomorphism, and \(\varphi(E)\) is called an embedded submanifold of \(M\) with the subspace topology.

\hypertarget{tangent-vector-and-tangent-space}{%
\subsection{Tangent vector and tangent space}\label{tangent-vector-and-tangent-space}}

The tangent vector at point \(p\) can be intuitively viewed as the velocity of a curve passing through point \(p\) or as the directional derivatives at \(p\). Here we define the tangent vector via the velocity of curves.

For any point \(p \in M\), let \(\gamma_1: (-\epsilon_1, \epsilon_1)\rightarrow M\) and \(\gamma_2: (-\epsilon_2, \epsilon_2)\rightarrow M\) be two smooth curves passing through \(p\), i.e.~\(\gamma_1(0) = \gamma_2(0) = p\). We say \(\gamma_1\) and \(\gamma_2\) are \emph{equivalent} if and only if there exists a chart \((U,\varphi)\) at \(p\) such that
\[
(\varphi \circ \gamma_1)^\prime(0) = (\varphi \circ \gamma_2)^\prime(0).
\]
A \emph{tangent vector} to a manifold \(M\) at point \(p\), denoted as \(v_p\), is any equivalent class of the differentiable curves initialized at \(p\). The set of all tangent vectors at \(p\) defines the \emph{tangent space} of \(M\) at \(p\), denoted as \(T_pM\). The tangent space is a vector space of dimension \(d\), equal to the dimension of \(M\), and it does not depend on the chart \(\varphi\) locally at \(p\). The collection of all tangent spaces defines the \emph{tangent bundle}, \(TM = \cup_{p \in M}T_pM\).

Tangent vectors can also be seen as the directional derivatives at \(p\). For a given coordinate chart \(\varphi=(x^1,\dots,x^d)\), the tangent vectors defining partial
derivatives are denoted as \(\frac{\partial}{\partial x^1}(p),\dots,\frac{\partial}{\partial x^d}(p)\), which defines a \emph{basis} of the tangent space.
The tangent space \(T_pM\) also admits a dual space \(T^\star_pM\) called the \emph{cotangent space} with the corresponding \emph{cotangent vectors} \(z_p: T^\star_pM \rightarrow \mathbb{R}^d\) and a basis denoted as \(dx^1(p),\dots,dx^d(p)\).

\hypertarget{riemannian-metric-and-geodesic-distance}{%
\subsection{Riemannian metric and geodesic distance}\label{riemannian-metric-and-geodesic-distance}}

A Riemannian metric \(g_p\) defined on the tangent space \(T_pM\) at each point \(p\) is a local inner product \(T_pM \times T_pM \rightarrow \mathbb{R}\), where \(g_p\) is a \(d\times d\) symmetric positive definite matrix and varies smoothly at \(p\). Generally, we
omit the subscript \(p\) and refer to \(g\) as the Riemannian metric. The inner product between two vectors \(u, v \in T_pM\) is written as \(\langle u, v \rangle_g = g_{ij}u^iv^j\) using the Einstein summation convention where implicit summation over all indices, \(\sum_{i,j} g_{ij}u^iv^j\), is assumed. A differentiable manifold \(M\) endowed with the Riemannian metric \(g\) on each tangent space \(T_pM\) is called a \emph{Riemannian manifold} \((M,g)\).

The Riemannian metric \(g\) can be used to define the norm of a vector \(u\), \(\|u\| = \sqrt{\langle u,v \rangle_g}\), and the angle between two vectors \(u\) and \(v\), \(\cos\theta = \frac{\langle u,v \rangle_g}{\|u\| \|v\|}\), which are the geometric quantities induced by \(g\). It could also be used to define the line element \(dl^2 = g_{ij}dx^i dx^j\) and the volume element \(dV_g = \sqrt{\det(g)}dx^1 \dots dx^d\), where \((x^1,\dots,x^d)\) are the local coordinates of the chart \((U, \varphi)\).
For a curve \(\gamma: I \rightarrow M\), the length of the curve is
\[
l(\gamma) = \sqrt{\int_0^1 \|\gamma^\prime(t)\|^2_g dt} = \sqrt{\int_0^1 g_{ij} \frac{dx^i}{dt} \frac{dx^j}{dt} dt},
\]
where \(\gamma(I) \subset U\). The volume of \(W \subset U\) is defined as
\[
Vol(W) = \int_W \sqrt{\det(g)}dx^1 \cdots dx^d,
\]
which is also called the \emph{Riemannian measure} on \(M\).

The \emph{geodesics} of \(M\) are the smooth curves that locally joins the points along the shortest path on the manifold. Intuitively, geodesics are the \emph{straightest possible curves} in a Riemannian manifold \autocite[Section 7.2.3 of][]{Nakahara2018-zs}.
A curve \(\gamma: I \rightarrow M\) is a geodesic if for all indices \(i,j,k\), the second-order ordinary differential equation is satisfied,
\[
\frac{d^2 x^i}{dt^2} + \Gamma^i_{jk} \frac{d x^j}{dt} \frac{dx^k}{dt} = 0,
\]
where \(\{x^i\}\) are the coordinates of the curve \(\gamma\) and \(\Gamma^i_{jk}\) is the \emph{Christoffel symbol} defined by
\[
\Gamma^i_{jk} = \frac{1}{2} \sum_l g^{il} (\frac{\partial g_{il}}{\partial x^k} 
+ \frac{\partial g_{kl}}{\partial x^j} - \frac{\partial g_{jk}}{\partial x^l}).
\]
The geodesics have a constant speed with norm \(\| \gamma^\prime(t) \|\), and they are the local minimizers of the arc
length functional \(l:\gamma \rightarrow \sqrt{\int_0^1 \| \gamma^\prime(t) \|_g^2 dt}\) when the curves are defined over the interval \([0,1]\).
The \emph{geodesic distance} \(d_g\) is the length of the shortest geodesic between two points on the manifold. For a point \(p \in M\), when the geodesic distance starting at \(p\) is not minimized, we call such set of points the \emph{cut locus} of \(p\), and the distance to the cut locus is the \emph{injectivity radius} at \(p \in M\). Therefore, the injectivity radius of the Riemannian manifold \((M,g)\), \(\textit{inj}_gM\), is the infimum of the injectivity radii over all points on the manifold.

\hypertarget{exponential-map-and-logarithmic-map}{%
\subsection{Exponential map and logarithmic map}\label{exponential-map-and-logarithmic-map}}

Denote \(B(p, r) \subset M\) as an open ball centered at point \(p\) with raidus \(r\). Then \(B(0_p, r) = exp_p^{-1}(B(p,r))\) is an open neighborhood of \(0_p\) in the tangent space at \(p\), \(T_pM\), where \(exp_p\) is the \emph{exponential map} at point \(p\). The exponential map maps a tangent vector \(u \in B(0_p, r)\) to the endpoint of the geodesic \(\gamma: I \rightarrow M\) satisfying \(\gamma(0)=p, \gamma^{\prime}(0)=u\), and \(\gamma(1)=exp_p(u)\). It is a differentiable bijective map of differentiable inverse (i.e.~\emph{diffeomorphism}). Intuitively, the exponential map moves point \(p\) to an endpoint at speed \(u\) after covering the length of \(\|u\|\) along the geodesic in one time unit.

The inverse of the exponential map is called the \emph{logarithm map}, denoted as \(\log_p(q):= \exp^{-1}_p(q)\), which gives the tangent vector to get from point \(p\) to \(q\) in one unit time. Also define the \emph{geodesic ball} centered at \(p\) of radius \(r > 0\) as the image by the exponential map of \(B(0_p, r) \subset T_pM\) with \(r < \textit{inj}_gM\). Then we could interpolate a geodesic \(\gamma\) between two points \(p\) and \(q\) with the exponential map and the logarithmic map, \(\gamma(t) = \exp_p(t\log_p(q))\), and the geodesic distance is given by \(d_g(p,q) = \|\log_p(q)\|_g\).

\hypertarget{pushforward-and-pullback-metric}{%
\subsection{Pushforward and pullback metric}\label{pushforward-and-pullback-metric}}

Pushforward and pullback are two notions corresponding to the notions of tangent and cotangent vectors.
Let \(\phi: M \rightarrow E\) be a smooth map between the Riemannian manifold \((M,g)\) to another smooth manifold \(E\). Then the differential of \(\phi\) at point \(p\) is a linear map \(d\phi_p: T_pM \rightarrow T_{\phi (p)}E\), which pushes the tangent vector \(u \in T_pM\) at point \(p\) forward to the tangent vector \(\phi_*u \in T_{\phi (p)}E\) at the mapping point \(\phi(p)\).
The image of the tangent vector \(u \in T_pM\) under the differential \(d\phi_p\), denoted as \(d\phi_p u\) is called the pushforward of \(u\) by the map \(\phi\).
Then pushforward metric \(h=\varphi_*g\) of the Riemannian metric \(g\) along \(\varphi\) is given by the inner product
\[
\langle \phi_*u,\phi_*v \rangle_{\varphi*g} = \langle d\phi_p \phi_*u, d\phi_p \phi_*v \rangle_{g}.
\]

The tangent vectors \(\phi_*u\) are equivalent to the velocity vector of a curve \(\gamma: I\rightarrow M\) passing through point \(p\) at time zero with a constant speed \(\gamma^{\prime}(0)=u\),
\[
d\phi_p(\gamma^{\prime}(0)) = (\phi \circ \gamma)^\prime (0).
\]
Similarly, the pullback maps the cotangent vectors \(z_{f(p)}\) at \(f(p) \in E\) to cotangent vectors at \(p \in M\) acting on tangent vectors \(u \in T_pM\). The linear map is called the pullback by \(\phi\) and is often denoted as \(\phi^*\).

\hypertarget{twinpeaksappe}{%
\section{Appendix: Rank comparison plots for twin peaks mapping}\label{twinpeaksappe}}

This appendix contains the comparison plots for the density rank between DC-KDE and KDE using different manifold learning algorithms, similar to \autoref{fig:tpisomapden}, \autoref{fig:tptsneden}, and \autoref{fig:tpisomapvs4ml}. By comparing these plots, it could be concluded that DC-KDE could categorize the density ranks into highest density regions more accurately than KDE. By correcting the distortion in different manifold learning embeddings, DC-KDE is more robust in identifying the lowest density regions, which are usually used to detect anomalies.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/Twin Peak2000_densityrank_comparison_lle_radius8_r0_5_logrank_rec_colprob_smallblocks3_crossfalse} 

}

\caption{Scatterplot of true density and estimated density ranks of LLE embedding for DC-KDE and KDE, with colors indicating the absolute rank errors weighted by the sum of true and estimated ranks. DC-KDE shows a strong linear positive relationship with a higher rank correlation compared to KDE.}\label{fig:tplleden}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/Twin Peak2000_densityrank_comparison_le_radius8_r0_5_logrank_rec_colprob_smallblocks3_crossfalse} 

}

\caption{Scatterplot of true density and estimated density ranks of Laplaxian Eigenmaps embedding for DC-KDE and KDE, with colors indicating the absolute rank errors weighted by the sum of true and estimated ranks. DC-KDE shows a strong linear positive relationship with a higher rank correlation compared to KDE.}\label{fig:tpleden}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/Twin Peak2000_densityrank_comparison_umap_radius8_r0_5_logrank_rec_colprob_smallblocks3_crossfalse} 

}

\caption{Scatterplot of true density and estimated density ranks of UMAP embedding for DC-KDE and KDE, with colors indicating the absolute rank errors weighted by the sum of true and estimated ranks. DC-KDE shows a strong linear positive relationship with a higher rank correlation compared to KDE.}\label{fig:tpumapden}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak2000_density_compare_llevs4ml_radius8_r0_5_rank} 

}

\caption{Comparison of outliers found by one manifold learning method compared to the other four for DC-KDE (on the left panel) and KDE (on the right panel). The four colors and shapes represents the four gaussian kernels in the 2-D meta data. Outliers found by DC-KDE are more consistent regardless of the manifold learning embedding.}\label{fig:tpllevs4ml-1}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak2000_density_compare_levs4ml_radius8_r0_5_rank} 

}

\caption{Comparison of outliers found by one manifold learning method compared to the other four for DC-KDE (on the left panel) and KDE (on the right panel). The four colors and shapes represents the four gaussian kernels in the 2-D meta data. Outliers found by DC-KDE are more consistent regardless of the manifold learning embedding.}\label{fig:tpllevs4ml-2}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak2000_density_compare_tsnevs4ml_radius8_r0_5_rank} 

}

\caption{Comparison of outliers found by one manifold learning method compared to the other four for DC-KDE (on the left panel) and KDE (on the right panel). The four colors and shapes represents the four gaussian kernels in the 2-D meta data. Outliers found by DC-KDE are more consistent regardless of the manifold learning embedding.}\label{fig:tpllevs4ml-3}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak2000_density_compare_umapvs4ml_radius8_r0_5_rank} 

}

\caption{Comparison of outliers found by one manifold learning method compared to the other four for DC-KDE (on the left panel) and KDE (on the right panel). The four colors and shapes represents the four gaussian kernels in the 2-D meta data. Outliers found by DC-KDE are more consistent regardless of the manifold learning embedding.}\label{fig:tpllevs4ml-4}
\end{figure}

\printbibliography

\end{document}
