\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Anomaly detection with kernel density estimation on manifolds},
            pdfkeywords={manifold learning, variable bandwidth, Riemannian metric, highest density region, Gaussian kernels},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Anomaly detection with kernel density estimation on manifolds}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\RequirePackage{bera}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\subsection}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}
        \placefig{2}{1.5}{width=5cm}{monash2}
        \placefig{16.9}{1.5}{width=2.1cm}{MBusSchool}
        \begin{textblock}{4}(16.9,4)ISSN 1440-771X\end{textblock}
        \begin{textblock}{7}(12.7,27.9)\hfill
        \includegraphics[height=0.7cm]{AACSB}~~~
        \includegraphics[height=0.7cm]{EQUIS}~~~
        \includegraphics[height=0.7cm]{AMBA}
        \end{textblock}
        \vspace*{2cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \footnotesize http://monash.edu/business/ebs/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}\vspace*{2cm}}}
\def\pageone{{\sffamily\setstretch{1}%
        \thispagestyle{empty}%
        \vbox to \textheight{%
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false, date=year}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}


\nojel

\RequirePackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}
\def\placefig#1#2#3#4{\begin{textblock}{.1}(#1,#2)\rlap{\includegraphics[#3]{#4}}\end{textblock}}


\nocover
\blind



\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf\@date}
\makeatother

%% Any special functions or other packages can be loaded here.
\usepackage{tabu,placeins,algorithmic}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\urlstyle{tt}  % use monospace font for urls
\usepackage[cal = txupr]{mathalpha}
\geometry{margin=2.2cm}
\mathtoolsset{showonlyrefs}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

% Adjust headwidth in case user has changed geometry in header-includes
\renewcommand{\headwidth}{\textwidth}

\begin{document}
\maketitle
\begin{keywords}
manifold learning, variable bandwidth, Riemannian metric, highest density region, Gaussian kernels
\end{keywords}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In manifold learning, the underlying idea is that the data lies on a low-dimensional smooth manifold which is embedded in a high-dimensional space. One of the fundamental objectives of manifold learning is to explore the geometry of the dataset, including the distances between points and volumes of regions of data. These intrinsic geometric attributes of the data, such as distances, angles, and areas, however, can be distorted in the low-dimensional embedding, leading to failure to recover the geometry of the manifold \autocite{Goldberg2008-co}. To tackle this problem and measure the distortion incurred in manifold learning, \textcite{Perrault-Joncas2013-pq} propose the Metric Learning algorithm to augment any existing embedding output with geometric information in the Riemannian metric of the manifold itself. By applying the Metric Learning algorithm, the outputs of different manifold learning methods can be unified and compared under the same framework, which would highly benefit in improving the effectiveness of the embedding.

The Riemannian metric defined at each point of the manifold is used to compute the geometric quantities, including angle, length, and volume, of the low-dimensional manifold embedding in any coordinate system, and be further applied to correct the distortion caused by the manifold learning algorithms.
In variable kernel density estimate, the bandwidth matrix \(H\) is also defined to control the amount of smoothing for each data point.
Therefore, if we could replace the bandwidth matrix with the Riemannian metric, we could further get the kernel density estimation of the manifold \(\mathcal{M}\). This kernel density estimate can then be used to produce the highest density region plots \autocite{Hyndman1996-lk} for anomaly detection.

By applying an existing manifold learning algorithm to the data \(X\in \mathbb{R}^r\) with \(n\) observations, a low-dimensional embedding \(f_n \in \mathbb{R}^d\) can be computed. Most manifold learning methods involve the construction of the nearest neighbor graph based on which the Laplace-Beltrami operator \(\Delta_\mathcal{M}\) is built. The Laplacian is quite useful because it can be coordinate-free while containing all the important geometry. \textcite{Perrault-Joncas2013-pq} have stated one way to compute the approximated \(\Delta_\mathcal{M}\) with a discrete consistent estimator, the geometric graph Laplacian \(\mathcal{L}_{\varepsilon,n}\) \autocite{Zhou2011-za}, where \(\varepsilon\) is the radius parameter for the nearest neighbor graph. The graph Laplacian together with the embedding can be used in the Metric Learning algorithm to achieve the augmented embedding with the Riemannian metric \((f_n, g_n)\). The highlighted two steps in Figure \ref{fig:vkde} are the main contributions of this main chapter, replacing the bandwidth matrix \(H_i\) with the Riemannian metric \(g_i\) for each point in variable kernel density estimate, and computing the highest density region plots based on the density estimates, \(\hat{f}(\mathbb{x})\), for anomaly detection.

The rest of the paper is organized as follows.
In \autoref{vkde}, we present the proposed algorithm to detect anomalies based on variable kernel density estimates of manifold embeddings. In this section, we provide justification for the use of Riemannien matrix as the bandwidth of variable kernel density estimation, including the comparison with fixed bandwidth.
\autoref{simulation} is composed with two simulations with the proposed algorithm; the first deals with a 2-dimensional meta data embedded in 3-D and the second with a 100-dimensional meta data.
\autoref{application} contains the application to visualize and identify anomalies in the {[}TODO{]} dataset.
Conclusions and discussions are presented in \autoref{conclusion}.

\hypertarget{vkde}{%
\section{Variable kernel density estimation with manifold embeddings}\label{vkde}}

In this section, we introduce the proposed method to detect anomalies based on the kernel density estimates of manifold learning embeddings where the Riemannian matrix is used as the pointwise variable bandwidth to measure the direction and angle of the distortion of the low-dimensional embeddings. For a high-dimensional data set, various manifold learning algorithms including ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP, are applied to get a low-dimensional embedding. The manifold learning algorithms map the points through nonlinear functions that stretches some regions of the space while shrinks others. \textcite{Perrault-Joncas2013-pq} gives us an idea of how to measure the direction and angle of the distortion using the Riemannian metric and the Riemannian metric is a positive simi-definite square matrix for each data point. To learn the distribution of the low-dimensional embedding, we use the kernel density estimation with the bandwidth matrix being the Riemannian metric. The outliers could then be defined as the points with lowest density estimates. The proposed schematic is shown in Figure \ref{fig:vkde}.



\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/vkde} 

}

\caption{The proposed schematic for variable kernel density estimation with recovered geometry.}\label{fig:vkde}
\end{figure}

To start with, we introduce the notations in this manuscript. Then we introduce the multivariate kernel density estimation method with variable bandwidth matrix and the metric learning algorithm to derive the pointwise Riemannian metrix. Finally, we propose our novel method to detect anomalies for high-dimensional data set.

\hypertarget{notations}{%
\subsection{Notations}\label{notations}}

Manifold learning finds a \(d\)-dimensional representation of data that lie on a manifold \(\mathcal{M}\) embedded in a \(p\)-dimensional ambient space with \(d \ll p\). We will denote the original data (or `input' points) as \(x_i\), \(i=1,\dots,N\), where \(x_i\in\mathbb{R}^p\), while the low-dimensional representation (or `output' points) will be denoted as \(y_i\), \(i=1,\dots,N\), where \(y_i\in\mathbb{R}^d\). Where two subscripts are used (e.g.~\(x_{ih}\) or \(y_{ih}\)), the second subscript refers to the \(h^{th}\) coordinate or dimension of the data.

\hypertarget{two-dimensional-kernel-density-estimation}{%
\subsection{Two dimensional kernel density estimation}\label{two-dimensional-kernel-density-estimation}}

For a bivariate random sample \(\pmb X_1, \pmb X_2, \ldots, \pmb X_n\)
drawn from a density \(f\),
the kernel density estimate is defined by
\[
\hat{f} (\pmb x; \pmb H) = n^{-1}\sum_{i=1}^n K_{\pmb H} ( \pmb x - \pmb{X}_i)
\]
where \(\pmb x = (x_1, x_2)^T\) and \(\pmb{X}_i = (X_{i1}, X_{i2})^T, i = 1, 2, \ldots, n\). Here
\(K(\pmb x)\) is the kernel which is a symmetric probability density function,
\(\pmb H\) is the bandwidth matrix which is symmetric and positive-definite,\\
and \(K_{\pmb H}(\pmb x) = |\pmb H|^{-1/2} K( \pmb H^{-1/2} \pmb x)\).
The choice of \(K\) is not crucial: we take
\(K(\pmb x) = (2\pi)^{-1} \exp(-\tfrac{1}{2} \pmb x^T \pmb x)\) the standard normal
throughout.\\
In contrast, the choice of \(\pmb H\) is crucial in determining the performance of \(\hat f\).
The most common parameterizations of the bandwidth matrix
are the diagonal and the
general or unconstrained which has no restrictions on \(\pmb H\)
provided that \(\pmb H\) remains positive definite and symmetric, that is
\[
\pmb H = \begin{bmatrix}h_1^2 & 0 \\0 & h_2^2 \end{bmatrix}
\ \mathrm{or} \ 
\pmb H = \begin{bmatrix}h_1^2 & h_{12} \\ h_{12}  & h_2^2 \end{bmatrix}.
\]
This latter parameterization allows kernels to have an arbitrary orientation
whereas the former only allows kernels which are oriented to the
co-ordinate axes.

An alternative way to think of kernel estimation is that kernel densities `borrow strength' from nearby points and the bandwidth determines what is ``nearby''. If the bandwidth is large then all points are ``nearby'' and we get an overly smooth kernel density estimate. The interesting thing about a bandwidth matrix is that it allows for different notions of what is ``nearby'' along different coordinates and even along diagonal directions.

\hypertarget{use-riemannian-matrix-as-variable-bandwidth}{%
\subsection{Use riemannian matrix as variable bandwidth}\label{use-riemannian-matrix-as-variable-bandwidth}}

The Riemannian estimated using the method of Perrault Joncas and Meila (2009) gives some idea of the distortion of an embedding (or so they claim). Mapping the points through a non-linear function ``stretches'' some regions of space and ``shrinks'' others. The Riemannian gives us an idea of the direction and angle of this stretching. The Riemannian is quite a technical concept but an important thing to understand is that the estimate that comes out of Perrault Joncas algorithm is a square matrix.

We saw how points that are far apart in the embedding may not have been so far apart on the original manifold. The Riemannian gives us some way of correcting this. Similarly a bandwidth matrix in a kernel density estimate is all about determining the ``directions'' in which there should be more or less ``closeness''. So the basic idea is to replace the kernel density estimate with

\[
\hat{f}(\pmb{x})=\sum\limits_{i=1}^N K_{\pmb{H}_i}(\pmb{x}-\pmb{x}_i)\\
K_{\pmb{H}_i}(\pmb{x}-\pmb{x}_i)=(2\pi)^{-d/2}|\pmb{H}_i|^{-1/2}\exp\left[-\frac{1}{2}(\pmb{x}-\pmb{x}_i)'\pmb{H}_i^{-1}(\pmb{x}-\pmb{x}_i)\right]
\]

where \(H_i\) is either the Riemannian or the inverse of the Riemannian (I am not totally sure which one). Notice that the bandwidth matrix is different for each point. This makes it a kernel density estimate with local smoothing, which is quite interesting, but we should take care to understand the properties of such things.\\

The Riemannian metric \(g\) is a symmetric and positive definite tensor field which defines an inner product \(<,>_g\) on the tangent space \(T_p\mathcal{M}\) for every point \(p \in \mathcal{M}\).
If the inner product of the tangent space is known for a given geometry, the Riemannian metric is a good measure to recover the geometry of manifold.
The Metric Learning algorithm \autocite{Perrault-Joncas2013-pq} then augment the embedded manifold with the Riemannian metric and produce a Riemannian manifold \((\mathcal{M}, g)\).

To recover the original geometry of the manifold, we need to know what the inner product corresponds to in the embedding.
The inner product between two vectors \(u,v \in T_p\mathcal{M}\), \(<u,v>_g=g_{ij}u^iv^j\)\footnote{Here the Einstein notation is used where superscripts denote summation over \(i\) and \(j\)}, can be used to define some geometric quantities, such as the vector norm \(\|u\|=\sqrt{<u,v>_g}\) and the angle between two vectors \(\cos{\theta}=\frac{<u,v>_g}{\|u\|\|v\|}\) in the tangent space. Therefore, for each point \(p\in \mathcal{M}\) in any coordinate system, the Riemannian metric \(g\) is a \(d\times d\) symmetric positive definite matrix, where \(d\) is the dimension of the manifold.

The line element and volume element of the full manifold or a subset of the manifold can also be computed from \(g\). The arc length of a curve \(c\in \mathcal{M}\) is defined as
\[
l(c)=\int_a^b \sqrt{g_{ij} \frac{dx^i}{dt} \frac{dx^j}{dt}} dt,
\]
where \((x^1,\dots,x^d)\) are the coordinates of chart \((U,x)\) and \(c(t)\) is a function mapping \([a,b]\) to \(\mathcal{M}\). While the volume of \(V\subset \mathcal{M}\) is computed by
\[
Vol(V)=\int_V \sqrt{\|g\|} dx^1\dots dx^d.
\]
Both the concepts of distance and volume are relevant to kernel density estimation.

The aim of the project is not to evaluate different manifold learning algorithms. Although ISOMAP worked OK here it is still distorted and in real examples you will never `know' that the data are uniform on the sphere. Instead the idea of this project is to do kernel density estimation in a way that takes distortion into account.

\hypertarget{metric-learning-algorithm}{%
\subsubsection{Metric Learning algorithm}\label{metric-learning-algorithm}}

\textcite{Perrault-Joncas2013-pq} propose the Metric Learning algorithm which mainly involves four main steps.

As pointed out by \textcite{Perrault-Joncas2013-pq}, if the embedding dimension \(s\) is larger than the manifold intrinsic dimension \(d\), the rank of the embedding metric \(h_n(p)\) is \(d\); otherwise, the Riemannian metric \(g_n\) will be returned.
This algorithm is also implemented in a Python library \emph{megaman} \autocite{McQueen2016-xz}. It is designed to apply the manifold learning methods to large-scale data sets, as well as computing the Riemannian metric of the manifold.

Learn metric algorithm

\begin{algorithm}[!htb]
  \caption{Learn metric algorithm}
  \label{alg:learnmetric}
  \DontPrintSemicolon
  \SetAlgoLined
  \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}\SetKwInOut{Parameter}{parameter}\SetKwInOut{OptParameter}{optimization parameter}
  \Input{ high-dimensional data $x_i \in \pmb{R}^p$ for all $i=1,\ldots,N$ }
  \Output{ low-dimensional data $y_i \in \pmb{R}^d$ and its Riemannian metric $h_i$ for all $i=1,\ldots,N$ }
  \Parameter{ embedding dimension $d$, bandwidth parameter $\sqrt{\varepsilon}$, manifold learning algorithm }
  \OptParameter{ manifold learning parameters }
  \BlankLine
  \begin{algorithmic}[1]

  \STATE Construct a weighted neighborhood graph $\mathcal{G}_{w,\varepsilon}$ with weight matrix $W$ where $w_{i,j}=\exp(-\frac{1}{\varepsilon}\|x_i-x_j\|^2)$ for data points $x_i,x_j \in \mathbb{R}^p$;

  \STATE Calculate the $N\times N$ geometric graph Laplacian $\widetilde{\mathcal{L}}_{\varepsilon,N}$ by
  $$
  \widetilde{\mathcal{L}}_{\varepsilon,N} = 1/(c\varepsilon)(\widetilde{D}^{-1} \widetilde{W} - I_N),
  $$
  where $\widetilde{D}=diag{\widetilde{W}\pmb{1}}$, $\widetilde{W} = D^{-1}WD^{-1}$, and $D = diag{W\pmb{1}}$;

  \STATE Embed each data point $x\in \mathbb{R}^p$ to embedding coordinates $y(x)=(y^1(x),\dots,y^d(x))$ by any existing manifold learning algorithm;

  \STATE Obtain the matrix $\tilde{h}(x)$ at each point by applying the graph Laplacian $\widetilde{\mathcal{L}}_{\sqrt{\varepsilon},N}$ to the embedding coordinates $y$ with each element being
  $$
    \tilde{h}^{i j}=\frac{1}{2}\left[\tilde{\mathcal{L}}_{\varepsilon, N}\left(y_i \cdot y_j\right)-y_i \cdot\left(\tilde{\mathcal{L}}_{\varepsilon, n} y_j\right)-y_j \cdot\left(\tilde{\mathcal{L}}_{\varepsilon, n} y_i\right)\right];
  $$

  \STATE Calculate the Riemannian metric $h(x)$ as the rank $d$ pseudo inverse of $\tilde{h}(x)$ with 
  $$
    h(x) = U diag{1/(\Lambda[1:d])} U^\prime,
  $$
  where $[U, \Lambda]$ is the eigendecomposition of matrix $\tilde{h}(x)$, and $U$ is the matrix of column eigenvectors ordered by the eigenvalues $\Lambda$ in descending order.

  \end{algorithmic}
\end{algorithm}

\hypertarget{proposed-algorithm}{%
\subsection{Proposed algorithm}\label{proposed-algorithm}}

Now we present our proposed method for anomaly detection based on variable kernel density estimates.

\begin{algorithm}[!htb]
  \caption{Variable kernel density estimates with Riemannian metric}
  \label{alg:vkderm}
  \DontPrintSemicolon
  \SetAlgoLined
  \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}\SetKwInOut{Parameter}{parameter}
  \Input{ high-dimensional data $x_i$ for all $i=1,\ldots,N$ }
  \Output{ outliers embedding coordinates $y_1, \dots, y_{n\_outliers}$ with their estimated densities $f_1, \dots, f_{n\_outliers}$ }
  \Parameter{ number of outliers $n\_outliers$, embedding dimension $d$ }
  \BlankLine
  \begin{algorithmic}[1]

  \STATE For all $i=1,\ldots,N$, compute the $d$-dimensional embeddings $y_i$ with any exsiting manifold leanring algorithms and the corresponding Riemannian metric $h_i$ using the Learn metric algorithm with inputs $d$ and $\sqrt{\varepsilon} = 0.4$ and $c=0.25$ for heat kernels;

  \STATE Set the variable bandwidth for each point as $\pmb{H}_i = h_i$;

  \STATE Compute the kernel density estimates for each point as
  $$
  \hat{f}(\pmb{y})=\sum\limits_{i=1}^N (2\pi)^{-d/2}|\pmb{H}_i|^{-1/2}\exp\left[-\frac{1}{2}(\pmb{y}-\pmb{y}_i)'\pmb{H}_i^{-1}(\pmb{y}-\pmb{y}_i)\right];
  $$

  \STATE Reorder the embedding coordinates $y$ according to the density estimates $f(y)$ and subset the top \textit{n\_outliers} as the outliers.

  \end{algorithmic}
\end{algorithm}

Now that we have proposed a way to take into account the distortion of manifold in kernel density estimate, it would be straightforward to produce the highest density region plots {[}HDR plots; \textcite{Hyndman1996-lk}{]} which are also computed using kernel density estimate of the embedding.

\hypertarget{simulation}{%
\section{Simulation}\label{simulation}}

In this section, we exam two scenarios for both low and high dimensions to test our proposed algorithm. For visualization purpose, \autoref{twodgaussian} presents a 2-D meta data example. We first simulate the data of size \(N=2000\) from a mixture of four Gaussian kernels with the same covariance but different means, each consisting of \(500\) points. Different mapping functions are then applied to the 2-D meta data to be mapped in a 3-D feature space, which gives the higher-dimensional input for different manifold learning algorithms, including ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP. The embedded dimension is set as \(d=2\), same as the meta data dimension. This enables us to compare the manifold learning embedding with the true meta data. We could now apply the algorithm described in \ref{alg:vkderm} to get the density estimates of all data points and further detect anomalies. As a high-dimensional example, the second simulation in \autoref{fivedgaussian} is based on a 5-D meta data embedded in a 100-D space and the corresponding embedding dimension is \(d=5\).

\hypertarget{twodgaussian}{%
\subsection{2-D Meta data from a Gaussian Mixture Model}\label{twodgaussian}}

We first generate a 2-dimensional data of size \(N=2000\) from a Gaussian mixture model with four components with different means \(\pmb{\mu_1}=(0.25, 0.25)^\prime, \pmb{\mu_2}=(0.25, 0.75)^\prime, \pmb{\mu_3}=(0.75, 0.25)^\prime, \pmb{\mu_4}=(0.75, 0.75)^\prime\) and the same variance-covariance matrix \(\pmb{\Sigma}_i=diag(0.02, 0.02), i=1,2,3,4\). The mixture proportions are equally set as \(\pi_i=0.25, i=1,2,3,4\).
Then the mixture Gaussian mixture density function is a weighted linear combination of the four component Gaussian densities as
\begin{equation}
\label{eq:gmm}
P(\pmb{X}=\pmb{x}) = \sum_{i=1}^{4}\pi_i \frac{1}{(2\pi)^{d/2}|\pmb{\Sigma}_i|^{-1/2}} \exp{\{-\frac{1}{2} (\pmb{x}-\pmb{\mu_i})^\prime \pmb{\Sigma}_i^{-1} (\pmb{x}-\pmb{\mu_i}) \}}.
\end{equation}
\autoref{fig:metadensity} shows the 2-dimensional meta data and the colors indicate the true density of all data points calculated from \eqref{eq:gmm}, with brighter colors showing higher densities and darker colors showing lower densities. We then define outliers as points with lowest densities shown in black and typical points with highest densities shown in yellow. Based on the true density plot, the outliers are scattered in the middle and the outer area of the whole structure, while typical points are near the means of four kernels. This is the baseline, which we call \emph{true densities}, to be compared with outliers from the kernel density estimates.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/truedensity_4kernels} 

}

\caption{True density of the Gaussian mixture model of four kenels with means $(0.25, 0.25), (0.25, 0.75), (0.75, 0.25), (0.75, 0.75)$ and the same variance-covariance matrix $diag(0.02, 0.02)$}\label{fig:metadensity}
\end{figure}

Given the 2-D meta data, multiple mapping functions could be applied to embed the data in a 3-D space. One of the most famous example in manifold learning is the swiss roll data, with the mapping function in \eqref{eq:swissroll}. The two-dimensional meta data \((\pmb{X}_1, \pmb{X}_2)\) is transformed into the three-dimensional data \((\pmb{X}, \pmb{Y}, \pmb{Z})\), shown in the left plot of \autoref{fig:mappings}.
\begin{equation}
\label{eq:swissroll}
\left\{ 
\begin{array}{lcl}
X = X_1 \cos{X_1}, \\
Y = X_2, \\
Z = X_1 \sin{X_1}.
\end{array}
\right.
\end{equation}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/mappings_sr_tp} 

}

\caption{3-D Mappings of the meta data with colors indicating four kernels. Left: swiss roll mapping. Right: twin peak mapping.}\label{fig:mappings}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Swiss Roll_outliers_comparison_4ml_3cases_riem001} 

}

\caption{Highest density region plots of four manifold learning embeddings of the swiss roll data. Colors are indicating densities from top: true densities from the Gaussian mixture model; middle: KDE with Riemannian matrix as variable bandwidth; and bottom: KDE with fixed bandwidth. Variable KDE preforms better in finding kernel structures with ISOMAP and LLE, and in locating outliers with t-SNE. Both methods are not detecting outliers accurately when using UMAP.}\label{fig:sroutliers}
\end{figure}

Now we are able to apply different manifold learning algorithms to \((\pmb{X}, \pmb{Y}, \pmb{Z})\) and reduce the dimension back to \(d=2\), and further estimate the density of the 2-D embedding. According to the density estimates, we could rank the data points and then identify which observations lie in an highest density region of specified coverage, eg. 1\%, 50\%, 99\%, \textgreater99\%. For each of the four manifold learning methods, namely ISOMAP, LLE, t-SNE, and UMAP, \autoref{fig:sroutliers} presents the 2-D embedding plot in the same column, with the colors indicating the densities levels, the top row for true densities from the Gaussian mixture model, the middle row for highest density region plots with densities from our proposed variable KDE method, and the bottom for similar HDR plots with densities from KDE with fixed bandwidth. The top ten outliers with lowest densities are also highlighted in black with point indexes in blue.
Since there are four kernels in the meta data and darker points are the defined outliers, we could derive that for ISOMAP and LLE, variable kernel density estimates could detect the number of kernels more accurately than the KDE with fixed bandwidth. As for t-SNE and UMAP, both embeddings are quite close to each other with points clustered together, but the mixture of kernel structure is not as clear as LLE. Instead, we could roughly compare the location of outliers. For t-SNE, the middle plot have indexed more true outliers than the bottom plot; while for UMAP, both the middle and bottom plot fail to find the true outliers, but the outliers from fixed bandwidth are more scattered along the curved embedding.

\hypertarget{twin-peaks-mapping}{%
\subsubsection{Twin peaks mapping}\label{twin-peaks-mapping}}

For comparison, we use the same meta data with different mapping function, twin peak mapping in Equation \eqref{eq:twinpeak}, with the corresponding 3-D data shown in the right plot of \autoref{fig:mappings}. The four colors in both mappings represents the four gaussian kernels used to generate the meta data \((\pmb{X}_1, \pmb{X}_2)\).
\begin{equation}
\label{eq:twinpeak}
\left\{ 
\begin{array}{lcl}
X = X_1, \\
Y = X_2, \\
Z = \sin(\pi X_1) \tanh (3 X_2).
\end{array}
\right.
\end{equation}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak_outliers_comparison_4ml_3cases_riem01} 

}

\caption{Highest density region plots of four manifold learning embeddings of the twin peak data. Variable KDE preforms better in finding kernel structures with ISOMAP and LLE, and in locating outliers with t-SNE and UMAP.}\label{fig:tpoutliers}
\end{figure}

Similar to \autoref{fig:sroutliers}, four manifold learning embeddings are obtained and used to detect outliers with two bandwidth selection methods in \autoref{fig:tpoutliers}.
{[}ANALYSE TWIN PEAK PLOT{]}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak_density_comparison_isomap_riem01} 

}

\caption{Scatterplot of true density and estimated density of ISOMAP embedding for KDE with both variable and fixed bandwidth. The four colors represents the four gaussian kernels in the 2-D meta data.}\label{fig:tpisomapden}
\end{figure}

With severe distortion in the t-SNE and UMAP embeddings, both variable and fixed bandwidth methods can not produce accurate density estimates. It would then be very difficult to uncover the Gaussian mixture model structure and

\begin{table}

\caption{\label{tab:tpcors}Correlation between true density and estimated density for four manifold learning embeddings.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{6cm}>{}l>{}l>{}l>{}l}
\toprule
  & ISOMAP & LLE & t.SNE & UMAP\\
\midrule
Variable bandwidth & \textbf{0.940} & \textbf{0.407} & 0.520 & 0.259\\
Fixed bandwidth & 0.626 & 0.399 & \textbf{0.660} & \textbf{0.655}\\
\bottomrule
\end{tabular}}
\end{table}

\hypertarget{fivedgaussian}{%
\subsection{5-D data embedding in 100-D space}\label{fivedgaussian}}

For the high-dimensional case, we generate the meta data from a 5-dimensional semi-hypersphere. First, we simulate \(N=2000\) points from a 4-dimensional Gaussian mixture model with two mixture components, \(\mathcal{N}(\pmb{\mu}_1, \pmb{\Sigma}_1)\) and \(\mathcal{N}(\pmb{\mu}_2, \pmb{\sigma}_2)\), where \(\pmb{\mu}_1 = \pmb{\mu}_2 =(0, 0, 0, 0)^\prime\), \(\pmb{\Sigma}_1 = diag(1,1,1,1)\), and \(\pmb{\Sigma}_1 = diag(25,25,25,25)\). In order to manually generate anomalies, the mixture proportions are set as \(\pi_1=0.99\) and \(\pi_2=0.01\).
The fifth dimension is calculated to satisfy \(x_1^2 + x_2^2 + x_3^2 + x_4^2 + x_5^2 = r^2\) where \(r\) is set as 15. Similarly, the Gaussian mixture densities could be calculated using Equation \eqref{eq:gmm} as the true density of the 5-d meta data. \autoref{fig:fivedmeta} shows a scatterplot when animating a 5-D tour path with the R package \emph{tourr} {[}REFERENCE{]}.
Then we initial the other 95 dimensions in the high-dimensional space as zero columns and further rotate the 100-dimensional data of size \(N\) (denote the transpose of the data matrix as \(X\)) to get rid of the zeros so that it could be passed to the manifold learning algorithms.
The rotation matrix is derived from the QR decomposition of a \(100\times 100\) matrix \(A\) with all components randomly generated from a uniform distribution \(\mathcal{U}(0,1)\).
For any real matrix \(A\) of dimension \(p\times q\), the QR decomposition could decompose the matrix into the multiplication of two matrix \(Q\) and \(R\) so that \(A = QR\), where the dimension of \(Q\) is a matrix with unit norm orthogonal vectors, \(Q^\prime Q = I\), and \(R\) is an upper triangular matrix. Matrix \(Q\) is used as the rotation matrix and it satisfies
\(X^\prime X=(QX)^\prime(QX)\), where the rotated data matrix \((QX)^\prime\) is now the input for the manifold learning algorithms.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/tourr_5d_semisphere} 

}

\caption{Scatterplot display of the animation of a 5-D tour path.}\label{fig:fivedmeta}
\end{figure}

Similar to \autoref{fig:sroutliers}, four manifold learning embeddings are obtained and used to detect outliers with two bandwidth selection methods in \autoref{fig:tpoutliers}.
{[}ANALYSE TWIN PEAK PLOT{]}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/fived_density_comparison_isomap_riem20} 

}

\caption{Scatterplot of true density and estimated density of different embeddings for KDE with both variable and fixed bandwidth. Top: ISOMAP; middle: LLE; bottom: UMAP.}\label{fig:fivedisomapden}
\end{figure}

\begin{table}

\caption{\label{tab:fivedcors}Correlation between true density and estimated density for four manifold learning embeddings.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{6cm}>{}l>{}l>{}l}
\toprule
  & ISOMAP & LLE & UMAP\\
\midrule
Variable bandwidth & \textbf{0.921} & \textbf{0.981} & \textbf{-0.130}\\
Fixed bandwidth & 0.806 & 0.940 & -0.341\\
\bottomrule
\end{tabular}}
\end{table}

\hypertarget{application}{%
\section{Application}\label{application}}

\hypertarget{victoria-smart-meter-dataset}{%
\subsection{Victoria smart meter dataset}\label{victoria-smart-meter-dataset}}

{[}TODO{]}

Next, we consider smart-meter data for residential and non-profiled meter consumers, collected in the \emph{CER Smart Metering Project - Electricity Customer Behaviour Trial, 2009-2010} in Ireland \autocite{cer2012-data}. Electricity smart meters record consumption, on a near real-time basis, at the level of individual commercial and residential properties. The CER dataset\footnote{accessed via the Irish Social Science Data Archive - www.ucd.ie/issda.} does not include energy for heating systems since it is either metered separately, or households use a different source of energy, such as oil or gas. In this study, the installed cooling systems are also not reported.

We use measurements of half-hourly electricity consumption gathered from 3,639 residential consumers over 535 consecutive days. Every meter provides electricity consumption between 14 July 2009 and 31 December 2010. Demand data from two smart meters (ID 1003 and 1539) are shown in \autoref{fig:smartmeter} as time series plots. It is obvious that these meters have relatively different patterns. Meter 1539 (bottom of \autoref{fig:smartmeter}) has a period of around 150 days with lower (approximately half) the electricity usage of remaining days and is otherwise relatively stable. In contrast, Meter 1003 (top of \autoref{fig:smartmeter}) exhibits regular spikes on weekends.

For electricity demand data, one particular problem of interest is to visualize and identify households or periods of the week with anomalous usage patterns. For this reason, the object of interest in comparing households or periods of the week is the distribution of electricity demand rather than the raw data itself \autocite{Hyndman2018-ia}. An additional advantage of looking at distributions rather than raw data is that it provides a convenient mechanism for handling missing data which can be a significant problem in smart meter data applications.
In the rest of this section, we first describe how discrete approximations to the distributions of interest are computed. We then approximate the distances between vectors containing probabilities with the consistent estimator in \autoref{estimator}, which allows us to apply manifold learning techniques on statistical manifolds.

Let \(d_{i,t}\) denote the electricity demand for observation \(i\) and for time period \(t\) (subsequently we will see that \(i\) can either index the household, time of the week, or both, while \(t\) may index the week or half-hour period). The objective is to estimate the distances between the distribution of electricity demand for observation \(i\), \(F_i\), over time. Since the raw data could be thought of as samples generated from the true electricity usage distributions for different households and different periods, we could apply the consistent Hellinger distance estimator derived in \autoref{estimator} to the smart meter data.
The first step is to group all data together regardless of \(i\) and \(t\), and take the data as samples from the reference probability distribution \(R\). The sample size \(r=93,616,914\). Next, we construct \(T_r=100\) intervals with about \(1\%\) samples from \(R\) within each interval (except for the last interval). The segments \(\{I_g^r\}_{g=1,2,\dots,100}\) can then be used to construct discrete distribution approximation to \(F_i\) by counting the ratio of samples points from \(F_i\) fallen into segment \(I_g^r\).
This is found by computing \(\pi_{i,g}=(1/m)\sum_t I(d_{i,t} \in I_g^r)\) where \(m\) is the sample size from \(F_i\).
Then the Hellinger distance between \(F_i\) and \(F_j\) could be estimated using Equation \eqref{eq:hellingerest1}.
In this way, we make it possible to search nearest neighbors more efficiently using ANN methods and further apply manifold learning algorithms.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Summary of the contribution and conclusion

Details about the process

Shortcoming and other references for future direction (improvement on density estimation, other kernels, densities on edges, bad embeddings with inaccurate distortions, multi-dimensional kernel density estimate).

Discuss simulation with non-uniform meta data.

\printbibliography

\end{document}
