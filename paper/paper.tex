\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Distortion corrected kernel density estimate on Riemannian manifolds},
            pdfkeywords={manifold learning, variable bandwidth, Riemannian metric, geodesic distance, Gaussian kernels},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Distortion corrected kernel density estimate on Riemannian manifolds}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\RequirePackage{bera}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\subsection}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}
        \placefig{2}{1.5}{width=5cm}{monash2}
        \placefig{16.9}{1.5}{width=2.1cm}{MBusSchool}
        \begin{textblock}{4}(16.9,4)ISSN 1440-771X\end{textblock}
        \begin{textblock}{7}(12.7,27.9)\hfill
        \includegraphics[height=0.7cm]{AACSB}~~~
        \includegraphics[height=0.7cm]{EQUIS}~~~
        \includegraphics[height=0.7cm]{AMBA}
        \end{textblock}
        \vspace*{2cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \footnotesize http://monash.edu/business/ebs/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}\vspace*{2cm}}}
\def\pageone{{\sffamily\setstretch{1}%
        \thispagestyle{empty}%
        \vbox to \textheight{%
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false, date=year}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}


\nojel

\RequirePackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}
\def\placefig#1#2#3#4{\begin{textblock}{.1}(#1,#2)\rlap{\includegraphics[#3]{#4}}\end{textblock}}


\nocover

\author{Fan~Cheng, Anastasios~Panagiotelis, Rob J~Hyndman}
\addresses{\textbf{Fan Cheng}\newline
Monash University
\newline{Email: \href{mailto:Fan.Cheng@monash.edu}{\nolinkurl{Fan.Cheng@monash.edu}}}\\[1cm]
\textbf{Anastasios Panagiotelis}\newline
The University of Sydney
\newline{Email: \href{mailto:Anastasios.Panagiotelis@sydney.edu.au}{\nolinkurl{Anastasios.Panagiotelis@sydney.edu.au}}}\\[1cm]
\textbf{Rob J Hyndman}\newline
Monash University
\newline{Email: \href{mailto:Rob.Hyndman@monash.edu}{\nolinkurl{Rob.Hyndman@monash.edu}}}\\[1cm]
}

\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf Cheng, Panagiotelis, Hyndman: \@date}
\makeatother

%% Any special functions or other packages can be loaded here.
\usepackage{tabu,placeins,algorithmic}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\urlstyle{tt}  % use monospace font for urls
\usepackage[cal = txupr]{mathalpha}
\geometry{margin=2.2cm}
\mathtoolsset{showonlyrefs}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
%\def\equationautorefname{Equation} % NOT showing Equation 1, but Section 2.1
\usepackage{bm}
\DeclareMathOperator\supp{supp}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

% Adjust headwidth in case user has changed geometry in header-includes
\renewcommand{\headwidth}{\textwidth}

\begin{document}
\maketitle
\begin{abstract}
Manifold learning can be used to obtain a low-dimensional representation of the underlying Riemannian manifold given the high-dimensional data. However, kernel density estimates of the low-dimensional embedding with a fixed bandwidth fail to account for the way manifold learning algorithms distort the geometry of the Riemannian manifold. We propose a novel distortion-corrected kernel density estimator (DC-KDE) for any manifold learning embedding by introducing the estimated Riemannian metric of each point to fix the distortion in the line and volume elements. The geometric information of the manifold guarantees a more accurate density estimation of the true manifold, which subsequently could be used for anomaly detection. To compare our proposed estimator with a fixed-bandwidth kernel density estimator, we run two simulations with a 2-D data from Gaussian mixture model mapped into a 3-D twin peaks shape and a 5-D semi-hypersphere mapped in a 100-D space. We demonstrate that the proposed DC-KDE could improve the density estimates given a good manifold learning embedding and has higher rank correlations with the true manifold density. A shiny app in R is also developed for various simulation scenarios. The proposed method is applied to density estimation in statistical manifolds of electricity usage with the Irish smart meter data. This demonstrates the estimator's capability to fix the distortion of the manifold geometry and a new approach to anomaly detection in high-dimensional data.
\end{abstract}
\begin{keywords}
manifold learning, variable bandwidth, Riemannian metric, geodesic distance, Gaussian kernels
\end{keywords}

\newpage

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Multivariate kernel density estimation has gained lots of attention in exploratory data analysis. It is a non-parametric technique to estimate the data density based on weighted kernels centered at the data which usually belongs to a subset of \(\mathbb{R}^d\). Applications of kernel density estimation {[}KDE; \textcite{Parzen1962-gt}; \textcite{Chen2017-dw}{]} include finding hot spots of traffic network in the GIS environment \autocite{Xie2008-eb,Okabe2009-nb}, automatic detection in visual surveillance systems \autocite{Elgammal2002-cw}, wind power density detection \autocite{Jeon2012-ac}, prime prediction via Twitter messages \autocite{Gerber2014-tq}, and so on. However, when samples are assumed to be drawn from a Riemannian manifold embedded in a high-dimensional space of much more than \(\mathbb{R}^d\), kernel density estimation has to be generalized to a non-Euclidean space and further approximation methods have to be adapted. \textcite{Pelletier2005-vu} propose a kernel density estimator based on the Riemannian geodesic distance of the manifold but it is only applicable when the underlying manifold is known. Manifold learning algorithms could be applied to reduce the dimension and get an approximation of the manifold, but different manifold learning algorithms could induce different distortions of the same manifold. Therefore, we propose a distortion-corrected kernel density estimator for Riemannian manifolds embedded in more than \(\mathbb{R}^d\). Our estimator could be applied to any reasonable manifold learning embedding from the high-dimensional sample data and fix the distortions at each point with estimated Riemannian geodesic distance and volume density function. This estimator could be further applied for unsupervised tasks such as anomaly detection where the outliers are the lowest density points.

For a given kernel function, kernel density estimation is flexible to learn the shape of the underlying density of the data controlled by the bandwidth and the selection of bandwidth is crucial in KDE \autocite{Jones1990-oe,Terrell1992-ut}.
Many bandwidth selection methods have been proposed in the literature, including the rule-of-thumb, cross-validation \autocite{Jones1992-ta,Sain1994-gr} and plug-in methods \autocites[See][]{Heidenreich2013-bl}[ for details]{Scott2015-vl}.
For univariate kernel density estimation, the bandwidth selection problem has been thoroughly investigated \autocites[See][]{Jones1992-ef,Cao1994-st,Jones1996-cb}[ for reviews]{Wand1994-xu}. The generalization to multivariate case could mostly be found in \textcite{Duong2003-sp}, \textcite{Duong2004-rh}, and \textcite{Chacon2010-wm}. In this paper, we focus on the multivariate kernel density estimation.

Note that a fixed bandwidth matrix \(\pmb{H}\) is a global smoothing parameter for all data points. However, when the local data structure is not universal for all sample data, which is true in most applications, an adaptive bandwidth matrix that is varying rather than fixed at each data point is needed.
The bandwidth is varied depending on either the location of the sample points {[}sample smoothing estimator; \textcite{Terrell1992-ut}{]} or that of the estimated points {[}balloon estimator; \textcite{Terrell1992-ut}{]}. In this paper, the densities are estimated at the sample points themselves, so we only need to consider the case where the bandwidth changes for each sample point and will refer to this as the \emph{variable/adaptive kernel density estimation} {[}VKDE; Section 6.6 of \textcite{Scott2015-vl}{]} unless otherwise stated.
However, these kernel density estimators are based on random samples in the Euclidean space.

For samples points lying on a manifold with the differentiable structure called the Riemannian manifold, \textcite{Pelletier2005-vu} generalize the kernel density estimator based on the kernel weights from the geodesic distance between the estimated points and the sample points. The idea of the estimator is to use a strictly positive function of the geodesic distance on the manifold and then normalize it with the volume density function of the Riemannian manifold for curvature \autocite{Henry2009-ll}.
However, in many application scenarios, we tend to find that the sample points are not drawn directly from the manifolds because they are embedded in a much higher-dimensional space. Therefore, the kernel density estimator from \textcite{Pelletier2005-vu} is not applicable because the geodesic distance and the volume density function are unknown.
This is when we introduce manifold learning to reduce the input data dimension. For these high-dimensional data set, various manifold learning algorithms including ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP (see details of these algorithms in \textcite{Cheng2021-ex}), could be applied to get a low-dimensional embedding, which are used as approximations of the underlying manifold.

In manifold learning, the underlying idea is that the data lies on a low-dimensional smooth manifold that is embedded in a high-dimensional space. One of the fundamental objectives of manifold learning is to explore the geometry of the dataset, including the distances between points and volumes of regions of data. These intrinsic geometric attributes of the data, such as distances, angles, and areas, however, can be distorted in the low-dimensional embedding, leading to failure in recovering the geometry of the manifold \autocite{Goldberg2008-co}. To tackle this problem and measure the distortion incurred in manifold learning,
\textcite{Perrault-Joncas2013-pq} propose the Learn Metric algorithm to augment any existing embedding output with geometric information in the Riemannian metric of the manifold itself. By applying this algorithm, the outputs of different manifold learning methods can be unified and compared under the same framework, which would highly benefit in improving the effectiveness of the embedding.
The Riemannian metric using the method of \textcite{Perrault-Joncas2013-pq} gives some idea of the distortion of an embedding. Mapping the points through a non-linear function ``stretches'' some regions of space and ``shrinks'' others. The Riemannian gives us an idea of the direction and angle of this stretching at each point, which is informative for learning the manifold.

By exploiting the connection between the estimated Riemannian metric and the Riemannian geodesic distance as well as the volume density function for curvature, we propose the main contribution of the paper, which is the variable distortion-corrected kernel density estimator (DC-KDE) for manifold learning embedding. Starting from the high-dimensional sample data, we apply manifold learning algorithms to get the low-dimensional embedding in the same dimensional space as the underlying manifold together with the estimated Riemannian matrix at each embedding point. Then the DC-KDE is used to estimate the density of the manifold and distortions induced by manifold learning methods are fixed with the estimated geometric information. Our distortion-corrected estimator is novel in filling the gap between the high-dimensional sample space and the density of the unknown manifold. These density estimates are useful in many other areas, including classification, clustering and anomaly detection. Similar to \textcite{Cheng2021-ex}, the highest density region plots\autocite{Hyndman1996-lk} could be generated using the kernel density estimates for outlier visualization, which brings a novel anomaly detection method for Riemannian manifolds.

The rest of the paper is organized as follows. In \autoref{kderm}, we present our distortion-corrected kernel density estimator for Riemannian manifolds. We start by introducing the multivariate kernel density estimate with adaptive bandwidth and the kernel density estimator for Riemannian manifolds. Then we provide justification for the use of Riemannian metric to correct the distortions in manifold learning embedding and further apply the proposed estimator for anomaly detection. \autoref{simulation} is composed of two simulations with the proposed anomaly detection algorithm; the first deals with 2-dimensional data from gaussian mixture model mapped into a 3-D twin peaks structure and the second with a 5-D semi-hypersphere data mapped in a 100-D space. Different manifold learning algorithms are applied to the high-dimensional data to get the low-dimensional embedding which are then used to estimate densities and detect anomalies.
\autoref{application} contains the application to visualize and identify anomalous households in the Irish smart meter dataset. Conclusions and discussions are presented in \autoref{conclusion}. Readers interested in the notions of Riemannian geometry mentioned in this paper could use \autoref{riemgeo} as a reference.

\hypertarget{kderm}{%
\section{Distortion Corrected Kernel density estimate on Riemannian manifolds}\label{kderm}}

In this section, we introduce our method for kernel density estimation on manifolds that uses an embedding from a dimension reduction algorithm while correcting for the distortion induced by this embedding. Some readers familiar with kernel density estimation may not be as familiar with the nuances of manifolds. Therefore, before introducing our own estimator, we first discuss kernel density estimation for data in Euclidean space, then illustrate in \autoref{Pellet} how this generalizes to the estimator of \textcite{Pelletier2005-vu}, when the data lie on some known manifold. In \autoref{MetLearn}, we describe the Learn Metric algorithm of \textcite{Perrault-Joncas2013-pq}, which augments an embedding derived from a dimension reduction algorithm with an estimate of the Riemannian metric expressed in local coordinates. By combining elements from the work of \textcite{Pelletier2005-vu} and \textcite{Perrault-Joncas2013-pq}, we derive our own novel distortion corrected kernel density estimate in \autoref{DCKDE}. To keep this section as succinct as possible, we do not define concepts such as manifolds, charts, geodesic distance etc., but provide this information for readers unfamiliar with differential geometry in \autoref{riemgeo}.

In the following, we denote \(M\) as the \(d\)-dimensional manifold from which our data are sampled. Points on this manifold are denoted \(\pmb{p}\) in general, with \(\pmb{p}_1,\dots,\pmb{p}_n\) denoting the observed sample. Often \(\pmb{p}_i\) will be high-dimensional vectors such that \(\pmb{p}_i\in\mathbb{R}^m\) with \(m\gg d\). However, it does not need to be the case. For instance, \(\pmb{p}_i\) may be probability distributions on a statistical manifold. The methods we propose for estimating the density at each \(\pmb{p}_i\) only require some sense of distance between the `input' points, \(d(\pmb{p}_i,\pmb{p}_j)\), such that we can apply dimension reduction algorithms to obtain an `output' embedding, \(\pmb{y}_1,\dots,\pmb{y}_n\), where \(\pmb{y}_i\in\mathbb{R}^d\). We will denote this embedding as \(\pmb{y}_i=\psi(\pmb{p}_i)\). Finally, we denote by \(\lambda\) the Lebesgue measure of \(\mathbb{R}^d\), letting \(\|\cdot\|\) be the usual Euclidean norm and following \textcite{Pelletier2005-vu}, we make these assumptions about the kernel function \(K:\mathbb{R}_+\rightarrow\mathbb{R}\),
\begin{align}
\label{eq:kernelcondition}
& (i) \int_{\mathbb{R}^{d}} K(\|\pmb{y}\|) \mathrm{d} \lambda(\pmb{y})=1;
(ii) \int_{\mathbb{R}^{d}} \pmb{y} K(\|\pmb{y}\|) \mathrm{d} \lambda(\pmb{y})=0;
(iii) \int_{\mathbb{R}^{d}}\|\pmb{y}\|^{2} K(\|\pmb{y}\|) \mathrm{d} \lambda(\pmb{y})<\infty; \\
& (iv) \supp K=[0; 1];
(v) \sup K(\|\pmb{y}\|)=K(0).
\end{align}
Note that these conditions are different from (and in some cases stricter than) those normally used for kernel density estimation. For instance, condition (iv) requires the support of the kernel to be bounded. The reasons for this will become clearer when we discuss the manifold setting in more detail. Also, for illustration purposes, in this section we pay particular attention to the uniform kernel for which \(K(z)\) equals one if \(0\leq z\leq1\) and zero otherwise. In our empirical section, more general kernel functions can be, and are, employed.

For data \(\pmb{y}_i\in\mathbb{R}^d\) with \(i=1,\dots,N\) and assuming a bandwidth matrix \(r\pmb{I}\) where \(r\) is a global bandwidth, then the usual kernel density estimator at a point \(\pmb{y}\) is given by
\begin{equation}
\label{eq:vkde}
\hat{f}(\pmb{y})=\frac{1}{N}\sum\limits_{i=1}^N \frac{1}{r^d} K\left(\frac{\|\pmb{y}-\pmb{y}_i\|}{r}\right).
\end{equation}

The intuition behind this estimator is very clear for a uniform kernel. The density at a point \(\pmb{y}\) is equal to the proportion of sample points that lie within a ball of radius \(r\) centered at \(\pmb{y}\), times a term that ensures the density integrates to 1. In general, the bandwidth matrix need not be proportional to the identity matrix. However, the intuition remains the same, only that the ball of radius \(r\) centered at \(\pmb{y}\) is found with respect to Mahalanobis distance rather than the usual Euclidean distance. For more on kernel density estimation in the Euclidean case, see \textcite{Scott2015-vl} and references therein.

While one could in principle use a standard kernel density on the output from a dimension reduction algorithm, it must be noted that the density of the output vectors \(\pmb{y}_i\) is different from the density on the manifold itself. As a non-linear transformation, any dimension reduction algorithm `distorts' the density. Furthermore, any analysis based on density estimates of the output embedding will be dependent on the choice of dimension reduction algorithm since each different algorithm will distort the density in a different way.

\hypertarget{Pellet}{%
\subsection{Kernel Density estimation on manifolds}\label{Pellet}}

For kernel density estimation on a known manifold, \textcite{Pelletier2005-vu} propose the following estimator,
\begin{equation}
\label{eq:denriem}
\hat{f}(\pmb{p}) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{r^d \theta_{\pmb{p}_i}(\pmb{p})} K\left(\frac{d_g(\pmb{p}, \pmb{p}_i)}{r}\right),
\end{equation}

where \(d_g(\pmb{p}, \pmb{p}_i)\) denotes the geodesic distance between two points on the manifold \(\pmb{p}\) and \(\pmb{p}_i\) and \(\theta_{\pmb{p}_i}(\pmb{p})\) is known as the volume density function. The intuition behind the term \(K\left(\frac{d_g(\pmb{p}, \pmb{p}_i)}{r}\right)\) is relatively clear. For example, for a uniform kernel, the estimator at point \(\pmb{p}\) will still depend on the proportion of sample points within a ball of radius \(r\) centered at \(\pmb{p}\). However in this case, the geodesic distance on the manifold is used, rather than Euclidean or Mahalanobis distance. An additional technical assumption is that \(r\) is less than the injectivity radius of the manifold. A definition of the injectivity radius is given by \textcite{Chavel2006-mp} and also provided in the appendix. For our purposes, it is sufficient to note that this assumption precludes the possibility that the radius of a ball around \(\pmb{p}\) is so large that some points `fall inside' the ball more than once. For example on a sphere, a ball with radius greater than half the circumference of a great circle will wrap back around the sphere. This phenomenon also explains why the kernel function must be bounded for density estimation on manifolds.

The inclusion of the volume density function \(\theta_{\pmb{p}_i}(\pmb{p})\) is perhaps not as immediately clear, therefore, before providing formal details, we will briefly discuss the intuition behind this term. We have already highlighted that when using a uniform kernel, the kernel density estimate at a point \(\pmb{p}\) directly depends on the proportion of sample points within a ball of radius \(r\) around \(\pmb{p}\). However, the volume of this ball must also be taken into account. In Euclidean space with the usual Lebesgue measure, a radius \(r\) ball will always have the same volume regardless of its center. The same does not hold for manifolds and the volume density function ensures that the density estimate integrates to one.

More formally, the volume density function can be explained as follows. Consider the exponential map around \(\pmb{p}\), given by \(exp_{\pmb{p}}(\pmb{q})\), mapping vectors in the tangent space, \(\pmb{v}\in T_{\pmb{p}}M\), to points on the manifold, \(\pmb{q}\in M\). Loosely, \(\pmb{v}\) `points' in the direction of the geodesic between \(\pmb{p}\) and \(\pmb{q}\) and travels along this geodesic at uniform speed \(\|\pmb{v}\|\) in one unit of time. Now, consider a chart \(\varphi\) mapping points in the neighborhood of \(\pmb{p}\), via the inverse of the exponential map, to these \(\pmb{v}\) vectors, expressed in some local coordinate system. The volume density function is the square root of the determinant of the Riemannian metric expressed in this coordinate system. For more on the volume density function, see \textcite{Le_Brigant2019-lj}.

\hypertarget{MetLearn}{%
\subsection{Riemannian metric estimation}\label{MetLearn}}

To be able to apply the estimator of \textcite{Pelletier2005-vu} to the case where the manifold is not known, but where coordinates \(\pmb{y}_i\) for \(i=1,\dots,n\) are obtained from a dimension reduction algorithm, requires an estimate of the Riemannian metric in the coordinate system. Formally, the Riemannian metric \(g\) is a symmetric and positive definite tensor field which defines an inner product \(\langle,\rangle_g\) on the tangent space \(T_{\pmb{p}} M\) for every point \(\pmb{p} \in M\). The inner product between two tangent vectors \(u,v \in T_{\pmb{p}}M\), given by \(\langle u,v \rangle_g\), can be used to define geometric quantities. For example, angles on a manifold are given by \(\cos{\theta}=\frac{\langle u,v \rangle_g}{|u| |v|}\), while distances and volumes on manifolds are also defined with reference to the Riemannian metric. While the defined tangent vectors, the Riemannian metric and the geometric quantities are invariant to any specific choice of coordinates, they can still be expressed in terms of local coordinates systems. This is precisely the situation when data on a manifold are mapped to \(d\)-dimensional Euclidean vectors \(\pmb{y}_1,\pmb{y}_2\dots,\pmb{y}_n\) via a dimension reduction algorithm. After this mapping, angles, distances and volumes in this Euclidean `output space' are not the same as on the manifold since dimension reduction algorithms introduce distortions. To alleviate this issue, \textcite{Perrault-Joncas2013-pq} propose a method to augment \(\pmb{y}_1,\pmb{y}_2\dots,\pmb{y}_n\) with \(d\times d\) positive definite matrices, \(\pmb{H}_1,\pmb{H}_2\dots,\pmb{H}_n\), at each data point. These matrices estimate the Riemannian metric in local coordinates defined by the dimension reduction algorithm. For example, the angle between \(\pmb{p}_j\) and \(\pmb{p}_k\) at \(\pmb{p}_i\) depends (up to a first order approximation) on the inner product \((\pmb{y}_j-\pmb{y}_i)'\pmb{H}^{-1}_i(\pmb{y}_k-\pmb{y}_i)\) rather than the usual Euclidean inner product \((\pmb{y}_j-\pmb{y}_i)'(\pmb{y}_k-\pmb{y}_i)\).

While full details are provided in \textcite{Perrault-Joncas2013-pq}, we briefly describe the Learn Metric algorithm here. There are four main steps in the algorithm. First, a weighted neighborhood graph is constructed, with edges between \(\pmb{p}_i\) and \(\pmb{p}_j\) when \(\pmb{p}_i\) is a K-nearest neighbor of \(\pmb{p}_j\) or vice versa, and edge weights depending on the distance between \(\pmb{p}_i\) and \(\pmb{p}_j\) on the manifold. Second, the discrete Laplacian on this graph \(\hat{\mathcal{L}}_{\varepsilon,n}\) is estimated \autocite{Zhou2011-za}, where \(\varepsilon\) is the radius parameter for the nearest neighbor graph. Third, a dimension reduction method is applied to obtain the output embedding \(\pmb{y}_1,\dots,\pmb{y}_n\). Fourth, the Riemannian metric at each point is estimated by exploiting the connection between the Riemannian metric and the Laplace Beltrami operator (to which the graph Laplacian at Step 2 is a discrete estimator). Full details on these four steps are provided in \autoref{alg:learnmetric}. This algorithm is implemented in a Python library \emph{megaman} \autocite{McQueen2016-xz} although our own results are based on a re-implementation of the algorithm in \emph{R}.

As pointed out by \textcite{Perrault-Joncas2013-pq}, dimension reduction can be carried out such that the dimension of the output vectors is larger than the intrinsic manifold dimension \(d\). In this case, the ranks of the matrices \(\pmb{H}_i\) are equal to \(d\). Using a larger embedding dimension is justified since it is in general not possible to embed a manifold of dimension \(d\) globally into \(d\)-dimensional Euclidean space. In our simulated examples, we abstract from this issue by constructing examples that can be globally embedded into \(d\)-dimesional Euclidean space. In practice, to determine the dimension of the manifold, the \emph{two-nearest neighbor estimator (TWO-NN estimator)} \autocite{Facco2017-rl,Denti2021-jl} can be used. The \emph{R} library \emph{intRinsic} \autocite{Denti2021-qc} implements this algorithm and is used in all examples involving real data where the intrinsic dimension is unknown.

\begin{algorithm}[!htb]
  \caption{Learn metric algorithm in \cite{Perrault-Joncas2013-pq} }
  \label{alg:learnmetric}
  \DontPrintSemicolon
  \SetAlgoLined
  \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}\SetKwInOut{Parameter}{parameter}\SetKwInOut{OptParameter}{optimization parameter}
  \Input{ high-dimensional data $\pmb{x}_i \in \mathbb{R}^s$ for all $i=1,\ldots,N$ }
  \Output{ low-dimensional data $\pmb{y}_i \in \mathbb{R}^d$ and its Riemannian metric $\pmb{H}(\pmb{y}_i)$ for all $i=1,\ldots,N$ }
  \Parameter{ embedding dimension $d$, bandwidth parameter $\sqrt{\varepsilon}$, manifold learning algorithm }
  \OptParameter{ manifold learning parameters EMBED }
  \BlankLine
  \begin{algorithmic}[1]

  \STATE Construct a weighted neighborhood graph $\mathcal{G}_{w,\varepsilon}$ with weight matrix $\pmb{W}$ where $w_{i,j}=\exp(-\frac{1}{\varepsilon}\|\pmb{x}_i-\pmb{x}_j\|^2)$ for data points $\pmb{x}_i,\pmb{x}_j \in \mathbb{R}^s$;

  \STATE Calculate the $N\times N$ geometric graph Laplacian $\widetilde{\mathcal{L}}_{\varepsilon,N}$ by
  $$
  \widetilde{\mathcal{L}}_{\varepsilon,N} = 1/(c\varepsilon)(\widetilde{D}^{-1} \widetilde{W} - I_N),
  $$
  where $\widetilde{D}=diag{\widetilde{W}\pmb{1}}$, $\widetilde{W} = D^{-1}WD^{-1}$, and $D = diag{W\pmb{1}}$;

  \STATE Embed all data point $\pmb{X}\in \mathbb{R}^s$ to embedding coordinates $\pmb{Y}=(\pmb{y}^1,\dots,\pmb{y}^d)^\prime$ by any existing manifold learning algorithm EMBED;

  \STATE Obtain the matrix $\pmb{\tilde{H}}$ of all data point by applying the graph Laplacian $\widetilde{\mathcal{L}}_{\sqrt{\varepsilon},N}$ to the embedding coordinates matrix $\pmb{Y}$ with each element vector in $\pmb{\tilde{H}}$ being
  $$
    \pmb{\tilde{H}}^{i j} = \frac{1}{2} \left[\tilde{\mathcal{L}}_{\varepsilon, N}\left(\pmb{y}^i \cdot \pmb{y}^j\right) - \pmb{y}_i \cdot\left(\tilde{\mathcal{L}}_{\varepsilon, n} \pmb{y}^j\right) - \pmb{y}^j \cdot\left(\tilde{\mathcal{L}}_{\varepsilon, n} \pmb{y}^i\right)\right],
  $$
  where $i,j=1,\dots,d$ and the $\cdot$ calculation is the elementwise product between two vectors; 

  \STATE Calculate the Riemannian metric $\pmb{H}$ as the rank $d$ pseudo inverse of $\tilde{\pmb{H}}$ with 
  $$
    \pmb{H} = \pmb{U} diag{1/(\Lambda[1:d])} \pmb{U}^\prime,
  $$
  where $[\pmb{U}, \Lambda]$ is the eigendecomposition of matrix $\pmb{\tilde{h}}(x)$, and $U$ is the matrix of column eigenvectors ordered by the eigenvalues $\Lambda$ in descending order.

  \end{algorithmic}
\end{algorithm}

\hypertarget{DCKDE}{%
\subsection{Distortion corrected KDE}\label{DCKDE}}

With all fundamentals introduced, we can now give our novel Distortion Corrected KDE (DC-KDE) as
\begin{equation}
\label{eq:denestimator}
\hat{f}(\pmb{y}_j) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{r^d} \bigg(\frac{|\det \pmb{H}_j|}{|\det \pmb{H}_i|} \bigg)^{1/2} K\bigg( \frac{\| \pmb{H}^{-1/2}_i (\pmb{y}_j - \pmb{y}_i)\|}{r} \bigg).
\end{equation}

The estimator has a similar structure to Equation \eqref{eq:denriem} with some key differences. To understand these differences, it is first critical to appreciate that the coordinates \(\pmb{H}^{-1/2}_i (\pmb{y}_j - \pmb{y}_i)\) give an embedding that is approximately isometric in a small neighborhood around the \(i^{th}\) observed point (this insight is discussed at length is Section 6.2 of \textcite{Perrault-Joncas2013-pq}). This is crucial for two reasons. First, this implies that the term \(\| \pmb{H}^{-1/2}_i (\pmb{y}_j - \pmb{y}_i)\|\) approximates the geodesic distance between \(\pmb{y}_i\) and \(\pmb{y}_j\). Second, the estimator in Equation \eqref{eq:denriem} is valid only when the coordinate mapping is the logarithmic map around \(\pmb{y}_i\), and it is this mapping that is approximated by \(\pmb{H}^{-1/2}_i (\pmb{y}_j - \pmb{y}_i)\). For this reason there is a ratio of two determinants to ensure the density integrates to one, the first is a consequence of the mapping from the manifold to the coordinate system (from a dimension reduction algorithm), while the second is the transformation \(\pmb{H}^{-1/2}_i (\pmb{y}_j - \pmb{y}_i)\) which ensures that the embedding approximates the logarithmic map. Also worth noting is the resemblance between the estimator and multivariate variable bandwidth estimation \autocite{Breiman1977-qc,Jones1990-oe,Terrell1992-ut}.

One limitation of the kernel density estimator is that the density can be estimated only at points where data have been observed since the estimator requires the Riemannian \(\pmb{H}_j\). To estimate the density at points that do not belong to the sampled points, any smoothed average of nearest neighbors can be used instead. We note that the particular downstream task that we are interested in is anomaly detection for which only the density estimates at observed sample points are required because anomalies are identified as the points with lowest density. The entire workflow is summarized in \autoref{fig:vkde}. The last two steps in \autoref{fig:vkde} are our main contributions, generating distortion-corrected KDE with adaptive Riemannian metric \(\pmb{H}_i\) at each point and computing the highest density region plots based on the density estimates for anomaly detection. Compared to the anomaly detection with a general kernel density estimator in \textcite{Cheng2021-ex}, the changes are also highlighted in blue. With this anomaly detection process, outliers based on lowest densities could be detected more accurately regardless of the distortion in manifold learning.



\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/DC-KDE} 

}

\caption{The proposed schematic for anomaly detection with distortion corrected kernel density estimates.}\label{fig:vkde}
\end{figure}

\hypertarget{simulation}{%
\section{Simulations}\label{simulation}}

In this section, we examine two scenarios for both low and high dimensions to test our proposed distortion correct KDE. For visualization purposes, \autoref{twodgaussian} presents an example of a two-dimensional manifold embedded in 3-dimensional ambient Euclidean space. As a high-dimensional example, the second simulation in \autoref{fivedgaussian} is based on a 5-dimensional manifold embedded in a 100-dimensional ambient space. To estimate the density, we use the dimension reduction algorithms ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP. In general, we aim to highlight two advantages of our proposed distortion corrected KDE compared to KDE applied directly to the output coordinates. First, the density estimates are closer to the ground truth when distortion correction is used, and as a consequence, distortion correction is more adept at detecting anomalies. Second, we show how density estimation and anomaly detection are more robust to a different choice of dimension reduction method when distortion correction is used.

\hypertarget{twodgaussian}{%
\subsection{Twin peaks example}\label{twodgaussian}}

The simulation setup for the twin peaks example is to first generate vectors \(\pmb{v}_1,\dots,\pmb{v}_N\) for \(N=2000\) from a 2-dimensional Gaussian mixture model. The mixture has four components with different means
\(\pmb{\mu_1}=(0.25, 0.25)^\prime, \pmb{\mu_2}=(0.25, 0.75)^\prime, \pmb{\mu_3}=(0.75, 0.25)^\prime, \pmb{\mu_4}=(0.75, 0.75)^\prime\) and the same variance-covariance matrix \(\pmb{\Sigma}_i=diag(0.016, 0.016), i=1,2,3,4\). The mixture proportions
are equally set as \(\pi_i=0.25, i=1,2,3,4\). The two dimensional data in \autoref{fig:metadensity} is mapped to a `twin peaks' surface via the following
\begin{equation}
\label{eq:twinpeak}
\begin{array}{lcl}
x_1 = v_1, \\
x_2 = v_2, \\
x_3 = \sin(\pi v_1) \tan (3 v_2).
\end{array}
\end{equation}

This is shown on the right panel of \autoref{fig:mappings}. We also considered the `Swiss Roll' mapping shown on the left panel of \autoref{fig:mappings}, the results for this manifold are summarized in \autoref{swissrollappe}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/truedensity_twinpeaks_dc} 

}

\caption{Underlying data for the Gaussian mixture model of four kenels with means $(0.25, 0.25), (0.25, 0.75), (0.75, 0.25), (0.75, 0.75)$ and the same variance-covariance matrix $diag(0.016, 0.016)$. The colors indicate the true density of the data when they are mapped via the twin peaks function. Lower density points in darker colors are scattered both in the outer and center areas.}\label{fig:metadensity}
\end{figure}

It is important to note that the \emph{true density} on the manifold is not simply a Gaussian mixture, since the mapping in Equation \eqref{eq:twinpeak} distorts the distribution. To recover the true distribution requires the correct Jacobian term for the pushforward from \(\pmb{v}\) to the volume form of twin peaks manifold. By treating the \(\pmb{v}\) as an `output' embedding from input points \(\pmb{x}\) that lie on the true manifold and applying the Learn Metric algorithm, we can obtain \(\pmb{\Gamma}_i\) for \(i=1,\dots,n\) where \(\pmb{\Gamma}_i\) is the Riemannian of the coordinate system given by \(\pmb{v}\), this notation is distinct from \(\pmb{H}_i\) which is the output of the Learn Metric algorithm for a coordinate system obtained via a dimension reduction. The true density on the manifold can be obtained as \(p(\pmb{p}_i)=f(\pmb{v}_i)|\pmb{\Gamma}_i|^{1/2}\), where \(f(\pmb{v}_i)\) is a the density of a four component mixture of normals. Knowledge of \(\pmb{v}\) and \(\pmb{\Gamma_i}\) will not be used when estimating the density but only to establish a `ground truth' densities on the manifold. \autoref{fig:metadensity} shows the simulated \(\pmb{v}\) with color indicating the true density of data on the manifold. Anomalies are defined as points with the lowest densities shown in darker colors and with `typical' points having the highest density shown in yellow. The anomalies are found around the edges of the plot, but there are is also a low density region between the means of the four mixture components. The objective is to determine whether we can correctly identify these features without any knowledge of the true density or the \(\pmb{v}\).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak2000_5levels_outliers_comparison_5ml_r0_5} 

}

\caption{Highest density region plots of five manifold learning embeddings of the twin peaks data in each row. The top 20 outliers, hightlighted in black and indexed in blue text, are found by the true manifold density (left panel), DC-KDE (middle panel) and KDE (right panel). DC-KDE finds more true outliers than KDE in all five rows.}\label{fig:tpoutliers}
\end{figure}

\autoref{fig:tpoutliers} summarizes the results. Each row of panels corresponds to a different dimension reduction technique, while the left, center and right columns correspond to density estimates for the ground truth density, distortion corrected KDE and KDE respectively. Colors show the different estimated density at each point with anomalies shown in blue, and higher density points shown in yellow. For many methods, the salient features of the ground truth distribution are clear regardless of whether distortion correction is applied, for example for ISOMAP, all three plots, identify a similar set of outliers and four high density regions. On the other hand for LLE, the left panel shows that dimension reduction pulls outliers on the manifold in towards the centre. The distortion corrected KDE can account for this, KDE without distortion correction on the other hand does not correctly identify the anomalies. For t-SNE, the ground truth and distortion corrected KDE identify four regions of high density, while a KDE estimate without distortion correction seems to identify a larger number of modes. This concurs with the common observation that t-SNE tends to output clusters even where such clusters may not be present in the underlying data.

\begin{table}

\caption{\label{tab:tpcors}Correlation between true density ranking and estimated density ranking for different manifold learning embeddings of the twin peak data. Variable bandwidth KDE outperfoms for LLE and UMAP, and LLE gives the highest rank correlation.}
\centering
\begin{tabular}[t]{l>{}l>{}l>{}l>{}l>{}l}
\toprule
  & ISOMAP & LLE & Laplacian.Eigenmaps & t.SNE & UMAP\\
\midrule
DC-KDE & \textbf{0.823} & \textbf{0.673} & \textbf{0.672} & \textbf{0.806} & \textbf{0.794}\\
KDE & 0.798 & 0.500 & 0.606 & 0.451 & 0.469\\
\bottomrule
\end{tabular}
\end{table}

We can gain further insight by comparing the correlation between ranks of true densities and estimated densities from KDE with and without density correction by \autoref{tab:tpcors}. Distortion correction improves the rank correlation for all dimension reduction algorithms. In particular while applying KDE to the output of t-SNE and UMAP leads to a moderate correlation below 0.5, applying distortion correction improves these rank correlations to values close to 0.8.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/Twin Peak2000_densityrank_comparison_isomap_radius8_r0_5_logrank_rec_colprob_smallblocks3_crossfalse} 

}

\caption{Scatterplot of true density and estimated density ranks of ISOMAP embedding for DC-KDE and KDE, with colors indicating the absolute rank errors weighted by the sum of true and estimated ranks. DC-KDE shows a strong linear positive relationship with a higher rank correlation compared to KDE.}\label{fig:tpisomapden}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/Twin Peak2000_densityrank_comparison_tsne_radius8_r0_5_logrank_rec_colprob_smallblocks3_crossfalse} 

}

\caption{Scatterplot of true density and estimated density ranks of t-SNE embedding for DC-KDE and KDE, with colors indicating the absolute rank errors weighted by the sum of true and estimated ranks. DC-KDE shows a strong linear positive relationship with a higher rank correlation compared to KDE.}\label{fig:tptsneden}
\end{figure}

In \autoref{fig:tpisomapden}, we plot the estimated density against the true density for the ISOMAP embedding with left panels showing results for distortion correction and the right panel showing results without distortion correction. Data are presented on a log scale to highlight anomalies. The bottom left shaded region contains all points that are truly anomalies and are identifies as such, where an anomaly is defined as a point not falling within a 99\% highest density region. The middle shaded and top right shaded regions respectively contain points that are anomalies in the sense of not falling within a 95\% and 90\% HDR. Points lying outside squares (shown as triangles) will be misidentified as anomalies. For example the three red triangles in the middle left of the left panel are truly anomalies (lie outside the 99\% HDR), but are not identified as such. Overall, the right panel contains many more misclassified anomalies showing that failing to apply distortion correction can have a severe impact on anomaly detection. \autoref{fig:tptsneden} shows the same plot but for t-SNE. The quality of t-SNE is worse than ISOMAP in this example therefore many more anomalies are misclassified. However, the difference between KDE with and without distortion correction is stark. These results highlight the importance of applying distortion correction especially when the quality of dimension reduction may not be hogh.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak2000_density_compare_isomapvs4ml_radius8_r0_5_rank} 

}

\caption{Comparison of ranked densities found by ISOMAP and other four manifold learning methods for DC-KDE (on the left panel) and KDE (on the right panel).}\label{fig:tpisomapvs4ml}
\end{figure}

Finally, \autoref{fig:tpisomapvs4ml} demonstrates the robustness of distortion correction methods to the use of dimension reduction algorithm. Each row of panels compares ranks of the estimated densities based on a dimension reduction algorithm to the estimated density based on ISOMAP. The left column shoes results when distortion correction is applied, the right column when it is not applied. It can be seen that the rank correlation between estimates based on different dimension reduction algorithms is much higher when distortion correction is applied. This is critical since conclusions will be robust to the choice of dimension reduction algorithm.

\hypertarget{fivedgaussian}{%
\subsection{100-D mapping from a 5-D semi-hypersphere}\label{fivedgaussian}}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/tourr_5d_semisphere} 

}

\caption{Scatterplot display of the animation of a 5-D tour path with shapes indexing the Gaussian mixture component and the colors showing the distance to the kernel cores.}\label{fig:fivedmeta}
\end{figure}

As a high-dimensional experiment, we generate the meta data from a 5-dimensional semi-hypersphere, transform it into a 100-dimensional space, and then embed it in \(d=5\) with manifold learning. First, we simulate \(N=2,000\) points,
\((\pmb{X}_1, \pmb{X}_2, \pmb{X}_3, \pmb{X}_4)^\prime\), from a 4-dimensional Gaussian mixture model with two mixture components,
\(\mathcal{N}(\pmb{\mu}_1, \pmb{\Sigma}_1)\) and \(\mathcal{N}(\pmb{\mu}_2, \pmb{\Sigma}_2)\), where \(\pmb{\mu}_1 = \pmb{\mu}_2 =(0, 0, 0, 0)^\prime\), \(\pmb{\Sigma}_1 = diag(1,1,1,1)\), and \(\pmb{\Sigma}_1 = diag(2,2,2,2)\).
In order to manually add anomalies to be distant points from the means, the mixture proportions are set as \(\pi_1=0.99\) and \(\pi_2=0.01\). The fifth dimension is calculated to satisfy the five-dimensional semi-hypersphere surface equation
\(X_1^2 + X_2^2 + X_3^2 + X_4^2 + X_5^2 = r^2\) where \(X_5>0\) and \(r\) is set as \(7\). The Gaussian mixture densities could be calculated using Equation \eqref{eq:gmm} as the true density of the 5-d meta data.
\autoref{fig:fivedmeta} shows a scatterplot display when animating a 5-D tour path with the R package \emph{tourr} {[}REFERENCE{]}. The round and triangular point shapes index the two mixture components \(\mathcal{N}(\pmb{\mu}_1, \pmb{\Sigma}_1)\) and
\(\mathcal{N}(\pmb{\mu}_2, \pmb{\sigma}_2)\), and the colors shows the distance between the simulated \(4-D\) data point from Gaussian mixture model and the kernel means \((0, 0, 0, 0)^\prime\). The more distant from the point to the kernel cores, the lower the true densities, which shows in a darker color in \autoref{fig:fivedmeta}. It can be seen that the most distant points are in a triangular shape, meaning that they are simulated from \(\mathcal{N}(\pmb{\mu}_2, \pmb{\Sigma}_2)\). The dark colors also indicate that they are the true outliers because of their low densities.

Then we initial the other 95 dimensions in the high-dimensional space as zero columns and further rotate the \(100\)-dimensional data of size \(N\)(denote the transpose of the data matrix as \(\pmb{X}_0\) with dimension \(100 \times N\)) to get rid of the zeros so that it could be passed to the manifold learning algorithms. The rotation matrix is derived from the QR decomposition of a \(100\times 100\) matrix \(\pmb{A}\) with all components randomly generated from a uniform distribution \(\mathcal{U}(0,1)\). For any real matrix \(\pmb{A}\) of dimension \(p\times q\), the QR decomposition could decompose the matrix into the multiplication of two matrix \(\pmb{Q}\) and \(\pmb{R}\) so that \(\pmb{A} = \pmb{QR}\), where the dimension of \(Q\) is a matrix with unit norm orthogonal vectors, \(\pmb{Q}^\prime \pmb{Q} = \pmb{I}\), and \(\pmb{R}\) is an upper triangular matrix. Matrix \(\pmb{Q}\) satisfies
\(\pmb{X}_0^\prime \pmb{X}=(\pmb{QX}_0)^\prime(\pmb{QX}_0)\), meaning that the pairwise Euclidean distances between data points in
\(\pmb{X}_0^\prime\) is equivalent to that of \((\pmb{QX}_0)^\prime\).
Therefore, we use matrix \(Q\) as the rotation matrix for where the rotated data matrix \(\pmb{X} = (\pmb{QX}_0)^\prime\) of dimension \(N \times 100\) is now the input data for the manifold learning algorithms. Again, we set the embedding dimension to be equal to the meta data dimension \(d=5\).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/sim4d10000_density_comparison_4ml_radius10_k200_rankdensity_circleoutlier_with1rec} 

}

\caption{Rank comparison between the true density and estimated density from both DC-KDE and KDE. Four manifold learning methods are used rowwise. The point shapes indicates whether they are the true outleirs, and the grey shading highlights the top 1\% rank region. The colors show the distance to the center of the semisphere, with darker points being distant from the center.}\label{fig:fivedisomapden}
\end{figure}

In \autoref{fig:fivedisomapden}, the estimated densities are compared with the true density on the x-axis for four manifold learning embeddings, ISOMAP, LLE, Laplacian Eigenmaps, and UMAP. Note that we exclude t-SNE algorithm in this section because it is designed mainly for low-dimensional visualization purposes, and it is only applicable to embedding dimensions within three. Similar to \autoref{fig:fivedmeta}, the point shapes show the two mixture component in the meta data, and the colors represent the distance to the kernel means, with distant outliers shown in darker colors. For well-estimated densities, the true
outliers with low true densities will also have low estimated densities, which suggests that darker-colored points should appear in the bottom-left corner in \autoref{fig:fivedisomapden}. This is true for both ISOMAP and LLE, partly true for Laplacian Eigenmaps, but not in UMAP where these outliers have relatively high density estimates. For variable bandwidth KDE, there is a strong positive linear relationship with the true densities for ISOMAP, LLE, and Laplacian Eigenmaps, and
the relationship is stronger than the fixed bandwidth. This suggests that our proposed KDE with variable bandwidth is more accurate than the fixed bandwidth in estimating the manifold learning embedding densities.
In KDE with fixed bandwidth, the bandwidth is often too large to smooth across all data points, especially when there is severe distortion in the embedding data. By introducing the pointwise variable Riemannian metric in kernel density estimation, it is reasonable to believe that it could fix the distortion introduced by these three manifold learning algorithms.

\begin{table}

\caption{\label{tab:fivedcors}Correlation between true density and estimated density for four manifold learning embeddings.}
\centering
\begin{tabular}[t]{l>{}l>{}l>{}l>{}l}
\toprule
  & ISOMAP & LLE & Laplacian.Eigenmaps & UMAP\\
\midrule
DC-KDE & 0.968 & 0.970 & \textbf{0.8674} & \textbf{0.782}\\
KDE & \textbf{0.976} & \textbf{0.971} & 0.0328 & -0.181\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:fourdhdrtable}Percentage comparison of correct highest density regions in density estimation of four manifold learning embeddings.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{l|>{}c>{}c|>{}c>{}c|>{}c>{}c|>{}cc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{ISOMAP} & \multicolumn{2}{c}{LLE} & \multicolumn{2}{c}{Laplacian Eigenmaps} & \multicolumn{2}{c}{UMAP} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5} \cmidrule(l{3pt}r{3pt}){6-7} \cmidrule(l{3pt}r{3pt}){8-9}
  & DC-KDE & KDE & DC-KDE & KDE & DC-KDE & KDE & DC-KDE & KDE\\
\midrule
>99 & \cellcolor[HTML]{E6E6E6}{\textbf{0.830}} & 0.490 & \cellcolor[HTML]{E6E6E6}{\textbf{0.830}} & 0.240 & \cellcolor[HTML]{E6E6E6}{\textbf{0.840}} & 0.840 & \cellcolor[HTML]{E6E6E6}{\textbf{0.770}} & 0.000\\
99 & \cellcolor[HTML]{E6E6E6}{\textbf{0.818}} & 0.685 & \cellcolor[HTML]{E6E6E6}{\textbf{0.805}} & 0.478 & \cellcolor[HTML]{E6E6E6}{\textbf{0.815}} & 0.595 & \cellcolor[HTML]{E6E6E6}{\textbf{0.632}} & 0.032\\
90 & \cellcolor[HTML]{E6E6E6}{0.919} & \textbf{0.926} & \cellcolor[HTML]{E6E6E6}{\textbf{0.921}} & 0.900 & \cellcolor[HTML]{E6E6E6}{\textbf{0.798}} & 0.440 & \cellcolor[HTML]{E6E6E6}{\textbf{0.779}} & 0.428\\
50 & \cellcolor[HTML]{E6E6E6}{0.825} & \textbf{0.851} & \cellcolor[HTML]{E6E6E6}{0.831} & \textbf{0.848} & \cellcolor[HTML]{E6E6E6}{\textbf{0.659}} & 0.512 & \cellcolor[HTML]{E6E6E6}{\textbf{0.623}} & 0.448\\
5 & \cellcolor[HTML]{E6E6E6}{0.508} & \textbf{0.556} & \cellcolor[HTML]{E6E6E6}{0.514} & \textbf{0.562} & \cellcolor[HTML]{E6E6E6}{\textbf{0.431}} & 0.000 & \cellcolor[HTML]{E6E6E6}{\textbf{0.220}} & 0.000\\
\addlinespace
1 & \cellcolor[HTML]{E6E6E6}{0.080} & \textbf{0.190} & \cellcolor[HTML]{E6E6E6}{0.090} & \textbf{0.230} & \cellcolor[HTML]{E6E6E6}{\textbf{0.000}} & 0.000 & \cellcolor[HTML]{E6E6E6}{\textbf{0.000}} & 0.000\\
\bottomrule
\end{tabular}}
\end{table}

\hypertarget{application}{%
\section{Application}\label{application}}

\hypertarget{irish-smart-meter-dataset}{%
\subsection{Irish smart meter dataset}\label{irish-smart-meter-dataset}}

In this application, we use the smart meter data from the \emph{CER Smart Metering Project - Electricity Customer Behaviour Trial, 2009-2010} in Ireland \autocite{cer2012-data} between 14 July 2009 and 31 December 2010. The CER dataset\footnote{accessed via the Irish Social Science Data Archive - www.ucd.ie/issda.} records the half-hourly electricity consumption of individual residential and commercial properties, not including energy for cooling or heating systems. We selected the \(3,639\) residential data with no missing values during the data collection period for a total of
\(535\) days.

For the electricity consumption data of residential individuals, it would be worthwhile to explore the distribution of electricity demand rather than the raw consumption data to study the usage patterns of different households or different periods or the week \autocite{Hyndman2018-ia}.
\textcite{Cheng2021-ex} propose two non-Euclidean distance estimators to enable manifold learning algorithms in statistical manifolds with each observation as a distribution. \textcite{Cheng2021-ex} use the same smart meter data for identifying outliers with kernel bandwidth estimation but fail to consider the distortion and information loss in the 2-dimensional embeddings given that the input data dimension is much higher. By introducing the Riemannian metric as the bandwidth matrix, we could take into account the distortion in the 2-D embedding and further improve the accuracy of the density estimation.

In this section, we first calculated the same empirical distributions of the \(336\) half-hourly periods of the week for each household, and apply the total variation distance estimator proposed in \textcite{Cheng2021-ex} in the statistical manifold learning to get the 2-D embedding of all households. Equation \eqref{eq:denestimator} is then used to obtain density estimates with the pointwise variable Riemannian metric as the bandwidth matrix and detect outliers. The data processing steps have been clearly
stated in the application section of \textcite{Cheng2021-ex} and they are skipped here. Unlike the simulations in \autoref{simulation}, we know nothing about the true density of the electricity distributions for all periods of the week and all households, so it is impossible to compare the estimated densities with the true meta data density as in \autoref{fig:fivedisomapden}. However, we could generate all the density estimates with the existing KDE method with fixed bandwidth, which is an optimal method for density estimation, and compare the densities from our proposed method with them.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/kde_2densities_raw_electricity} 

}

\caption{Electricity usage plots of all 535 days for the most typical household and two anomalies in rows and two bandwidth selection methods in columns.}\label{fig:electricityoutliers}
\end{figure}

\autoref{fig:electricityoutliers} shows the electricity usage data of three households for both density estimation methods respectively, with the top one being the most typical household with the highest density and the bottom two being the top two outliers with the lowest densities.
The typical households in the top row are close except that there are a few spikes for the one with fixed bandwidth. As to the anomalies, variable bandwidth tends to capture the unusual electricity demand volume when the usage is very low or high. It could also capture the unusual usage pattern when there are sudden spikes in ID 3243 or very high base electricity usage for the middle time periods in ID 2838. In contrast, fixed bandwidth KDE is more sensitive to spikes even when the spikes happen in a certain time window in 7049, or when the usage has an obvious time-of-week pattern with a few low electricity usage periods.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/hdr_electricity_hdrbox_3id_7dow} 

}

\caption{Two smart-meter demand examples, ID 1003 and ID 1539, from the Irish smart meter data set.}\label{fig:electricityhdrsfixed}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/electricity_hdrbox_3id_7dow} 

}

\caption{Two smart-meter demand examples, ID 1003 and ID 1539, from the Irish smart meter data set.}\label{fig:electricityhdrsvkde}
\end{figure}

Further insights could be gained by comparing the quantile region plots of electricity demand against the time of the week for the same typical or anomalous households in \autoref{fig:electricityhdrsfixed} and \autoref{fig:electricityhdrsvkde}. Again the distribution of both typical households in the top panel has shown a repeated period-of-the-week usage pattern, with higher usage during mealtime on all seven days of the week and slightly higher usage for weekends.
However, this repeated pattern in a week window is clearly for the typical household ID 1321. As for the distributions for outliers, the middle row outliers from variable bandwidth have spikes only on Tuesday and Sunday noons, while the fixed bandwidth has an increasing electricity demand across the day of the week. The bottom row outliers both have a repeated time usage pattern, but the electricity usage amount is higher with the highest median above 3kWh. These findings show the difference in finding typical and anomalous households with different bandwidth selections.

\hypertarget{conclusion}{%
\section{Conclusions}\label{conclusion}}

In this paper, we propose a new method to estimate the density of manifold learning embedding and further identify outliers based on the densities. The Riemannian metric measure the direction and angle of the distortion when mapping data points through a non-linear function in manifold learning algorithms. By introducing the Riemannian metric as the pointwise variable bandwidth matrix in kernel density estimation, the local distortion in the low-dimensional embedding could be used to estimate densities, leading to a more accurate description of the data distributions. We compare our proposed method with fixed bandwidth KDE by two simulation settings, 2-D meta data mapped as a 3-D swiss roll or twin peaks data and 5-D semi-hypersphere mapped in 100-D space, and show that variable bandwidth could improve the density estimation given a good manifold learning embedding.

As an empirical example, we explore the distributions of different households and time periods of the week in the Irish smart meter data. Five manifold learning algorithms, including ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP, are applied to get the 2-D embeddings, and KDE with both variable and fixed bandwidth are used to get the density estimates. We compare both density estimates by looking at the distributions of the most typical households with the highest densities and the most anomalous households with the lowest densities. Both methods could identify the typical households with certain usage patterns, while the outliers are anomalous in different ways.

There are several open questions to be explored. The first involves the selection of tuning parameters for the manifold algorithm so that a maximal level of embedding quality is achieved, where embedding quality is measured using one of the metrics discussed in the online supplementary material of \textcite{Cheng2021-ex}. The scaling of the Riemannian metric to get the closest range of the true densities is also worth exploring. The scale of distortion in each point in manifold learning could vary a lot. If we could smooth across all data points, eg. multiply the Riemannian matrix with the ratio between its determinant
and the sum of all Riemannian matrix determinants, the global density estimates could be potentially smoothed. The density estimates on the edges of the whole data structure could be improved because most outer area points tend to be detected as outliers. The choice of manifold learning algorithms also has a large impact on the embedding accuracy, which we have seen will affect the density estimation and outlier detection. However, the outperformance of VKDE with Riemannian bandwidth than the fixed bandwidth has been shown in the higher dimensional simulation data and the electricity usage data, which are more related
to real-life data sets.

\hypertarget{acknowledgment}{%
\section*{Acknowledgment}\label{acknowledgment}}
\addcontentsline{toc}{section}{Acknowledgment}

This research was supported in part by the Monash eResearch Centre and eSolutions-Research Support Services through the use of the MonARCH HPC Cluster. The first author acknowledges the financial support of the Monash Graduate Scholarship (MGS) and the Monash International Tuition Scholarship (MITS) at the Monash University.

\appendix

\hypertarget{riemgeo}{%
\section{Appendix: Notions about Riemannian geometry}\label{riemgeo}}

In this appendix, we present some notions about the Riemannian geometry used in this paper.

\hypertarget{differentiable-manifolds}{%
\subsection{Differentiable manifolds}\label{differentiable-manifolds}}

In topology, a \emph{homeomorphism} is a bijective map between two topological spaces that is continuous in both directions. A \emph{Hausdorff space} is a topological space where any two distinct points can be separated by disjoint neighborhoods. And a \(d\)-dimensional (topological) \emph{manifold} \(M\) is a connected Hausdorff space \((M, \mathcal{T}_M)\) where the neighborhood \(U\) for each point \(p\) is homeomorphic to an open subset \(V\) of the Euclidean space \(\mathbb{R}^d\). Such a homeomorphism \(\varphi: U \rightarrow V\) together with \(U\) gives a (coordinate) \emph{chart}, denoted as \((U, \varphi)\), with the corresponding local coordinates \((x^1(p),\dots, x^d(p)) := \varphi(p)\). Further, a collection of charts \(\{U_\alpha, \varphi_\alpha\}\) ranging over the manifold \(M\) is called an \emph{atlas}, denoted as \(\mathcal{A}\).

The manifold \(M\) is a \emph{differentiable manifold} if there exists an atlas of \(M\), \(\{U_\alpha, \varphi_\alpha\}\), such that the \emph{transition maps} between any two charts,
\[
\varphi_\beta \circ \varphi_\alpha^{-1}: \varphi_\alpha(U_\alpha \cap U_\beta) \rightarrow \varphi_\beta(U_\alpha \cap U_\beta),
\] are differentiable of class \(C^\infty\) (smooth).
Let \(\varphi\) be an injective map: \(E \rightarrow \varphi(E)\). Then \(\varphi\) is an \emph{embedding} of \(E\) into \(M\) if and only if
\(\varphi: E \rightarrow \varphi(E)\) is a homeomorphism, and \(\varphi(E)\) is called an embedded submanifold of \(M\) with the subspace topology.

\hypertarget{tangent-vector-and-tangent-space}{%
\subsection{Tangent vector and tangent space}\label{tangent-vector-and-tangent-space}}

The tangent vector at point \(p\) can be intuitively viewed as the velocity of a curve passing through point \(p\) or as the directional derivatives at \(p\). Here we define the tangent vector via the velocity of curves.

For any point \(p \in M\), let \(\gamma_1: (-\epsilon_1, \epsilon_1)\rightarrow M\) and \(\gamma_2: (-\epsilon_2, \epsilon_2)\rightarrow M\) be two smooth curves passing through \(p\), i.e.~\(\gamma_1(0) = \gamma_2(0) = p\). We say \(\gamma_1\) and \(\gamma_2\) are \emph{equivalent} if and only if there exists a chart \((U,\varphi)\) at \(p\) such that
\[
(\varphi \circ \gamma_1)^\prime(0) = (\varphi \circ \gamma_2)^\prime(0).
\]
A \emph{tangent vector} to a manifold \(M\) at point \(p\), denoted as \(v_p\), is any equivalent class of the differentiable curves initialized at \(p\). The set of all tangent vectors at \(p\) defines the \emph{tangent space} of \(M\) at \(p\), denoted as \(T_pM\). The tangent space is a vector space of dimension \(d\), equal to the dimension of \(M\), and it does not depend on the chart \(\varphi\) locally at \(p\). The collection of all tangent spaces defines the \emph{tangent bundle}, \(TM = \cup_{p \in M}T_pM\).

Tangent vectors can also be seen as the directional derivatives at \(p\). For a given coordinate chart \(\varphi=(x^1,\dots,x^d)\), the tangent vectors defining partial
derivatives are denoted as \(\frac{\partial}{\partial x^1}(p),\dots,\frac{\partial}{\partial x^d}(p)\), which defines a \emph{basis} of the tangent space.
The tangent space \(T_pM\) also admits a dual space \(T^\star_pM\) called the \emph{cotangent space} with the corresponding \emph{cotangent vectors} \(z_p: T^\star_pM \rightarrow \mathbb{R}^d\) and a basis denoted as \(dx^1(p),\dots,dx^d(p)\).

\hypertarget{riemannian-metric-and-geodesic-distance}{%
\subsection{Riemannian metric and geodesic distance}\label{riemannian-metric-and-geodesic-distance}}

A Riemannian metric \(g_p\) defined on the tangent space \(T_pM\) at each point \(p\) is a local inner product \(T_pM \times T_pM \rightarrow \mathbb{R}\), where \(g_p\) is a \(d\times d\) symmetric positive definite matrix and varies smoothly at \(p\). Generally, we
omit the subscript \(p\) and refer to \(g\) as the Riemannian metric. The inner product between two vectors \(u, v \in T_pM\) is written as \(\langle u, v \rangle_g = g_{ij}u^iv^j\) using the Einstein summation convention where implicit summation over all indices, \(\sum_{i,j} g_{ij}u^iv^j\), is assumed. A differentiable manifold \(M\) endowed with the Riemannian metric \(g\) on each tangent space \(T_pM\) is called a \emph{Riemannian manifold} \((M,g)\).

The Riemannian metric \(g\) can be used to define the norm of a vector \(u\), \(\|u\| = \sqrt{\langle u,v \rangle_g}\), and the angle between two vectors \(u\) and \(v\), \(\cos\theta = \frac{\langle u,v \rangle_g}{\|u\| \|v\|}\), which are the geometric quantities induced by \(g\). It could also be used to define the line element \(dl^2 = g_{ij}dx^i dx^j\) and the volume element \(dV_g = \sqrt{\det(g)}dx^1 \dots dx^d\), where \((x^1,\dots,x^d)\) are the local coordinates of the chart \((U, \varphi)\).
For a curve \(\gamma: I \rightarrow M\), the length of the curve is
\[
l(\gamma) = \sqrt{\int_0^1 \|\gamma^\prime(t)\|^2_g dt} = \sqrt{\int_0^1 g_{ij} \frac{dx^i}{dt} \frac{dx^j}{dt} dt},
\]
where \(\gamma(I) \subset U\). The volume of \(W \subset U\) is defined as
\[
Vol(W) = \int_W \sqrt{\det(g)}dx^1 \cdots dx^d,
\]
which is also called the \emph{Riemannian measure} on \(M\).

The \emph{geodesics} of \(M\) are the smooth curves that locally joins the points along the shortest path on the manifold. Intuitively, geodesics are the \emph{straightest possible curves} in a Riemannian manifold \autocite[Section 7.2.3 of][]{Nakahara2018-zs}.
A curve \(\gamma: I \rightarrow M\) is a geodesic if for all indices \(i,j,k\), the second-order ordinary differential equation is satisfied,
\[
\frac{d^2 x^i}{dt^2} + \Gamma^i_{jk} \frac{d x^j}{dt} \frac{dx^k}{dt} = 0,
\]
where \(\{x^i\}\) are the coordinates of the curve \(\gamma\) and \(\Gamma^i_{jk}\) is the \emph{Christoffel symbol} defined by
\[
\Gamma^i_{jk} = \frac{1}{2} \sum_l g^{il} (\frac{\partial g_{il}}{\partial x^k} 
+ \frac{\partial g_{kl}}{\partial x^j} - \frac{\partial g_{jk}}{\partial x^l}).
\]
The geodesics have a constant speed with norm \(\| \gamma^\prime(t) \|\), and they are the local minimizers of the arc
length functional \(l:\gamma \rightarrow \sqrt{\int_0^1 \| \gamma^\prime(t) \|_g^2 dt}\) when the curves are defined over the interval \([0,1]\).
The \emph{geodesic distance} \(d_g\) is the length of the shortest geodesic between two points on the manifold. For a point \(p \in M\), when the geodesic distance starting at \(p\) is not minimized, we call such set of points the \emph{cut locus} of \(p\), and the distance to the cut locus is the \emph{injectivity radius} at \(p \in M\). Therefore, the injectivity radius of the Riemannian manifold \((M,g)\), \(\textit{inj}_gM\), is the infimum of the injectivity radii over all points on the manifold.

\hypertarget{exponential-map-and-logarithmic-map}{%
\subsection{Exponential map and logarithmic map}\label{exponential-map-and-logarithmic-map}}

Denote \(B(p, r) \subset M\) as an open ball centered at point \(p\) with raidus \(r\). Then \(B(0_p, r) = exp_p^{-1}(B(p,r))\) is an open neighborhood of \(0_p\) in the tangent space at \(p\), \(T_pM\), where \(exp_p\) is the \emph{exponential map} at point \(p\). The exponential map maps a tangent vector \(u \in B(0_p, r)\) to the endpoint of the geodesic \(\gamma: I \rightarrow M\) satisfying \(\gamma(0)=p, \gamma^{\prime}(0)=u\), and \(\gamma(1)=exp_p(u)\). It is a differentiable bijective map of differentiable inverse (i.e.~\emph{diffeomorphism}). Intuitively, the exponential map moves point \(p\) to an endpoint at speed \(u\) after covering the length of \(\|u\|\) along the geodesic in one time unit.

The inverse of the exponential map is called the \emph{logarithm map}, denoted as \(\log_p(q):= \exp^{-1}_p(q)\), which gives the tangent vector to get from point \(p\) to \(q\) in one unit time. Also define the \emph{geodesic ball} centered at \(p\) of radius \(r > 0\) as the image by the exponential map of \(B(0_p, r) \subset T_pM\) with \(r < \textit{inj}_gM\). Then we could interpolate a geodesic \(\gamma\) between two points \(p\) and \(q\) with the exponential map and the logarithmic map, \(\gamma(t) = \exp_p(t\log_p(q))\), and the geodesic distance is given by \(d_g(p,q) = \|\log_p(q)\|_g\).

\hypertarget{pushforward-and-pullback-metric}{%
\subsection{Pushforward and pullback metric}\label{pushforward-and-pullback-metric}}

Pushforward and pullback are two notions corresponding to the notions of tangent and cotangent vectors.
Let \(\phi: M \rightarrow E\) be a smooth map between the Riemannian manifold \((M,g)\) to another smooth manifold \(E\). Then the differential of \(\phi\) at point \(p\) is a linear map \(d\phi_p: T_pM \rightarrow T_{\phi (p)}E\), which pushes the tangent vector \(u \in T_pM\) at point \(p\) forward to the tangent vector \(\phi_*u \in T_{\phi (p)}E\) at the mapping point \(\phi(p)\).
The image of the tangent vector \(u \in T_pM\) under the differential \(d\phi_p\), denoted as \(d\phi_p u\) is called the pushforward of \(u\) by the map \(\phi\).
Then pushforward metric \(h=\varphi_*g\) of the Riemannian metric \(g\) along \(\varphi\) is given by the inner product
\[
\langle \phi_*u,\phi_*v \rangle_{\varphi*g} = \langle d\phi_p \phi_*u, d\phi_p \phi_*v \rangle_{g}.
\]

The tangent vectors \(\phi_*u\) are equivalent to the velocity vector of a curve \(\gamma: I\rightarrow M\) passing through point \(p\) at time zero with a constant speed \(\gamma^{\prime}(0)=u\),
\[
d\phi_p(\gamma^{\prime}(0)) = (\phi \circ \gamma)^\prime (0).
\]
Similarly, the pullback maps the cotangent vectors \(z_{f(p)}\) at \(f(p) \in E\) to cotangent vectors at \(p \in M\) acting on tangent vectors \(u \in T_pM\). The linear map is called the pullback by \(\phi\) and is often denoted as \(\phi^*\).

\hypertarget{twinpeaksappe}{%
\section{Appendix: Rank comparison plots for twin peaks mapping}\label{twinpeaksappe}}

This appendix contains the comparison plots for the density rank between DC-KDE and KDE using different manifold learning algorithms, similar to \autoref{fig:tpisomapden}, \autoref{fig:tptsneden}, and \autoref{fig:tpisomapvs4ml}. By comparing these plots, it could be concluded that DC-KDE could categorize the density ranks into highest density regions more accurately than KDE. By correcting the distortion in different manifold learning embeddings, DC-KDE is more robust in identifying the lowest density regions, which are usually used to detect anomalies.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/Twin Peak2000_densityrank_comparison_lle_radius8_r0_5_logrank_rec_colprob_smallblocks3_crossfalse} 

}

\caption{Scatterplot of true density and estimated density ranks of LLE embedding for DC-KDE and KDE, with colors indicating the absolute rank errors weighted by the sum of true and estimated ranks. DC-KDE shows a strong linear positive relationship with a higher rank correlation compared to KDE.}\label{fig:tplleden}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/Twin Peak2000_densityrank_comparison_le_radius8_r0_5_logrank_rec_colprob_smallblocks3_crossfalse} 

}

\caption{Scatterplot of true density and estimated density ranks of Laplaxian Eigenmaps embedding for DC-KDE and KDE, with colors indicating the absolute rank errors weighted by the sum of true and estimated ranks. DC-KDE shows a strong linear positive relationship with a higher rank correlation compared to KDE.}\label{fig:tpleden}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/Twin Peak2000_densityrank_comparison_umap_radius8_r0_5_logrank_rec_colprob_smallblocks3_crossfalse} 

}

\caption{Scatterplot of true density and estimated density ranks of UMAP embedding for DC-KDE and KDE, with colors indicating the absolute rank errors weighted by the sum of true and estimated ranks. DC-KDE shows a strong linear positive relationship with a higher rank correlation compared to KDE.}\label{fig:tpumapden}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak2000_density_compare_llevs4ml_radius8_r0_5_rank} 

}

\caption{Comparison of outliers found by one manifold learning method compared to the other four for DC-KDE (on the left panel) and KDE (on the right panel). The four colors and shapes represents the four gaussian kernels in the 2-D meta data. Outliers found by DC-KDE are more consistent regardless of the manifold learning embedding.}\label{fig:tpllevs4ml-1}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak2000_density_compare_levs4ml_radius8_r0_5_rank} 

}

\caption{Comparison of outliers found by one manifold learning method compared to the other four for DC-KDE (on the left panel) and KDE (on the right panel). The four colors and shapes represents the four gaussian kernels in the 2-D meta data. Outliers found by DC-KDE are more consistent regardless of the manifold learning embedding.}\label{fig:tpllevs4ml-2}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak2000_density_compare_tsnevs4ml_radius8_r0_5_rank} 

}

\caption{Comparison of outliers found by one manifold learning method compared to the other four for DC-KDE (on the left panel) and KDE (on the right panel). The four colors and shapes represents the four gaussian kernels in the 2-D meta data. Outliers found by DC-KDE are more consistent regardless of the manifold learning embedding.}\label{fig:tpllevs4ml-3}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak2000_density_compare_umapvs4ml_radius8_r0_5_rank} 

}

\caption{Comparison of outliers found by one manifold learning method compared to the other four for DC-KDE (on the left panel) and KDE (on the right panel). The four colors and shapes represents the four gaussian kernels in the 2-D meta data. Outliers found by DC-KDE are more consistent regardless of the manifold learning embedding.}\label{fig:tpllevs4ml-4}
\end{figure}

\hypertarget{swissrollappe}{%
\section{Appendix: Simulation with swiss roll mapping}\label{swissrollappe}}

In this appendix, we demonstrate the simulation results for the data in \autoref{twodgaussian} with the swiss roll mapping.

One of the most famous examples in manifold learning is the swiss roll data, with the mapping function in Equation \eqref{eq:swissroll}. The two-dimensional meta data \((\pmb{X}_1, \pmb{X}_2)^\prime\) is transformed into the three-dimensional data \((\pmb{X}, \pmb{Y}, \pmb{Z})^\prime\), shown in the left plot of \autoref{fig:mappings}. The four colors in the mappings represent the four Gaussian kernels used to generate the meta data \((\pmb{X}_1, \pmb{X}_2)^\prime\).
\begin{equation}
\label{eq:swissroll}
\left\{
\begin{array}{lcl}
X = X_1 \cos{X_1}, \\
Y = X_2, \\
Z = X_1 \sin{X_1}.
\end{array}
\right.
\end{equation}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Swiss Roll_outliers_comparison_4ml_3cases_riem0_08} 

}

\caption{Highest density region plots of five manifold learning embeddings of the swiss roll data. Colors are indicating densities from left: true densities from the Gaussian mixture model; middle: KDE with Riemannian matrix as variable bandwidth; and right: KDE with fixed bandwidth. Variable KDE performs better in finding kernel structures with ISOMAP, LLE, and Laplacian Eigenmaps, and in locating outliers with ISOMAP and LLE. The t-SNE and UMAP embeddings are highly distorted and the outliers found are clustered.}\label{fig:sroutliers}
\end{figure}

Now we are able to apply different manifold learning algorithms to \((\pmb{X}, \pmb{Y}, \pmb{Z})^\prime\) and reduce the dimension back to \(d=2\), and further estimate the density of the 2-D embedding. According to the density estimates, we could rank the data points and then identify which observations lie in the highest density region of specified coverage, eg. 1\%, 5\%, 50\%, 99\%, \textgreater99\%. For each of the five manifold learning methods, namely ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP, \autoref{fig:sroutliers} presents the 2-D embedding plot in the same row, with the colors indicating the densities levels,
the left column for true densities from the Gaussian mixture model, the middle column for highest density region plots with densities from our proposed variable KDE method, and the right for similar HDR plots with densities from KDE with fixed bandwidth. The top twenty outliers with the lowest densities are highlighted in black with point indexes in blue. From \autoref{fig:metadensity} and the data generating process, we know that there are four highest density regions. However, in all
manifold learning embeddings colored with true densities (left column in \autoref{fig:sroutliers} ), except for LLE, the number of highest density regions are not the same as the meta data. When comparing the number of HDRs for variable and fixed bandwidth (middle and right column in \autoref{fig:sroutliers} ), our proposed method with variable bandwidth outperforms fixed bandwidth for ISOMAP, LLE, and Laplacian Eigenmaps (top three rows in \autoref{fig:sroutliers} ). In terms of the top 20 outliers found rowwise, variable bandwidth could find most outliers lying on the left area of the embedding in ISOMAP and UMAP, and both methods in LLE embedding could find the outliers in the outer area, but for the other methods, both variable and fixed bandwidth are not detecting true outliers accurately. For t-SNE and UMAP embedding, the embedding structure is highly distorted and the points are clustered together in a discontinuous way, which is also shown in the clustered outliers.

\begin{table}

\caption{\label{tab:srcors}Correlation between true density ranking and estimated density ranking for different manifold learning embeddings of the swiss roll data. Variable bandwidth KDE outperfoms for LLE and UMAP, and LLE gives the highest rank correlation.}
\centering
\begin{tabular}[t]{l>{}l>{}l>{}l>{}l>{}l}
\toprule
  & ISOMAP & LLE & Laplacian.Eigenmaps & t.SNE & UMAP\\
\midrule
Variable bandwidth & 0.0696 & \textbf{0.400} & -0.2357 & 0.023 & \textbf{0.0138}\\
Fixed bandwidth & \textbf{0.2798} & 0.351 & \textbf{0.0141} & \textbf{0.367} & -0.0110\\
\bottomrule
\end{tabular}
\end{table}

To further compare the accuracy of the estimated densities for all data points, we calculate the correlation between the rank of true densities and the estimated densities and present in \autoref{tab:srcors}. It can be seen that the rank correlation of our proposed method with variable bandwidth is higher for LLE and UMAP, although the correlation for UMAP is very close to zero. The highest correlation comes from our method in LLE embedding, which is mainly due to it being closest to the
rectangular structure of the meta data shown in \autoref{fig:metadensity}. For Laplacian Eigenmaps, our method has wrongly estimated the left area with lower densities even though their true densities are very high in yellow, leading to a negative
correlation. The negative correlation would occur typically when the highest or lowest true density areas are not well estimated. As for the estimates in highly distorted embedding, including ISOMAP, t-SNE, and UMAP, the rank correlations are quite low. This shows that our proposed method could improve the kernel density estimate of manifold learning embedding by considering the distortion using the Riemannian metric.
However, if the distortion is too severe, eg. ISOMAP, or when the embedding is discontinuous, eg. t-SNE and UMAP, the density estimates are not as reliable for outlier detection.

\printbibliography

\end{document}
