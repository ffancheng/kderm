\documentclass[11pt,a4paper,]{article}
\usepackage{lmodern}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Anomaly detection with kernel density estimation on manifolds},
            pdfkeywords={manifold learning, variable bandwidth, Riemannian metric, highest density region, Gaussian kernels},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\usepackage[style=authoryear-comp,]{biblatex}
\addbibresource{references.bib}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Anomaly detection with kernel density estimation on manifolds}

%% MONASH STUFF

%% CAPTIONS
\RequirePackage{caption}
\DeclareCaptionStyle{italic}[justification=centering]
 {labelfont={bf},textfont={it},labelsep=colon}
\captionsetup[figure]{style=italic,format=hang,singlelinecheck=true}
\captionsetup[table]{style=italic,format=hang,singlelinecheck=true}

%% FONT
\RequirePackage{bera}
\RequirePackage{mathpazo}

%% HEADERS AND FOOTERS
\RequirePackage{fancyhdr}
\pagestyle{fancy}
\rfoot{\Large\sffamily\raisebox{-0.1cm}{\textbf{\thepage}}}
\makeatletter
\lhead{\textsf{\expandafter{\@title}}}
\makeatother
\rhead{}
\cfoot{}
\setlength{\headheight}{15pt}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\sffamily\thepage} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

%% MATHS
\RequirePackage{bm,amsmath}
\allowdisplaybreaks

%% GRAPHICS
\RequirePackage{graphicx}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.8}

%\RequirePackage[section]{placeins}

%% SECTION TITLES
\RequirePackage[compact,sf,bf]{titlesec}
\titleformat{\section}[block]
  {\fontsize{15}{17}\bfseries\sffamily}
  {\thesection}
  {0.4em}{}
\titleformat{\subsection}[block]
  {\fontsize{12}{14}\bfseries\sffamily}
  {\thesubsection}
  {0.4em}{}
\titlespacing{\section}{0pt}{*5}{*1}
\titlespacing{\subsection}{0pt}{*2}{*0.2}


%% TITLE PAGE
\def\Date{\number\day}
\def\Month{\ifcase\month\or
 January\or February\or March\or April\or May\or June\or
 July\or August\or September\or October\or November\or December\fi}
\def\Year{\number\year}

\makeatletter
\def\wp#1{\gdef\@wp{#1}}\def\@wp{??/??}
\def\jel#1{\gdef\@jel{#1}}\def\@jel{??}
\def\showjel{{\large\textsf{\textbf{JEL classification:}}~\@jel}}
\def\nojel{\def\showjel{}}
\def\addresses#1{\gdef\@addresses{#1}}\def\@addresses{??}
\def\cover{{\sffamily\setcounter{page}{0}
        \thispagestyle{empty}
        \placefig{2}{1.5}{width=5cm}{monash2}
        \placefig{16.9}{1.5}{width=2.1cm}{MBusSchool}
        \begin{textblock}{4}(16.9,4)ISSN 1440-771X\end{textblock}
        \begin{textblock}{7}(12.7,27.9)\hfill
        \includegraphics[height=0.7cm]{AACSB}~~~
        \includegraphics[height=0.7cm]{EQUIS}~~~
        \includegraphics[height=0.7cm]{AMBA}
        \end{textblock}
        \vspace*{2cm}
        \begin{center}\Large
        Department of Econometrics and Business Statistics\\[.5cm]
        \footnotesize http://monash.edu/business/ebs/research/publications
        \end{center}\vspace{2cm}
        \begin{center}
        \fbox{\parbox{14cm}{\begin{onehalfspace}\centering\Huge\vspace*{0.3cm}
                \textsf{\textbf{\expandafter{\@title}}}\vspace{1cm}\par
                \LARGE\@author\end{onehalfspace}
        }}
        \end{center}
        \vfill
                \begin{center}\Large
                \Month~\Year\\[1cm]
                Working Paper \@wp
        \end{center}\vspace*{2cm}}}
\def\pageone{{\sffamily\setstretch{1}%
        \thispagestyle{empty}%
        \vbox to \textheight{%
        \raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}
        \vspace{2cm}\par
        \hspace{1cm}\parbox{14cm}{\sffamily\large\@addresses}\vspace{1cm}\vfill
        \hspace{1cm}{\large\Date~\Month~\Year}\\[1cm]
        \hspace{1cm}\showjel\vss}}}
\def\blindtitle{{\sffamily
     \thispagestyle{plain}\raggedright\baselineskip=1.2cm
     {\fontsize{24.88}{30}\sffamily\textbf{\expandafter{\@title}}}\vspace{1cm}\par
        }}
\def\titlepage{{\cover\newpage\pageone\newpage\blindtitle}}

\def\blind{\def\titlepage{{\blindtitle}}\let\maketitle\blindtitle}
\def\titlepageonly{\def\titlepage{{\pageone\end{document}}}}
\def\nocover{\def\titlepage{{\pageone\newpage\blindtitle}}\let\maketitle\titlepage}
\let\maketitle\titlepage
\makeatother

%% SPACING
\RequirePackage{setspace}
\spacing{1.5}

%% LINE AND PAGE BREAKING
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000
\brokenpenalty = 10000
\RequirePackage{microtype}

%% PARAGRAPH BREAKS
\setlength{\parskip}{1.4ex}
\setlength{\parindent}{0em}

%% HYPERLINKS
\RequirePackage{xcolor} % Needed for links
\definecolor{darkblue}{rgb}{0,0,.6}
\RequirePackage{url}

\makeatletter
\@ifpackageloaded{hyperref}{}{\RequirePackage{hyperref}}
\makeatother
\hypersetup{
     citecolor=0 0 0,
     breaklinks=true,
     bookmarksopen=true,
     bookmarksnumbered=true,
     linkcolor=darkblue,
     urlcolor=blue,
     citecolor=darkblue,
     colorlinks=true}

%% KEYWORDS
\newenvironment{keywords}{\par\vspace{0.5cm}\noindent{\sffamily\textbf{Keywords:}}}{\vspace{0.25cm}\par\hrule\vspace{0.5cm}\par}

%% ABSTRACT
\renewenvironment{abstract}{\begin{minipage}{\textwidth}\parskip=1.4ex\noindent
\hrule\vspace{0.1cm}\par{\sffamily\textbf{\abstractname}}\newline}
  {\end{minipage}}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[showonlyrefs]{mathtools}
\usepackage[no-weekday]{eukdate}

%% BIBLIOGRAPHY

\makeatletter
\@ifpackageloaded{biblatex}{}{\usepackage[style=authoryear-comp, backend=biber, natbib=true]{biblatex}}
\makeatother
\ExecuteBibliographyOptions{bibencoding=utf8,minnames=1,maxnames=3, maxbibnames=99,dashed=false,terseinits=true,giveninits=true,uniquename=false,uniquelist=false,doi=false, isbn=false,url=true,sortcites=false, date=year}

\DeclareFieldFormat{url}{\texttt{\url{#1}}}
\DeclareFieldFormat[article]{pages}{#1}
\DeclareFieldFormat[inproceedings]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[incollection]{pages}{\lowercase{pp.}#1}
\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{title}{\MakeCapital{#1}}
\DeclareFieldFormat[inproceedings]{title}{#1}
\DeclareFieldFormat{shorthandwidth}{#1}
% No dot before number of articles
\usepackage{xpatch}
\xpatchbibmacro{volume+number+eid}{\setunit*{\adddot}}{}{}{}
% Remove In: for an article.
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{%
  \printtext{\bibstring{in}\intitlepunct}}}

\makeatletter
\DeclareDelimFormat[cbx@textcite]{nameyeardelim}{\addspace}
\makeatother
\renewcommand*{\finalnamedelim}{%
  %\ifnumgreater{\value{liststop}}{2}{\finalandcomma}{}% there really should be no funny Oxford comma business here
  \addspace\&\space}


\nojel

\RequirePackage[absolute,overlay]{textpos}
\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}
\def\placefig#1#2#3#4{\begin{textblock}{.1}(#1,#2)\rlap{\includegraphics[#3]{#4}}\end{textblock}}


\nocover
\blind



\date{\sf\Date~\Month~\Year}
\makeatletter
 \lfoot{\sf\@date}
\makeatother

%% Any special functions or other packages can be loaded here.
\usepackage{tabu,placeins,algorithmic}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\urlstyle{tt}  % use monospace font for urls
\usepackage[cal = txupr]{mathalpha}
\geometry{margin=2.2cm}
\mathtoolsset{showonlyrefs}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

% Adjust headwidth in case user has changed geometry in header-includes
\renewcommand{\headwidth}{\textwidth}

\begin{document}
\maketitle
\begin{abstract}
In this paper, we propose a new method to estimate the density of manifold learning embedding and further identify outliers based on the densities. The Riemannian metric measure the direction and angle of the distortion when mapping data points through a non-linear function in manifold learning algorithms. By introducing the Riemannian metric as the pointwise variable bandwidth matrix in kernel density estimation, the local distortion in the low-dimensional embedding could be used to estimate densities, leading to a more accurate description of the data distributions. We compare our proposed method with fixed bandwidth KDE by two simulation settings, 2-D meta data mapped as 3-D swiss roll or twin peaks data and 5-D semi-hypersphere meta data mapped in 100-D space, and show that variable bandwidth could improve the density estimation given a good manifold learning embedding. As an empirical example, we explore the distributions of different households and time periods of the week in the Irish smart meter data. Five manifold learning algorithms, including ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP, are applied to get the 2-D embeddings, and KDE with both variable and fixed bandwidth are used to get the density estimates. We compare both density estimates by looking at the distributions of the most typical households with highest densities and the most anomalous households with lowest densities. Both methods could identify the typical households with certain usage patterns, while the outliers are anomalous in different ways.
\end{abstract}
\begin{keywords}
manifold learning, variable bandwidth, Riemannian metric, highest density region, Gaussian kernels
\end{keywords}

\newpage

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In manifold learning, the underlying idea is that the data lies on a low-dimensional smooth manifold which is embedded in a high-dimensional space. One of the fundamental objectives of manifold learning is to explore the geometry of the dataset, including the distances between points and volumes of regions of data. These intrinsic geometric attributes of the data, such as distances, angles, and areas, however, can be distorted in the low-dimensional embedding, leading to failure to recover the geometry of the manifold \autocite{Goldberg2008-co}. To tackle this problem and measure the distortion incurred in manifold learning, \textcite{Perrault-Joncas2013-pq} propose the Metric Learning algorithm to augment any existing embedding output with geometric information in the Riemannian metric of the manifold itself. By applying the Metric Learning algorithm, the outputs of different manifold learning methods can be unified and compared under the same framework, which would highly benefit in improving the effectiveness of the embedding.

The Riemannian metric defined at each point of the manifold is used to compute the geometric quantities, including angle, length, and volume, of the low-dimensional manifold embedding in any coordinate system, and be further applied to correct the distortion caused by the manifold learning algorithms.
In variable kernel density estimate, the bandwidth matrix \(H\) is also defined to control the amount of smoothing for each data point.
Therefore, if we could replace the bandwidth matrix with the Riemannian metric, we could further get the kernel density estimation of the manifold \(\mathcal{M}\). This kernel density estimate can then be used to produce the highest density region plots \autocite{Hyndman1996-lk} for anomaly detection.

By applying an existing manifold learning algorithm to the data \(X\in \mathbb{R}^r\) with \(n\) observations, a low-dimensional embedding \(f_n \in \mathbb{R}^d\) can be computed. Most manifold learning methods involve the construction of the nearest neighbor graph based on which the Laplace-Beltrami operator \(\Delta_\mathcal{M}\) is built. The Laplacian is quite useful because it can be coordinate-free while containing all the important geometry. \textcite{Perrault-Joncas2013-pq} have stated one way to compute the approximated \(\Delta_\mathcal{M}\) with a discrete consistent estimator, the geometric graph Laplacian \(\mathcal{L}_{\varepsilon,n}\) \autocite{Zhou2011-za}, where \(\varepsilon\) is the radius parameter for the nearest neighbor graph. The graph Laplacian together with the embedding can be used in the Metric Learning algorithm to achieve the augmented embedding with the Riemannian metric \((f_n, g_n)\). The highlighted two steps in Figure \ref{fig:vkde} are the main contributions of this main chapter, replacing the bandwidth matrix \(H_i\) with the Riemannian metric \(g_i\) for each point in variable kernel density estimate, and computing the highest density region plots based on the density estimates, \(\hat{f}(\mathbb{x})\), for anomaly detection.

The rest of the paper is organized as follows.
In \autoref{vkde}, we present the proposed algorithm to detect anomalies based on variable kernel density estimates of manifold embeddings. In this section, we provide justification for the use of Riemannian metric as the bandwidth of variable kernel density estimation, including the comparison with fixed bandwidth.
\autoref{simulation} is composed with two simulations with the proposed algorithm; the first deals with a 2-dimensional meta data mapped into a 3-D swiss roll or twin peaks data and the second with a 5-D semi-hypersphere mapped in a 100-D space.
\autoref{application} contains the application to visualize and identify anomalies in the Irish smart meter dataset.
Conclusions and discussions are presented in \autoref{conclusion}.

\hypertarget{vkde}{%
\section{Variable kernel density estimation with manifold embeddings}\label{vkde}}

In this section, we introduce the proposed method to detect anomalies based on the kernel density estimates of manifold learning embeddings where the Riemannian matrix is used as the pointwise variable bandwidth to measure the direction and angle of the distortion of the low-dimensional embeddings. For a high-dimensional data set, various manifold learning algorithms including ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP, are applied to get a low-dimensional embedding. The manifold learning algorithms map the points through nonlinear functions that stretches some regions of the space while shrinks others. \textcite{Perrault-Joncas2013-pq} gives us an idea of how to measure the direction and angle of the distortion using the Riemannian metric and the Riemannian metric is a positive simi-definite square matrix for each data point. To learn the distribution of the low-dimensional embedding, we use the kernel density estimation with the bandwidth matrix being the Riemannian metric. The outliers could then be defined as the points with lowest density estimates. The proposed schematic is shown in Figure \ref{fig:vkde}.



\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/vkde} 

}

\caption{The proposed schematic for variable kernel density estimation with recovered geometry.}\label{fig:vkde}
\end{figure}

To start with, we introduce the notations used in this manuscript. Then we introduce the multivariate kernel density estimation method with variable bandwidth matrix and the metric learning algorithm to derive the pointwise Riemannian metric. Readers familiar with these topics could skip the corresponding subsections. Finally, we propose our novel algorithm to estimate densities and detect anomalies for high-dimensional data set.

\hypertarget{notations}{%
\subsection{Notations}\label{notations}}

Denote the \(s\)-dimensional input data with observations \(x_i, i = 1, 2, \dots, N\), where \(x_i \in \mathcal{R}^s\). The underlying manifold \(\mathcal{M}\) has the true density \(f\). Manifold learning takes the input data and finds its \(d\)-dimensional representation of data that lie on the manifold \(\mathcal{M}\), with output data points \(y_i, i = 1, 2, \dots, N\), where \(y_i \in \mathcal{R}^d\) and \(d \ll s\). The Riemannian metric of the embedding is denoted as \(\pmb{g_i}, i = 1, 2, \dots, N\). In kernel density estimate, the pointwise variable bandwidth matrix \(H_i\) is used to estimate the density of the embedding, \(\hat{f}\).

\hypertarget{kernel-density-estimator-on-riemannian-manifolds}{%
\subsection{Kernel density estimator on Riemannian manifolds}\label{kernel-density-estimator-on-riemannian-manifolds}}

In the case of non-Euclidean sample space, the observations often lie on a manifold with the differentiable structure called the Riemannian manifold.
Consider a compact Riemannian manifold \((\mathcal{M}, g)\) of dimension \(d\) without boundary and a probability distribution with density \(f\) on the manifold. Assume \((\mathcal{M}, d_g)\) is a complete metric space, where \(d_g\) is the Riemannian distance induced by \(g\) and the strictly positive injectivity radius \autocite{Chavel2006-mp} of the manifold. Denote \(A_i, i = 1, 2, \dots, N\), where \(A_i \in \mathcal{R}^d\), as i.i.d. random objects on \(\mathcal{M}\) with density \(f\). For each \(p\in \mathcal{M}\), \textcite{Pelletier2005-vu} proposed the kernel density estimator of \(f\) to be
\begin{equation}
\label{eq:denriem}
f_N(p) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{h^d \theta_p(A_i)} K(\frac{d_g(p, A_i)}{h}),
\end{equation}
where \(\theta_p(x)\) is the volume density function on the manifold, \(K: \mathcal{R}_{+} \rightarrow \mathcal{R}\) is a non-negative function, and \(h\) is the bandwidth.
\textcite{Pelletier2005-vu} discussed the volume density function \(\theta_p(x)\) defined for \(p\) in a neighborhood of \(x\) on the manifold, with the geodesic normal coordinates at \(x\), \(\theta_p(x)\) is equal to the determinant of the Riemannian metric \(g\) at \(\exp_p^{-1}(x)\). The expression in \eqref{eq:denriem} is also proved to be consistent with the kernel density estimators in the Euclidean case as
\begin{equation}
\label{eq:denconsist}
f_N(p) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{h^d} K(\frac{\|p-A_i\|}{h}),
\end{equation}
and converges at the same rate as the Euclidean kernel density estimator \autocite{Henry2009-ll}.

However, in application, we usually found the observations lie on a \(d\)-dimensional manifold embedded in a \(s\)-dimensional space, where \(d \ll s\). Then manifold learning can be applies to find the \(d\)-dimensional representation of the manifold \(M\). Therefore, based on the estimator in \eqref{eq:denriem}, we propose to a kernel density estimator of f with the manifold embedding \(y_i, i = 1, 2, \dots, N\) to be
\begin{equation}
\label{eq:denestimator}
f_N(p) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{h_i^d |g(\exp_p^{-1}(y_i))|} K(\frac{d_g(p, y_i)}{h_i}),
\end{equation}
where \(h_i, i = 1, 2, \dots, N\) is the pointwise variable bandwidth matrix and \(h_i\) is positive definite with dimension \(d\times d\).

An important goal of manifold learning is to recover the local or global features of the data. In order to recover the geometry of the manifold, \textcite{Perrault-Joncas2013-pq} propose a method to augment any existing reasonable embedding and allow for the computation of geometric values to be calculated with an estimation of the Riemannian metric \(g\). This could also be used to fix the geometric distortion in the embedding. It is worth noticing that the Riemannian metric from \textcite{Perrault-Joncas2013-pq} is a \(d\times d\) positive definite matrix for each data point. Therefore, we propose to take the geometric distortion into consideration when estimating the kernel densities of the embedding with Equation \eqref{eq:denestimator}. In this way, we present a consistent kernel density estimator with pointwise variable bandwidth as the Riemannian metric.

\hypertarget{multivariate-kernel-density-estimation}{%
\subsection{Multivariate kernel density estimation}\label{multivariate-kernel-density-estimation}}

In general, a multivariate variable kernel density estimate of \(f\) at point \(\pmb{x} \in \mathcal{R}^d\) is defined as
\[
\hat{f}(\pmb{x})=\sum\limits_{i=1}^N K_{\pmb{h}_i}(\pmb{x}-\pmb{a}_i),
\]
where \(a_i, i = 1, 2, \dots, N\) is the random sample drawn from a density \(f\), \(\pmb{h}_i\) is a \(d\times d\) symmetric positive definite matrix that is variable at \(i\), and \(K_{\pmb{h}_i}\) is the kernel function
\[
K_{\pmb{h}_i}(\pmb{x}-\pmb{a}_i)=(2\pi)^{-d/2}|\pmb{h}|^{-1/2}\exp\left[-\frac{1}{2}(\pmb{x}-\pmb{a}_i)'\pmb{h}_i^{-1}(\pmb{x}-\pmb{a}_i)\right],
\]
with \(|\pmb{h}_i|\) being the determinant of matrix \(\pmb{h}_i\). The bandwidth matrix is important in controlling the smoothing across all data points and the shape is determined by the kernel function \(K(\pmb{x})\). An adaptive bandwidth could be used to control the amount of smoothing on the location \(\pmb{x}\) {[}balloon estimator; \textcite{Terrell1992-ef}{]} or the data point \(\pmb{a}_i\) {[}sample smoothing estimator; \textcite{Terrell1992-ef}{]}. We use the sample smooth estimator for each data point, which make a kernel density estimate with local smoothing.
\textcite{Duong2007-up} present an R package \emph{ks} that implement kernel density estimates with fixed bandwidth and variable bandwidth, but it is limited to bandwidth vector or diagonal matrix and is not applicable to full bandwidth matrix.

\hypertarget{riemannian-metric-as-variable-bandwidth}{%
\subsection{Riemannian metric as variable bandwidth}\label{riemannian-metric-as-variable-bandwidth}}

The Riemannian metric \(g\) is a symmetric and positive definite tensor field which defines an inner product \(<,>_g\) on the tangent space \(T_p\mathcal{M}\) for every point \(p \in \mathcal{M}\).
If the inner product of the tangent space is known for a given geometry, the Riemannian metric is a good measure to recover the geometry of manifold.
The Metric Learning algorithm \autocite{Perrault-Joncas2013-pq} then augment the embedded manifold with the Riemannian metric and produce a Riemannian manifold \((\mathcal{M}, g)\).

To recover the original geometry of the manifold, we need to know what the inner product corresponds to in the embedding.
The inner product between two vectors \(u,v \in T_p\mathcal{M}\), \(<u,v>_g=g_{ij}u^iv^j\)\footnote{Here the Einstein notation is used where superscripts denote summation over \(i\) and \(j\)}, can be used to define some geometric quantities, such as the vector norm \(\|u\|=\sqrt{<u,v>_g}\) and the angle between two vectors \(\cos{\theta}=\frac{<u,v>_g}{\|u\|\|v\|}\) in the tangent space. Therefore, for each point \(p\in \mathcal{M}\) in any coordinate system, the Riemannian metric \(g\) is a \(d\times d\) symmetric positive definite matrix, where \(d\) is the dimension of the manifold.

The line element and volume element of the full manifold or a subset of the manifold can also be computed from \(g\). The arc length of a curve \(c\in \mathcal{M}\) is defined as
\[
l(c)=\int_a^b \sqrt{g_{ij} \frac{dx^i}{dt} \frac{dx^j}{dt}} dt,
\]
where \((x^1,\dots,x^d)\) are the coordinates of chart \((U,x)\) and \(c(t)\) is a function mapping \([a,b]\) to \(\mathcal{M}\). While the volume of \(V\subset \mathcal{M}\) is computed by
\[
Vol(V)=\int_V \sqrt{\|g\|} dx^1\dots dx^d.
\]
Both the concepts of distance and volume are relevant to kernel density estimation.

The Riemannian metric using the method of \textcite{Perrault-Joncas2013-pq} gives some idea of the distortion of an embedding. Mapping the points through a non-linear function ``stretches'' some regions of space and ``shrinks'' others. The Riemannian gives us an idea of the direction and angle of this stretching, which is informative for learning the manifold.
\textcite{Perrault-Joncas2013-pq} propose the Learn metric algorithm which mainly involves four steps: weighted neighborhood graph construction; geometric graph Laplacian calculation; manifold learning; and Riemannian metric calculation. We restate these four steps in Algorithm \ref{alg:learnmetric}.

As pointed out by \textcite{Perrault-Joncas2013-pq}, if the embedding dimension \(s\) is larger than the manifold intrinsic dimension \(d\), the rank of the embedding metric \(h_n(p)\) is \(d\); otherwise, the Riemannian metric \(g_n\) will be returned.
This algorithm is also implemented in a Python library \emph{megaman} \autocite{McQueen2016-xz}. It is designed to apply the manifold learning methods to large-scale data sets, as well as computing the Riemannian metric of the manifold.

\begin{algorithm}[!htb]
  \caption{Learn metric algorithm in \cite{Perrault-Joncas2013-pq} }
  \label{alg:learnmetric}
  \DontPrintSemicolon
  \SetAlgoLined
  \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}\SetKwInOut{Parameter}{parameter}\SetKwInOut{OptParameter}{optimization parameter}
  \Input{ high-dimensional data $x_i \in \mathcal{R}^s$ for all $i=1,\ldots,N$ }
  \Output{ low-dimensional data $y_i \in \mathcal{R}^d$ and its Riemannian metric $h_i$ for all $i=1,\ldots,N$ }
  \Parameter{ embedding dimension $d$, bandwidth parameter $\sqrt{\varepsilon}$, manifold learning algorithm }
  \OptParameter{ manifold learning parameters }
  \BlankLine
  \begin{algorithmic}[1]

  \STATE Construct a weighted neighborhood graph $\mathcal{G}_{w,\varepsilon}$ with weight matrix $W$ where $w_{i,j}=\exp(-\frac{1}{\varepsilon}\|x_i-x_j\|^2)$ for data points $x_i,x_j \in \mathbb{R}^s$;

  \STATE Calculate the $N\times N$ geometric graph Laplacian $\widetilde{\mathcal{L}}_{\varepsilon,N}$ by
  $$
  \widetilde{\mathcal{L}}_{\varepsilon,N} = 1/(c\varepsilon)(\widetilde{D}^{-1} \widetilde{W} - I_N),
  $$
  where $\widetilde{D}=diag{\widetilde{W}\pmb{1}}$, $\widetilde{W} = D^{-1}WD^{-1}$, and $D = diag{W\pmb{1}}$;

  \STATE Embed each data point $x_i\in \mathcal{R}^s$ to embedding coordinates $\pmb{y}=(\pmb{y}^1,\dots,\pmb{y}^d)^\prime$ by any existing manifold learning algorithm;

  \STATE Obtain the matrix $\pmb{\tilde{h}}$ of all data point by applying the graph Laplacian $\widetilde{\mathcal{L}}_{\sqrt{\varepsilon},N}$ to the embedding coordinates matrix $\pmb{y}$ with each element vector in $\pmb{\tilde{h}}$ being
  $$
    \pmb{\tilde{h}}^{i j} = \frac{1}{2} \left[\tilde{\mathcal{L}}_{\varepsilon, N}\left(y^i \cdot \pmb{y}^j\right) - \pmb{y}_i \cdot\left(\tilde{\mathcal{L}}_{\varepsilon, n} \pmb{y}^j\right) - \pmb{y}^j \cdot\left(\tilde{\mathcal{L}}_{\varepsilon, n} \pmb{y}^i\right)\right],
  $$
  where $i,j=1,\dots,d$ and the $\cdot$ calculation is the elementwise product between two vectors; 

  \STATE Calculate the Riemannian metric $\pmb{h}$ as the rank $d$ pseudo inverse of $\tilde{\pmb{    h}}$ with 
  $$
    \pmb{h} = U diag{1/(\Lambda[1:d])} U^\prime,
  $$
  where $[U, \Lambda]$ is the eigendecomposition of matrix $\pmb{\tilde{h}}(x)$, and $U$ is the matrix of column eigenvectors ordered by the eigenvalues $\Lambda$ in descending order.

  \end{algorithmic}
\end{algorithm}

\hypertarget{algorithm-for-outlier-detection-on-manifold-learning-embedding}{%
\subsection{Algorithm for outlier detection on manifold learning embedding}\label{algorithm-for-outlier-detection-on-manifold-learning-embedding}}

Now we present our proposed algorithm for anomaly detection based on variable kernel density estimates in \ref{alg:vkderm}. There are mainly four steps involved in the algorithm. The first step is to apply the Learn metric algorithm described in \ref{alg:learnmetric} to the input high-dimensional data \(x_i,i=1,\ldots,N\) where \(x_i\in \mathcal{R^s}\) to get the low-dimensional embedding \(y_i\), where \(y_i\in \mathcal{R^d}\) and the Riemannian metric \(\pmb{H}_i\) with dimension \(d\times d\) for each observation. Two parameters \(\sqrt{\varepsilon} = 0.4\) and \(c=0.25\) are given for Gaussian kernels. Then we could use the pointwise Riemannian metric to calculate the kernel density estimate with our proposed estimator in Equation \eqref{eq:denestimator}. The top outliers of size \(n\_outliers\) are obtained by ordering the embedding points \(y_i\) according to their density estimates \(f_N(\pmb{y}_i)\).

\begin{algorithm}[!htb]
  \caption{Variable kernel density estimates with Riemannian metric}
  \label{alg:vkderm}
  \DontPrintSemicolon
  \SetAlgoLined
  \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}\SetKwInOut{Parameter}{parameter}
  \Input{ high-dimensional data $x_i$ for all $i=1,\ldots,N$ }
  \Output{ outliers embedding coordinates $y_1, \dots, y_{n\_outliers}$ with their estimated densities $f_1, \dots, f_{n\_outliers}$ }
  \Parameter{ number of outliers $n\_outliers$, embedding dimension $d$ }
  \BlankLine
  \begin{algorithmic}[1]

  \STATE For all $i=1,\ldots,N$, compute the $d$-dimensional embeddings $y_i$ with any exsiting manifold leanring algorithms and the corresponding Riemannian metric $\pmb{g}_i$ using the Learn metric algorithm with inputs $d$ and $\sqrt{\varepsilon} = 0.4$ and $c=0.25$ for heat kernels;

  \STATE Set the variable bandwidth for each observation as $\pmb{h}_i = \pmb{g}_i$;

  \STATE Compute the kernel density estimates $f_N(\pmb{y}_i)$ for all $i=1,\ldots,N$ using Equation \\ref{equ:denestimator};

  \STATE Reorder the embedding coordinates $\pmb{y}$ according to the density estimates $f_N(y)$ and subset the top \textit{n\_outliers} as the outliers.

  \end{algorithmic}
\end{algorithm}

If the embedding dimension \(d=2\), the HDR plots \autocite{Hyndman1996-lk} could be used to identify the relative location of outliers in the embedding and find which observations lie in a highest density region of specified coverage, eg. 1\%, 50\%, 99\%, \textgreater99\%.

\hypertarget{simulation}{%
\section{Simulation}\label{simulation}}

In this section, we exam two scenarios for both low and high dimensions to test our proposed algorithm. For visualization purpose, \autoref{twodgaussian} presents a 2-D meta data example. We first simulate the data of size \(N=2,000\) from a mixture of four Gaussian kernels with the same covariance but different means, each consisting of \(500\) points. Different mapping functions are then applied to the 2-D meta data to be mapped in a 3-D feature space, which gives the higher-dimensional input for different manifold learning algorithms, including ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP. The embedded dimension is set as \(d=2\), same as the meta data dimension. This enables us to compare the manifold learning embedding with the true meta data. We could now apply Algorithm \ref{alg:vkderm} to get the density estimates of all data points and further detect anomalies. As a high-dimensional example, the second simulation in \autoref{fivedgaussian} is based on a 5-D meta data of size \(N=2,000\) embedded in a 100-D space and the corresponding embedding dimension is \(d=5\).

\hypertarget{twodgaussian}{%
\subsection{3-D mapping from a 2-D Gaussian Mixture Model}\label{twodgaussian}}

We first generate a 2-dimensional data of size \(N=2000\) from a Gaussian mixture model with four components with different means \(\pmb{\mu_1}=(0.25, 0.25)^\prime, \pmb{\mu_2}=(0.25, 0.75)^\prime, \pmb{\mu_3}=(0.75, 0.25)^\prime, \pmb{\mu_4}=(0.75, 0.75)^\prime\) and the same variance-covariance matrix \(\pmb{\Sigma}_i=diag(0.02, 0.02), i=1,2,3,4\). The mixture proportions are equally set as \(\pi_i=0.25, i=1,2,3,4\).
Then the mixture Gaussian mixture density function is a weighted linear combination of the four component Gaussian densities as
\begin{equation}
\label{eq:gmm}
P(\pmb{X}=\pmb{x}) = \sum_{i=1}^{4}\pi_i \frac{1}{(2\pi)^{d/2}|\pmb{\Sigma}_i|^{-1/2}} \exp{\{-\frac{1}{2} (\pmb{x}-\pmb{\mu_i})^\prime \pmb{\Sigma}_i^{-1} (\pmb{x}-\pmb{\mu_i}) \}}.
\end{equation}
\autoref{fig:metadensity} shows the 2-dimensional meta data and the colors indicate the true density of all data points calculated from \eqref{eq:gmm}, with brighter colors showing higher densities and darker colors showing lower densities. We then define outliers as points with lowest densities shown in black and typical points with highest densities shown in yellow. Based on the true density plot, the outliers are scattered in the middle and the outer area of the whole structure, while typical points are near the means of four kernels. These are \emph{true outliers} to be compared with outliers from the kernel density estimates.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/truedensity_4kernels} 

}

\caption{True density of the Gaussian mixture model of four kenels with means $(0.25, 0.25), (0.25, 0.75), (0.75, 0.25), (0.75, 0.75)$ and the same variance-covariance matrix $diag(0.02, 0.02)$. The colors indicate the density of the data and lower density points in darker colors are scattered both in the outer and center areas. The shapes indicate the four kernels.}\label{fig:metadensity}
\end{figure}

\hypertarget{swiss-roll-mapping}{%
\subsubsection{Swiss roll mapping}\label{swiss-roll-mapping}}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figures/mappings_sr_tp} 

}

\caption{3-D Mappings of the meta data with colors and shapes indicating the four kernels. Left: swiss roll mapping. Right: twin peak mapping.}\label{fig:mappings}
\end{figure}

Given the 2-D meta data, multiple mapping functions could be applied to embed the data in a 3-D space. One of the most famous example in manifold learning is the swiss roll data, with the mapping function in \eqref{eq:swissroll}. The two-dimensional meta data \((\pmb{X}_1, \pmb{X}_2)^\prime\) is transformed into the three-dimensional data \((\pmb{X}, \pmb{Y}, \pmb{Z})^\prime\), shown in the left plot of \autoref{fig:mappings}. The four colors in the mappings represents the four Gaussian kernels used to generate the meta data \((\pmb{X}_1, \pmb{X}_2)^\prime\).
\begin{equation}
\label{eq:swissroll}
\left\{
\begin{array}{lcl}
X = X_1 \cos{X_1}, \\
Y = X_2, \\
Z = X_1 \sin{X_1}.
\end{array}
\right.
\end{equation}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Swiss Roll_outliers_comparison_4ml_3cases_riem0_08} 

}

\caption{Highest density region plots of five manifold learning embeddings of the swiss roll data. Colors are indicating densities from left: true densities from the Gaussian mixture model; middle: KDE with Riemannian matrix as variable bandwidth; and right: KDE with fixed bandwidth. Variable KDE preforms better in finding kernel structures with ISOMAP, LLE, and Laplacian Eigenmaps, and in locating outliers with ISOMAP and LLE. The t-SNE and UMAP embeddings are highly distorted and the outliers found are clustered.}\label{fig:sroutliers}
\end{figure}

Now we are able to apply different manifold learning algorithms to \((\pmb{X}, \pmb{Y}, \pmb{Z})^\prime\) and reduce the dimension back to \(d=2\), and further estimate the density of the 2-D embedding. According to the density estimates, we could rank the data points and then identify which observations lie in a highest density region of specified coverage, eg. 1\%, 5\%, 50\%, 99\%, \textgreater99\%. For each of the five manifold learning methods, namely ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP, \autoref{fig:sroutliers} presents the 2-D embedding plot in the same row, with the colors indicating the densities levels, the left column for true densities from the Gaussian mixture model, the middle column for highest density region plots with densities from our proposed variable KDE method, and the right for similar HDR plots with densities from KDE with fixed bandwidth. The top twenty outliers with lowest densities are highlighted in black with point indexes in blue.
From \autoref{fig:metadensity} and the data generating process, we know that there are four highest density regions. However, in all manifold learning embeddings colored with true densities (left column in \autoref{fig:sroutliers} ), except for LLE, the number of highest density regions are not the same as the meta data. When comparing the number of HDRs for variable and fixed bandwidth (middle and right column in \autoref{fig:sroutliers} ), our proposed method with variable bandwidth outperforms fixed bandwith for ISOMAP, LLE, and Laplacian Eigenmaps (top three rows in \autoref{fig:sroutliers} ).
In terms of the top 20 outliers found rowwise, variable bandwidth could find most outliers lying on the left area of the embedding in ISOMAP and UMAP, and both methods in LLE embedding could find the outliers in the outer area, but for the other methods, both variable and fixed bandwidth are not detect true outliers accurately. For t-SNE and UMAP embedding, the embedding structure is highly distorted and the points are clustered together in a discontinuous way, which is also shown in the clustered outliers.

\begin{table}

\caption{\label{tab:srcors}Correlation between true density ranking and estimated density ranking for different manifold learning embeddings of the swiss roll data. Variable bandwidth KDE outperfoms for LLE and UMAP, and LLE gives the highest rank correlation.}
\centering
\begin{tabular}[t]{l>{}l>{}l>{}l>{}l>{}l}
\toprule
  & ISOMAP & LLE & Laplacian.Eigenmaps & t.SNE & UMAP\\
\midrule
Variable bandwidth & 0.0696 & \textbf{0.400} & -0.2357 & 0.023 & \textbf{0.0138}\\
Fixed bandwidth & \textbf{0.2798} & 0.351 & \textbf{0.0141} & \textbf{0.367} & -0.0110\\
\bottomrule
\end{tabular}
\end{table}

To further compare the accuracy of the estimated densities for all data points, we calculate the correlation between the rank of true densities and the estimated densities and present in \autoref{tab:srcors}. It can be seen that the rank correlation of our proposed method with variable bandwidth is higher for LLE and UMAP, although the correlation for UMAP is very close to zero. The highest correlation comes from our method in LLE embedding, which is mainly due to it is closest to rectangular structure of the meta data shown in \autoref{fig:metadensity}. For Laplacian Eigenmaps, our method has wrongly estimated the left area with lower densities even though their true densities are the very high in yellow, leading to a negative correlation. The negative correlation would occur typically when the highest or lowest true density areas are not well estimated. As for the estimates in highly distorted embedding, including ISOMAP, t-SNE, and UMAP, the rank correlations are quite low. This shows that our proposed method could improve the kernel density estimate of manifold learning embedding by considering the distortion using the Riemannian metric. However, if the distortion is too severe, eg. ISOMAP, or when the embedding is discontinuous, eg. t-SNE and UMAP, the density estimates are not as reliable for outlier detection.

\hypertarget{twin-peaks-mapping}{%
\subsubsection{Twin peaks mapping}\label{twin-peaks-mapping}}

For comparison, we use the same 2-D meta data in \autoref{fig:metadensity} with a different mapping function, twin peaks mapping in Equation \eqref{eq:twinpeak}, with the corresponding 3-D structure shown in the right plot of \autoref{fig:mappings}.
\begin{equation}
\label{eq:twinpeak}
\left\{ 
\begin{array}{lcl}
X = X_1, \\
Y = X_2, \\
Z = \sin(\pi X_1) \tanh (3 X_2).
\end{array}
\right.
\end{equation}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak5levels_outliers_comparison_4ml_3cases_riem0_1} 

}

\caption{Highest density region plots of four manifold learning embeddings of the twin peak data. Variable KDE preforms better in finding kernel structures with ISOMAP and LLE, and in locating outliers with t-SNE and UMAP.}\label{fig:tpoutliers}
\end{figure}

Similar to \autoref{fig:sroutliers}, different manifold learning embeddings are obtained and used to detect outliers with true densities and two bandwidth selection methods shown in \autoref{fig:tpoutliers}.
In general, the four highest density regions in yellow are identified in almost all manifold learning embeddings except for ISOMAP with fixed bandwidth and t-SNE. For ISOMAP, our proposed variable KDE, compared with the true density, gives the most accurate mixture kernel structure with lowest estimated densities (darker colored points) in the outside and the center of the embedding, and the kernel means (yellow points) with highest densities are also clearly identified. In contrast, the fixed bandwidth KDE failed to identify the lowest density area in the center. Both variable and fixed bandwidth KDE are quite close with LLE and Laplacian Eigenmaps, but in Laplacian Eigenmaps embedding, the top outliers are indexed in the middle instead of the true outer areas due to the large distortion in the middle. For t-SNE and UMAP, there are four clusters in the embedding and UMAP does a better job in finding the HDRs than t-SNE. Also due to the clusters in the embedding, the outliers found in UMAP are also clustered.

\begin{table}

\caption{\label{tab:tpcors}Correlation between true density ranking and estimated density ranking for different manifold learning embeddings of the twin peak data. Variable bandwidth KDE outperfoms for LLE and UMAP, and LLE gives the highest rank correlation.}
\centering
\begin{tabular}[t]{l>{}l>{}l>{}l>{}l>{}l}
\toprule
  & ISOMAP & LLE & Laplacian.Eigenmaps & t.SNE & UMAP\\
\midrule
Variable bandwidth & \textbf{0.899} & 0.385 & 0.620 & 0.259 & \textbf{0.659}\\
Fixed bandwidth & 0.626 & \textbf{0.399} & \textbf{0.622} & \textbf{0.663} & 0.653\\
\bottomrule
\end{tabular}
\end{table}

We can gain further insight by comparing the correlation between ranks of true densities and estimated densities from varaible and fixed bandwidth KDE by \autoref{tab:tpcors}.
Again the highest correlations appears from embedding with higher quality, including ISOMAP, Laplacian Eigenmaps, and UMAP. The rank correlations between variable and fixed bandwidth is equivalent to the thrid decimal place in Laplacian Eigemaps and UMAP. As for t-SNE, the four clusters in the embedding are less seperated than in UMAP and our proposed method has misidentified the kernel cores, leading to a lower rank correlation in variable bandwidth. Since the embeddings from twin peak data generally capture the rectangular structure in the meta data than those from the swiss roll data, the rank correlations are much higher in \autoref{tab:tpcors} than in \autoref{tab:srcors}, with the lowest correlation being 0.259. This again suggests that the accuracy of outlier detection is highly related to the quality of manifold learning embedding.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/Twin Peak_density_comparison_isomap_riem0_1} 

}

\caption{Scatterplot of true density and estimated density of ISOMAP embedding for KDE with both variable and fixed bandwidth. The four colors and shapes represents the four gaussian kernels in the 2-D meta data. Variable bandwidth shows a strong linear positive relationship.}\label{fig:tpisomapden}
\end{figure}

In \autoref{fig:tpisomapden}, we plot the estimated density against the true density of the ISOMAP embedding for KDE with variable and fixed bandwidth, with colors and shapes showing the four kernels in the meta data. The linear positive relationship between the true densities and variable KDEs on the left handside is stronger than that of the fixed bandwidth KDEs. Combined with the top-right subplot in \autoref{fig:tpoutliers}, we could tell that most points are underestimated near the true kernel cores, which also suggests that the fixed bandwidth tries to smooth across all the data points and fails to fix the local distortions in the manifold learning process like the proposed pointwise variable bandwidth.

\hypertarget{fivedgaussian}{%
\subsection{100-D mapping from a 5-D semi-hypersphere}\label{fivedgaussian}}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{figures/tourr_5d_semisphere} 

}

\caption{Scatterplot display of the animation of a 5-D tour path with shapes indexing the Gaussian mixture component and the colors showing the distance to the kernel cores.}\label{fig:fivedmeta}
\end{figure}

As a high-dimensional experiment, we generate the meta data from a 5-dimensional semi-hypersphere, transform it into a 100-dimensional space, and then embed it in \(d=5\) with manifold learning. First, we simulate \(N=2,000\) points, \((\pmb{X}_1, \pmb{X}_2, \pmb{X}_3, \pmb{X}_4)^\prime\), from a 4-dimensional Gaussian mixture model with two mixture components, \(\mathcal{N}(\pmb{\mu}_1, \pmb{\Sigma}_1)\) and \(\mathcal{N}(\pmb{\mu}_2, \pmb{\Sigma}_2)\), where \(\pmb{\mu}_1 = \pmb{\mu}_2 =(0, 0, 0, 0)^\prime\), \(\pmb{\Sigma}_1 = diag(1,1,1,1)\), and \(\pmb{\Sigma}_1 = diag(2,2,2,2)\). In order to manually add anomalies to be distant points from the means, the mixture proportions are set as \(\pi_1=0.99\) and \(\pi_2=0.01\).
The fifth dimension is calculated to satisfy the five-dimensional semi-hypersphere surface equation \(X_1^2 + X_2^2 + X_3^2 + X_4^2 + X_5^2 = r^2\) where \(X_5>0\) and \(r\) is set as \(7\).
The Gaussian mixture densities could be calculated using Equation \eqref{eq:gmm} as the true density of the 5-d meta data. \autoref{fig:fivedmeta} shows a scatterplot display when animating a 5-D tour path with the R package \emph{tourr} {[}REFERENCE{]}. The round and triangular point shapes index the two mixture components \(\mathcal{N}(\pmb{\mu}_1, \pmb{\Sigma}_1)\) and \(\mathcal{N}(\pmb{\mu}_2, \pmb{\sigma}_2)\), and the colors shows the distance between the simulated \(4-D\) data point from Gaussian mixture model and the kernel means \((0, 0, 0, 0)^\prime\). The more distant from the point to the kernel cores, the lower the true densities, which shows in a darker color in \autoref{fig:fivedmeta}. It can be seen that the most distant points are in triangular shape, meaning that they are simulated from \(\mathcal{N}(\pmb{\mu}_2, \pmb{\Sigma}_2)\). The dark colors also indicate that they are the true outliers because of their low densities.

Then we initial the other 95 dimensions in the high-dimensional space as zero columns and further rotate the \(100\)-dimensional data of size \(N\) (denote the transpose of the data matrix as \(\pmb{X}_0\) with dimension \(100 \times N\)) to get rid of the zeros so that it could be passed to the manifold learning algorithms.
The rotation matrix is derived from the QR decomposition of a \(100\times 100\) matrix \(\pmb{A}\) with all components randomly generated from a uniform distribution \(\mathcal{U}(0,1)\).
For any real matrix \(\pmb{A}\) of dimension \(p\times q\), the QR decomposition could decompose the matrix into the multiplication of two matrix \(\pmb{Q}\) and \(\pmb{R}\) so that \(\pmb{A} = \pmb{QR}\), where the dimension of \(Q\) is a matrix with unit norm orthogonal vectors, \(\pmb{Q}^\prime \pmb{Q} = \pmb{I}\), and \(\pmb{R}\) is an upper triangular matrix. Matrix \(\pmb{Q}\) satisfies
\(\pmb{X}_0^\prime \pmb{X}=(\pmb{QX}_0)^\prime(\pmb{QX}_0)\), meaning that the pairwise Euclidean distances between data points in \(\pmb{X}_0^\prime\) is equivalent to that of \((\pmb{QX}_0)^\prime\). Therefore, we use matrix \(Q\) as the rotation matrix for where the rotated data matrix \(\pmb{X} = (\pmb{QX}_0)^\prime\) of dimension \(N \times 100\) is now the input data for the manifold learning algorithms. Again, we set the embedding dimension to be equal to the meta data dimension \(d=5\).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/fived_density_comparison_4ml_riem20} 

}

\caption{Scatterplot of true density and estimated density of different embeddings for KDE with both variable and fixed bandwidth. The point shapes indicates the two Gaussian mixture components and the colors shows the distance to the kernel cores.}\label{fig:fivedisomapden}
\end{figure}

In \autoref{fig:fivedisomapden}, the estimated densities are compared with the true density on the x-axis for four manifold learning embeddings, ISOMAP, LLE, Laplacian Eigenmaps, and UMAP. Note that we exclude t-SNE algorithm in this section because it is designed mainly for low-dimensional visualization purposes, and it is only application to embedding dimension within three.
Similar to \autoref{fig:fivedmeta}, the point shapes show the two mixture component in the meta data, and the colors represent the distance to the kernel means, with distant outliers shown in darker colors. For well estimated densities, the true outliers with low true densities will also have low estimated densities, which suggest that darker-colored points should appear in the bottom-left corner in \autoref{fig:fivedisomapden}. This is true for both ISOMAP and LLE, partly true for Laplacian Eigenmaps, but not in UMAP where these outliers have relatively high density estimates.
For variable bandwidth KDE, there is a strong positive linear relationship with the true densities for ISOMAP, LLE, and Laplacian Eigenmaps, and the relationship is stronger than the fixed bandwidth. This suggests that our proposed KDE with variable bandwidth is more accurate than the fixed bandwidth in estimating the manifold learning embedding densities. In KDE with a fixed bandwidth, the bandwidth is often too large to smooth across all data points, especially when there is severe distortion in the embedding data. By introducing the pointwise variable Riemannian metric in kernel density estimation, it is reasonable to believe that it could fix the distortion introduced by these three manifold learning algorithms.

\begin{table}

\caption{\label{tab:fivedcors}Correlation between true density and estimated density for four manifold learning embeddings.}
\centering
\begin{tabular}[t]{l>{}l>{}l>{}l>{}l}
\toprule
  & ISOMAP & LLE & Laplacian.Eigenmaps & UMAP\\
\midrule
Variable bandwidth & \textbf{0.921} & \textbf{0.981} & \textbf{0.662} & \textbf{-0.130}\\
Fixed bandwidth & 0.806 & 0.940 & -0.181 & -0.341\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{application}{%
\section{Application}\label{application}}

\hypertarget{irish-smart-meter-dataset}{%
\subsection{Irish smart meter dataset}\label{irish-smart-meter-dataset}}

In this application, we use the smart meter data from the \emph{CER Smart Metering Project - Electricity Customer Behaviour Trial, 2009-2010} in Ireland \autocite{cer2012-data} between 14 July 2009 and 31 December 2010. The CER dataset\footnote{accessed via the Irish Social Science Data Archive - www.ucd.ie/issda.} records the half-hourly electricity consumption of individual residential and commercial properties, not including energy for cooling or heating systems. We selected the \(3,639\) residential data with no missing values during the data collection period for a total of \(535\) days.

For the electricity consumption data of residential individuals, it would be worthwhile to explore the distribution of electricity demand rather than the raw consumption data to study the usage patterns of different households or different periods or the week \autocite{Hyndman2018-ia}. \textcite{Cheng2021-ex} propose two non-Euclidean distance estimators to enable manifold learning algorithms in statistical manifolds with each observation as a distribution. \textcite{Cheng2021-ex} use the same smart meter data for identifying outliers with kernel bandwidth estimation but fail to consider the distortion and information loss in the 2-dimensional embeddings given that the input data dimension is much higher. By introducing the Riemannian metric as the bandwidth matrix, we could take into account the distortion in the 2-D embedding and further improve the accuracy of the density estimation.

In this section, we first calculated the same empirical distributions of the \(336\) half-hourly periods of the week for each household, and apply the total variation distance estimator proposed in \textcite{Cheng2021-ex} in the statistical manifold learning to get the 2-D embedding of all households. Algorithm \ref{alg:vkderm} is then used to obtain density estimates with the pointwise variable Riemannian metric as the bandwidth matrix and detect outliers. The data processing steps has been clearly stated in the application section of \textcite{Cheng2021-ex} and they are skipped here. Unlike the simulations in \autoref{simulation}, we know nothing about the true density of the electricity distributions for all periods of week and all households, so it is impossible to compare the estimated densities with the true meta data density as in \autoref{fig:fivedisomapden}. However, we could generate all the density estimates with the existing KDE method with fixed bandwidth, which is an optimal method for density estimation, and compare the densities from our proposed method with them.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

In this paper, we propose a new method to estimate the density of manifold learning embedding and further identify outliers based on the densities. The Riemannian metric measure the direction and angle of the distortion when mapping data points through a non-linear function in manifold learning algorithms. By introducing the Riemannian metric as the pointwise variable bandwidth matrix in kernel density estimation, the local distortion in the low-dimensional embedding could be used to estimate densities, leading to a more accurate description of the data distributions. We compare our proposed method with fixed bandwidth KDE by two simulation settings, 2-D meta data mapped as 3-D swiss roll or twin peaks data and 5-D semi-hypersphere meta data mapped in 100-D space, and show that variable bandwidth could improve the density estimation given a good manifold learning embedding.

As an empirical example, we explore the distributions of different households and time periods of the week in the Irish smart meter data. Five manifold learning algorithms, including ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, and UMAP, are applied to get the 2-D embeddings, and KDE with both variable and fixed bandwidth are used to get the density estimates. We compare both density estimates by looking at the distributions of the most typical households with highest densities and the most anomalous households with lowest densities. Both methods could identify the typical households with certain usage patterns, while the outliers are anomalous in different ways.

There are several open questions to be explored. The first involves the selection of tuning parameters for the manifold algorithm so that a maximal level of embedding quality is achieved, where embedding quality is measured using one of the metrics discussed in the online supplementary material of \textcite{Cheng2021-ex}. The scaling of the Riemannian metric to get the closest range of the true densities is also worth exploring. The scale of distortion in each point in manifold learning could vary a lot. If we could smooth across all data points, eg. multiply the Riemannian matrix with the ratio between its determinant and the sum of all Riemannian matrix determinants, the global density estimates could be potentially smoothed. The density estimates on the edges of the whole data structure could be improved because most outer area points tend to be detected as outliers. The choice of manifold learning algorithms also has a large impact on the embedding accuracy, which we have seen will affect the density estimation and outlier detection. However, the outperformance of VKDE with Riemannian bandwidth than the fixed bandwidth has been shown in the higher dimensional simulation data and the electricity usage data, which are more related to real-life data sets.

\hypertarget{acknowledgment}{%
\section*{Acknowledgment}\label{acknowledgment}}
\addcontentsline{toc}{section}{Acknowledgment}

This research was supported in part by the Monash eResearch Centre and eSolutions-Research Support Services through the use of the MonARCH HPC Cluster.

\printbibliography

\end{document}
